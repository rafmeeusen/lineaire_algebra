\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\setlength{\parindent}{0em}


\title{Oefenzittingen lineaire algebra}
\author{Raf Meeusen}
\date{2023-2024}


\begin{document}
\maketitle

\section{Oefenzitting 1, 3 okt, week 2}

(10u30, lokaal 01.14; Tom geeft oefenzittingen). 

Oefeningen 1.7 pagina 47-54. 

Oefening 1 matrix A1 gemaakt. 
Oefening 2 matrix B1 gemaakt. 
Oefeningen 4a en 4b gemaakt. Denk wel juist. 
Oefening 5d gemaakt. 
Oefening 6d gemaakt, maar niet zo heel netjes opgeschreven. 
Oefening 10 linkse stelsel gemaakt. Denk redelijk juist gemaakt. 
Oefening 11b gemaakt. Wel juist in grote lijnen. 
Oefening 12 gemaakt. Maar niet goed aangepakt, en nog eens opnieuw gedaan. Kwam derdegraadvergelijking in b in, die je moet oplossen door nulpunt (b=1) te raden, en dan veelterm te delen. 



Oefening 14 begonnen. Maar gestopt, te veel schrijfwerk. 



\section{Oefenzitting 2, 10 okt, week 3}

\subsection{Voorbereiding}

Oefening 1.7-19 p.51 gemaakt. Deel 2 $C C^T$ kwam symmetrische matrix uit. Misschien altijd zo? Element $c_{ij}$ van product komt uit rij $i$ van linkse matrix en kolom $j$ van rechts. Element $c_{ji}$ komt uit rij $j$ van linkse en kolom $i$ van rechts. Rij $i$ van linkse matrix is kolom $i$ van rechtse. En kolom $j$ van linkse is rij $j$ van rechtse matrix. Dus inderdaad altijd zo. 


Oefening 1.7-20 p.51. Volgens mij is de clou dat na herschrijven volgt dat $A-A^T = -2B^T$, dus dat we hieruit makkelijk $-2B^T$ kunnen halen, en dan ook $B$. 

Oefening 1.7-24 matrix A1 p.52. OK, inverse berekend met truuk $(A|I)$. Uitkomst: 
\begin{math}
\begin{pmatrix}
-\frac{2}{7} & 1 & \frac{6}{7}\\
-\frac{1}{7} & 1 & \frac{3}{7} \\
\frac{4}{7} & -1 & -\frac{5}{7} 
\end{pmatrix}
\end{math}

Nagekeken met online calculator, klopte. Ook nog eens gekeken en nagedacht hoe te schrijven als product van elementaire. Niet vergeten dat het ook inverse van elementaire worden op een gegeven moment! Niet helemaal uitgeschreven, maar ik snap het principe wel. 

Oefening 1.7-34 p.53. Ok, gemaakt. Deel a) met inductie en ook uit ongerijmde (als k-de macht verschillend van nul is, is ook (k+1)-de macht verschillend van nul), en dan gezien dat dat niet kan. Deel b) gewoon uitgerekend. Die inverse die voorgesteld wordt kan altijd berekend worden, dan vermendigvuldigd met $I-A$ en uitgerekend.


\subsection{Oefenzitting}

Oefening 27(a) p.52: OK uitgerekend, klopte. 

Opdracht 1.27 op p.34: OK, klopt na uitschrijven van Tr(A-B) en dan te hergroeperen. 

Oefening 1.7-21 p.51
21a) $AA^T$: OK. Element $c_{ij}$ uitschrijven als som, dan meepakken dat $b_{kj} = a_{jk}$ voor de getransponeerde, en dan ook eens element $c_{ji}$ uitschrijven, en dan zie je dat ze hetzelfde zijn. Matrix met $c_{ij} = c{ji}$ is een symmetrische. Voor $A+A^T$ is zelfs nog makkelijker. 
Andere manier: definitie van symmetrisch: $A=A^T$. Dus als we $(A+A^T)^T$ uitrekenen en opnieuw $A+A^T$ uitkomen, dan is deze inderdaad symmetrisch. En er is een eigenschap van de getransponeerde van een som, dus kan ook makkelijke zo aangetoond worden. Ook voor product moet dat waarschijnlijk zo kunnen (eigenschap $(AB)^T$), maar niet verder uitgewerkt. 
22b) is ook redelijk triviaal. Dan 22c), gewoon uitgerekend dat som klopt. Dan 22d), was wat moeilijker. Gesteld dat $A=B+C$ met $B=B^T$ en  $C=-C^T$. Dan kan je berekenen dat die B en C gelijk zijn aan de uitdrukkingen uit vorige deeloefening door uitdrukkingen voor $A$ en $A^T$ verder uit te schrijven en op te lossen naar $B$ en $C$. 
21e geskipt. 


Oefening 1.7-35 p.53. Idempotente matrix. Ene vinden: eenheidsmatrix, en eerste rij maal -1 doen. Of tweede rij, etc. Deel b is makkelijk te berekenen. 

Oefening 1.7-39 p.54. 39a is vals. Tegenvoorbeeld is ooit in de les gegeven. 39b is waar. Uit gegeven volgt dat $A=A^{-1}$ en $B=B^{-1}$. Dan is $(AB)^{-1} = (A^{-1}B^{-1})^{-1}$. Via eigenschap inverse van product volgt het gevraagde. 
39c: vals. Stel $B=-A$, dan is $A+B=0$; tegenvoorbeeld.
39d: waar. 

39e: moeilijk, ik zie hem niet. Blijkbaar vals, moet paar keer proberen met bvb. 2x2 matrix. 

39f: makkelijk, ongerijmde. Stel AB inverse wel, dan moet A wel inverteerbaar zijn. 

39g: vals. Bvb. twee rijen wisselen, en een rij met  lambda vermenigvuldigen; operaties omkeren, en het klopt niet meer. 



\section{Oefenzitting 3, 17 okt, week 4}

\subsection {Voorbereiding}


Oefening 2.4-1 (p. 78):  
\begin{itemize}
\item Matrix A1: OK, berekend op 2 manieren (resultaat: 22) 
\item Matrix A6: OK: berekend op 2 manieren (resultaat: 12) 
\item Matrix A8: niet gemaakt 
\end{itemize}

Oefening 2.4-3 (p. 78):   
Gebruikmakend van $det(AB)=det(A)det(B)$: ofwel $det(A)=0$, ofwel $det(B)=0$. Uit eerste volgt: $x=4$ of $x=1$. Uit tweede: $x=0$ of $x=-2$. 

Oefening 2.4-4 (p. 78) : 
Matrix A1: twee rijwissels om van $A1$ terug tot $A$ te komen, dus tweemaal tekenverandering determinant, dus $detA_1=detA$. 


\subsection{Oefenzitting}

(niet geweest, was nog wat ziek) 

Oefening 1.7-38(a) (p. 54): LU decompositie doen; eerst wat theorie gelezen, want ik kende het niet goed genoeg (ook niet in les gezien hoe dat juist werkt). Dan oefening gemaakt, niet moeilijk. Principes begrepen (herleiden naar trapvorm via $E_i$ waarbij de $E_i$ zelf alleen benedendriehoeks zijn, en trapvorm zelf is een bovendriehoeks, (etc. etc.). Opgemerkt dat LU-decompositie ook voor mxn matrices gedefineerd is, waarbij dan L een vierkante (mxm) is, en U een mxn. Ook nog niet helemaal duidelijk: hoe is boven/benedendriehoeks juist gedefineerd voor niet-vierkante? Ook een LDU-decompositie bekeken: een diagonaalmatrix links-buitenbrengen van een willekeurige matrix is gewoon per rij delen door corresponderende getal, ook interessant te zien dat elke diagonaalmatrix een product is van meerdere elementaire matrices van het type dat $\lambda R_i$ doet. Elementaire matrices overigens altijd vierkant. Inverteerbare ook. Maar je kan links- of rechts-inverse vinden van een niet-vierkante! 


Oefening 2.4-12 (p. 79): lang op gezocht, niet gevonden. 

Oefening 2.4-28 (p. 81-82; c/d was huistaak) 
hier: (a,b,f,g,h,i,j,k,n)

a) niet waar ; bvb. $det(\mathbb{I}_n)=1$ vs. $det(\mathbb{I}_n + \mathbb{I}_n) = 2^n$

b) waar? ik dacht eerst niet waar; geprobeerd, en bleek voorbeeld pro: 
$\begin{pmatrix}1 & 0 \\1 & 2 \end{pmatrix} \begin{pmatrix}0 & 1 \\1 & 2 \end{pmatrix} = \begin{pmatrix}0 & 1 \\2 & 5 \end{pmatrix}$. En $ \begin{pmatrix}0 & 1 \\1 & 2 \end{pmatrix} \begin{pmatrix}1 & 0 \\1 & 2 \end{pmatrix} = \begin{pmatrix}1 & 2 \\3 & 4 \end{pmatrix}$.  Nog eens nagedacht. Uiteraard waar. Want $det(AB)=det(A) det(B) = det(B)  det(A)$

c) en d) : zie huistaak 

e) vals; 

f)  waar, dit matrix bestaat niet; Ongerijmde: stel er bestaat een 3x3 matrix $A$ zodat $A^2=-\mathbb{I}_n$, dan zou $det(A^2) = (-1)^3 = -1$. Maar $det(A^2) = det(A)det(A) = (det(A))^2$. Kan niet negatief zijn. 

g) waar; Ongerijmde; Stel $det(A) \neq 0$ en $A^k=0$. Dan is $det(A^k)=det(0)=0$. Maar $det(A^k)$ is ook gelijk aan $[det(A)]^k$. Als $[det(A)]^k =0$, dan kan uitgangspunt $det(A) \neq 0$ niet kloppen. 

h) vals ; met $k^n$ zijn dat je buitenbrengt. 

i) vals ; tegenvoorbeeld, neem bvb. $2\mathbb{I}_3$ en schrijf als som: $2\mathbb{I}_3 = \begin{pmatrix}1 & 0 &0\\0 & 1& 0\\0&0&1 \end{pmatrix} + \begin{pmatrix}1 & 0 &0\\0 & 1&0\\0&0&2 \end{pmatrix}$ ; we weten dat $det(2\mathbb{I}_3) = 8$; terwijl $det\begin{pmatrix}1 & 0 &0\\0 & 1& 0\\0&0&1 \end{pmatrix} = 1$ en $det\begin{pmatrix}1 & 0 &0\\0 & 1&0\\0&0&2 \end{pmatrix} = 2$

j) waar; matrix $uv^T$ is een nxn matrix, en $v^T$ is een enkele rij;  $uv^T$kan geschreven worden als rijen onder mekaar $\begin{pmatrix}R_1=u_1v^T\\R_2=u_2v^T\\...\\R_n=u_nv^T\end{pmatrix}$; deze heeft rijen die allemaal veelvouden zijn van elkaar, bvb. $R_2 = \frac{u_2}{u_1}R_1$, en dus kan hij herleid worden tot matrix met nulrij via een rij-operatie $R_x \rightarrow R_x+\lambda R_y$, en dus is de determinant $0$. 

k) waar; combinatie van twee stellingen: $AX=B$ heeft \'e\'en oplossing asa $A$ is inverteerbaar (stelling 1.39); $A$ is inverteerbaar asa $det(A) \neq 0$ (stelling 2.4). 

l) niet gemaakt

n)  niet gemaakt 


\section{Oefenzitting 4, 24 okt, , week 5}

\subsection {Voorbereiding}

Oefening 2.4-28(l) (p.82): waar. Definitie $adj(A) = det(A) A^{-1}$. Uit gegeven volgt dan hier: $adj(A) = A^{-1}$. Opnieuw $adj$ nemen van linkse en rechtse geeft: $adj(adj(A)) = adj(A^{-1})$. Maar $adj(A) = A^{-1}$ dus is $adj(A^{-1}) = {(A^{-1}})^{-1} = A$, en dus $adj(adj(A)) = A$. QED. 

Oefening 2.4-25 (p.81): oefening op Cramer. Truuk van Cramer is per onbekende een kolom vin $A$ vervangen door kolommatrix $B$, en dan determinant hiervan berekenen, en delen door $det(A)$, wat dan $x_i$ geeft met $i$ de kolom die is vervangen. Uitgerekend en nagekeken, klopte. 

Bewijs lemma 3.7 (p.95): Gegeven $v+x=w+x$. Te bewijzen: $v=w$. Bewijs: gelijkheid $v+x+x'=w+x+x'$ volgt uit gegeven, voor elke $x'$, en wegens associativiteit dan ook $v+(x+x')=w+(x+x')$ voor elke $x'$. Kiezen we $x'$ het tegengesteld element van $x$, dan wordt dit: $v+(0)=w+(0)$, en hieruit volgt $v=w$, want $0$ is het NE. 

\subsection {Oefenzitting}

Oefening 2.4.24 (p. 81): $A$ en $B$ inverteerbaar. 

\begin{enumerate}
\item[(a)] Toon aan dat $adj(AB)=adj(B) \cdot adj(A)$. Bewijs: we hebben eigenschappen $A \cdot adj(A) = det(A) \mathbb{I}_n$, $A^{-1} = \frac{1}{det(A)} adj(A)$ en $adj(A) = det(A)A^{-1}$. Eerste eigenschap toepassen op $AB$: $AB \cdot adj(AB) = det(AB) \mathbb{I}_n$. Links en rechts links-vermenigvuldigen met $(AB)^{-1}$: $adj(AB) = (AB)^{-1} det(A) det(B) = B^{-1} A^{-1} det(A) det(B) =  det(B)B^{-1} det(A)A^{-1}$. Hieruit volgt het te bewijzen via derde eigenschap. 

\item[(b)] $adj(QAQ^{-1}) = adj(Q^{-1}) adj(A) adj(Q) $ via vorige eigenschap in a). Dan $adj(Q)=det(Q) Q^{-1}$ toepassen op $Q$ en ook op $Q^{-1}$, en wetende dat $det(X^{-1}) = \frac{1}{det(X)}$, geeft het gevraagde. 

\item[(c)] Gegeven $AB=BA$. Hint gekregen in oefeningzitting: gebruik vorige eigenschap. 
$AB=BA$, dan is $ ABB^{-1} = BAB^{-1} $ of $A = BAB^{-1}$. Dan is ook  $adj(A) = adj(BAB^{-1})$. Voor rechterlid kan eigenschap uit (b) gebruikt worden, en dan is: $adj(A) = B adj(A) B^{-1}$. Links en rechts via rechter-matrix-vermenigvuldiging met $B$: $ adj(A) B = B adj(A) $. QED. 



\end{enumerate}

Oefening 2.4.29 (p. 82-83): begonnen, maar best moeilijk. Hint in opgave suggereert bewijs door volledige inductie. Niet meer zelf gemaakt, maar in oefenzitting heeft Tom drie verschillende bewijzen (of toch outlines van bewijzen) gegeven. 
\begin{enumerate}
    \item[(1)] bewijs alternatief 1: via volledige inductie. 
    \item[(2)] bewijs alternatief 2: via determinant van een trapvorm (stelling 2.4, driehoeksmatrix: product diagonaalelementen), en via een Lemma dat $det(P)=det(A)det(B)$ blijft gelden na een ERO. 
    \item[(3)] bewijs alternatief 3: via formule (2.2) voor determinanten. 
\end{enumerate}


Bewijs alternatief 1: stap 1: we tonen aan dat $det(P)=det(A).det(B)$ geldig is voor $k=1$. Gewoon ontwikkelen naar 1e kolom, geeft $det(P)=a_{11}det(B)= det(A)det(B)$. QED. 
Dan stap 2, de inductiestap. Inductiehypothese: we nemen aan dat voor $A \in \mathbb{R}^{(k-1)\times (k-1)}$ geldt: $det(P) = det(A)det(B)$. Schrijven we dan $det(P)$ uit voor $A \in \mathbb{R}^{k\times k}$: 
\[ det(P) = det 
\begin{pmatrix}
a_{11} & ... & a_{1k}  &  \\
       & ... &   &  C \\
a_{k1} & ... & a_{kk}  & \\
       &   &         &   \\ 
       & O   &         & B 
\end{pmatrix} \]  
en ontwikkelen we naar kolom 1 ($i = $ rij-index, en wetende dat we nultermen hebben voor $i > k$):  
\[ det(P) = \sum_{i=1}^k (-1)^{i+1} a_{i1} det(M_{i1})  \]
met 
\[ det(M_{i1}) = det \begin{pmatrix}
a_{12} & ... & a_{1k}  &  \\
       & ... &   &   \\
a_{(i-1)2} & ... & a_{(i-1)k}  & C_i\\
a_{(i+1)2} & ... & a_{(i+1)k}  & \\
       & ... &   &   \\
a_{k2} & ... & a_{kk}  & \\
       &   &         &   \\ 
       & O   &         & B 
\end{pmatrix} = det \begin{pmatrix}
A_i  &  C_i \\
 O   &  B 
\end{pmatrix} \]
waarbij $A_i$ een $(k-1) \times (k-1)$ matrix is omdat de eerste kolom van $A$ is weggelaten, en de $i$-de rij van $A$ geschrapt is (en $C_i$ eveneens van $C$ is afgeleid door de $i$-de rij te schrappen). 
Nu is $M_{i1}$ een matrix die voldoet aan de vorm van matrix $P$ van de inductiehypothese, dus $det(M_{i1}) = det(A_j) det(B) $. Dus geldt: 
\[ det(P) = \sum_{i=1}^k (-1)^{i+1} a_{i1} det(A_j) det(B) =  det(B) \sum_{i=1}^k (-1)^{i+1} a_{i1} det(A_i) \]
Hierin is de som nu gewoon de ontwikkeling van de originele matrix $A$ naar de eerste kolom, dus staat er: 
\[ det(P) = det(B)det(A) = det(A)det(B)\]
QED. 

Bewijs alternatief 2: niet uitgewerkt, eventueel nog een TODO. 

Bewijs alternatief 3: heeft op bord gestaan, met truuk door sommatie over $\sigma \in \S_n$ te splitsen over twee sommaties $\sigma_1 \in S_k$ en $\sigma_2 \in S_{n-k}$. Niet helemaal gesnapt, maar denk wel dat ik die zou kunnen vinden als ik wat zoek. 


Oefening 3: zie opgave (vrij lang). Even naar gekeken, en op papier wat aan gewerkt. Zag er tegenop om me bezig te houden met die speciale + en speciale $\cdot$ notatie. Leek me allemaal wel triviaal, maar gewoon heel lastig om telkens uit te schrijven dat $\forall x: f(x)=...$. 

Oefening 4 = Doe opdracht 3.9 = p. 96 in boek. Toon aan dat $\lambda v = 0 \implies \lambda = 0$ of $v=0$. Contrapositie, we mogen ook aantonen $\lambda \neq 0 \land v \neq 0 \implies  \lambda v \neq 0$ . Uit definitie 3.3, co\"eff.1: $1.v = v$. Omdat $\lambda \neq 0$, volgt $ (\lambda \frac{1}{\lambda}) v = v$, of ook $  \frac{1}{\lambda} ( \lambda v ) = v$. Maar $v \neq 0$, dus is ook $  \frac{1}{\lambda} ( \lambda v ) \neq 0 $. In ongelijkheid links en rechts maal $\lambda$ geeft: $ \lambda v \neq 0$ (via Lemma 3.7 $\lambda 0 = 0$). 

Oefening 5: Maak oefening 3.6.3. (p. 128): Wat neergesschreven en geprobeerd: optelling niet commutatief. Ik vind geen neutraal element. Optelling niet associatief. Gemengde associativiteit wel OK.  



\section{Oefenzitting 5, week 6}

\subsection{3.6.2 p.128-129}

W2, W5, W6, en W8: voorbereiding oefenzitting (ook in nabeschouwing)

$W_2$ is geen vectorruimte. Eigenlijk "pythagorese drietallen". Nemen we (3,4,5) en (5,12,13), beide in $W2$. Eenvoudige lineaire combinatie, de som: (8,16,18).   Kwadraten: (64, 256, 324): $64 + 256 - 324 \neq 0$. 
Kunnen ook formeel aantonen dat voor willekeurige $r(x_1,y_1,z_1) + s(x_2,y_2,z_2)$ niet geldt dat hij in $W_2$ zit. 

$W_5$ is geen vectorruimte. Neem bvb. $det(rA_1 + sA_2$, en neem $A_2 = -\frac{r}{s} A_1$. Je kan nagaan dat als $det(A_1) \neq 0$, dan ook $det(A_2) \neq 0$, dus beide in $W_5$. Maar de lineaire combinatie is de nulmatrix, dus $det(rA_1 + sA_2) = 0$. 

$W_6$ is een vectorruimte. Bekijk $A_3 = rA_1 + sA_2$. Dan is $A_3^T = (rA_1 + sA_2)^T = (rA_1)^T + (sA_2)^T = r(A_1^T) + s(A_2^T) = rA_1 + sA_2 = A_3$. Dus elke $A_3$ is ook symmetrisch. En de deelruimte is niet de nulruimte want er bestaan symmetrische matrices of course. 

$W_8$ is geen vectorruimte. Eenvoudig: $rf_1 + sf_2$ geeft als functiewaarde op $0$ natuurlijk niet meer $1$ maar $r+s$. 

\subsection{3.6.5 p. 129} 
(noot: tweede implicatie niet zelf gevonden, hulp nodig assistent; best moeilijk!) 
$U, W$ deelruimten van een vectorruimte $(\mathbb{R},V,+)$. 
Twee implicaties te bewijzen. 
Implicatie ($\Leftarrow$): Gegeven is dat $U \subset W$ of $W \subset U$, met $U,W$ deelruimten van $V$.  T.B. $U \cup W$ is een deelruimte van $V$. 
Bewijs: geval $U \subset W$, dan is $U \cup W = W$, dus is $U \cup W$ een deelruimte. Gelijkaardig voor geval $W \subset U$. 

Implicatie ($\Rightarrow$): Gegeven is dat $U,W$ en $U \cup W$ deelruimten van $V$. T.B. $U \subset W$ of $W \subset U$. Bewijs: 
Via contrapositie: we tonen aan dat als $\neg(U \subset W \lor W \subset U)$, dan is $U \cup W$ geen deelruimte. Stel dus $U \not\subset W \land W \not\subset U$. Kies willekeurige $u \in U, u\not\in W$ en willekeurige $w \in W, w\not\in U$. We kijken na of $u+w$ in $U$ zit. Stel $u+w \in U$, dan geldt $u' = u+w, u' \in U$, maar dan geldt ook $w= u' - u$. Dit is echter een tegenspraak, want $w \not\in U$ en $u' - u \in U$. Zo kunnen we ook nagaan dat $u+w \not\in W$. We hebben een vector $u \in U \cup W$, en een vector $w \in U \cup W$, en de som $u+w \not\in U \cup W$. Dus $U \cup W$ is geen vectorruimte. QED. 


\subsection{3.6.6 p.129}
Als stelsel geschreven (willekeurige lineair combinatie van die 3 vectoren gelijkstellen aan gegeven vector). Stel in matrix, proberen oplossen, ik kwam strijdig stelsel uit, dus antwoord: Nee. 

\subsection{3.6.7 p. 129}
Dit is oefening 3 in oefenzitting. 
Volgens mijn gewoon stelsel oplossen. Niet gedaan. 

\subsection{3.6.9 p. 129 } 
Dit is oefening 4 in oefenzitting. 
Komt er ook op neer stelsels op te lossen. Niet afgewerkt, lijkt me doenbaar. 

\subsection{3.6.13 p. 130} 
Dit is oefening 5 in oefenzitting. 
Nee. Tegenvoorbeeld: e1=(1,0) en e2=(0,1) zijn basis; maar e1-e2 en e2-e1 niet vrij, want ene is gewoon tegengestelde van andere, dus niet lin. onafh. 


\subsection{3.6.28 p. 132}
(was in nabeschouwing, maar ook oefening 6 in oefenzitting) 
Hint: oefening 1.7.21. Die oefening zegt ergens: elke matrix $A$ kan op precies 1 manier als som van een symmetrische en een scheefsymmetrische matrix geschreven worden. 
$U$ is deelruimte van alle symmetrische matrices. $W$ is deelruimte van alle scheefsymmetrische matrices. 
Notatie $U \oplus W$ betekent twee dingen: het is de somruimte, en het gaat over een directe som. 
Aantonen dat $U \oplus W = \mathbb{R}^{n\times n}$ : kan bvb. door dubbele inclusie. Neem willekeurige $v \in U \oplus W$, en toon aan dat hij in $\mathbb{R}^{n\times n}$ zit. En omgekeerd. Via die oefening, is het eigenlijk vrij triviaal. Niet duidelijk of je moet aantonen dat het direct som is, en geen gewone som. Ook nog rekening houden met dimensies (moet voor alle $n$ gelden; wel steeds vierkante matrices). 

Test latex: grote directe som: $\bigoplus_{i=1}^k$ of \[ \bigoplus_{i=1}^k \]


\subsection{Opdracht 3.25, p. 103}
(was voorbereiding)  
Deel 1: voorbeelden $U_1, U_2, U_3 \subset \mathbb{R}^3$. $U_2$ zit in beide directe sommen, dus ik zou $U_2$ een vlak nemen. En dan zijn $U_1$ en $U_3$ bvb. twee verschillende rechten die dat vlak snijden in de oorsprong. 
Deel 2: beschrijf $U+W$ voor gegeven deelruimten. $U = \{ k + lX \}$, dus is $U+W = \{ (a+k) + (a+l)X + bX^2 + bX^3 | a,b,k,l \in \mathbb{R} \} = \{ k' + l'X + bX^2 + bX^3 | b,k',l' \in \mathbb{R} \}$. Niet directe som, omdat er oneindig veel manieren zijn om twee willekeurige re\"ele getallen $(k' , l')$ te schrijven als $(a+k, a+l)$, en dus oneindig veel manieren om een vector in $U+W$ te schrijven als som van vectoren uit resp. $U$ en $W$. 

\subsection{Oefening 1 uit PDF}
(a) Waar of fout? Toon aan of geef een tegenvoorbeeld. Zij $V$ een vectorruimte en $U_1$ en $U_2$ deelruimten van $V$ . Dan is de unie $U_1 \cup U_2$ opnieuw een deelruimte van $V$.
Antwoord: niet waar. Neem x-as en y-as in het vlak. Het zijn deelruimten, maar de unie niet, want een vector op x-as opgeteld bij vector op y-as ligt bijna nooit in de unie. 

(b) Doe Opdracht 3.21 (p. 101) (voor $k = 2$). Hint: Je moet twee inclusies aantonen. Voor \'e\'en van beide kan je de bewering dat $vct(D)$ de kleinste deelruimte is van $V$ die $D$ omvat gebruiken (boven Definitie 3.15 op p. 99). 

Beginnen we met $k=2$. 

Gegeven: $U_1,U_2$ deelruimten van $V$. 

T.B.1: $U_1+U_2 \subset vct(U_1 \cup U_2)$

T.B.2: $vct(U_1 \cup U_2) \subset U_1+U_2  $

Bewijs 1: Neem willekeurige $u \in U_1+U_2$. Uit definitie volgt: $u = u_1 + u_2$ met $u_1 \in U_1$ en $u_2 \in U_2$. Maar er geldt dan ook: $u_1 \in U_1 \cup U_2$ en $u_2 \in U_1 \cup U_2$. We kunnen dus een willeurige $u$ schrijven als de lineaire combinatie $u = 1 u_1 + 1u_2$, met $u_1,u_2 \in  U_1 \cup U_2$, waaruit volgt dat $u \in vct(U_1 \cup U_2)$. 

Bewijs 2: todo 



\section{Oefenzitting week 7}

( Niet geweest. )

Huistaak: 3.6: 12, 14 (zie aparte tex) 

Voor te bereiden: 3.6: 21

In de oefenzitting: Zie PDF (oefenzitting 6) 




\section{Oefenzitting week 8}

Voor te bereiden: 

Voorbeeld 4.4(9 en 11), 

\subsection{Oefening 4.8.5 p. 186}

Afbeelding $L_1$. Ik had intu\"itief een oplossing gezocht en gevonden, door ervan uit te gaan dat $L_1$ een lineaire afbeelding is, en via $(0,1)=(1,2)+(-1,-1)$ het beeld van $(0,1)$ te vinden, en dan nog wat verder te zoeken voor beeld van $(1,0)$. Je hebt dan beeld van een basis, en bijgevolg beeld van alles. 
Ik kwam uit: 
$L_1((1,0))=(-4,-1)$ en 
$L_1((0,1))=(2,0)$. 
Omdat $(x,y)=x(1,0) + y(0,1)$ geldt: 
$L_1((x,y)) = ( 2y-4x, -x )$
Oefenzitting Tom: je kan ook $L_1$ voorstellen als matrix-vermenigvuldiging: 
\[
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
\begin{pmatrix}
1\\
2
\end{pmatrix}
=
\begin{pmatrix}
0\\
-1
\end{pmatrix}
\]
Ook zo'n matrixvermenigvuldiging voor de tweede gegeven $L_1(v_1)$, geeft dan vier vergelijkingen voor vier onbekenden $a,b,c,d$. 

Afbeelding $L_2$. Redelijk triviaal denk ik, want $L_2$ van basis is gegeven. 

Afbeelding $L_3$. Nee, want derde gegeven beeld is van een LC van eerste twee: $(2,1,0) = (4,5,6)-2(1,2,3)$. Klopt het beeld met eerste twee: ja, dat klopt, dus geen tegenspraak, $L_2$ lijkt te bestaan. Maar dan zijn er vele $L_2$ vermoed ik. 
Derde gegeven beeld weglaten, en in matrix gieten, geeft matrix met 12 onbekenden, en geeft 8 vergelijkingen (stel $2x2$ matrix voor als 1 kolom met afspraak wat wat is). Dus 4 vrijheidsgraden bij wijze van spreke. 

Nog interessant bij discussie met Tom: ik sprak van contstante lineaire afbeelding. Hij zei: pas op, die gaat altijd nul zijn. Wat natuurlijk logisch is, anders kan je nooit aan lineariteit voldoen. 



In de oefenzitting: 
\subsection{3.6.24 p.131}
Voor $A_2$, vind een basis voor $R(A_2)$, $C(A_2$ en $N(A_2)$. 

Even over nadenken voor we beginnen tellen 
Rijruimte $R(A_2)$ is een deelverzameling van $\mathbb{R}^{3}$. En als in $A_2$ drie lineair onafhankelijke rijen zitten (van de 4 rijen die we hebben), dan is $R(A_2) = \mathbb{R}^{3}$. 

Kolomruimte 

\subsection{3.6.19 p.131}

\subsection{3.6.25 p.132}

\subsection{3.6.35 p.134}

\subsection{3.6.36 p.134}


ter ondersteuning: een extra document over de praktijk van het het uitbreiden en uitdunnen van basissen, zie 


\section{Oefenzitting week 9} 

Voor te bereiden: 

\subsection{Oefening 4.8.1(a), p.185} 
L1, L2, L6, L7, ev$_a$

\subsection{Oefening 4.8.3 p.186} 


In de oefenzitting: 

\subsection{Opdracht 4.9 p.158}



\subsection{Oefening 4.8.7, p.187}

\subsection{4.8.6(c) p.186}

\subsection{4.8.9 p.187}

\subsection{opdracht 4.19 p.164-165}

\section{Oefenzitting week 10} 

Wegens de tussentijdse toets is er deze week geen voorbereiding.

Niet geweest naar deze oefenzitting. 
In de oefenzitting: 

4.8.14, 
4.8.17, 
4.8.4, 
4.8.20, 
4.8.21


\section{Oefenzitting week 11}

Huistaak: 
4.8: 24, 5.7: 5 
De huistaak telt niet mee voor punten, maar is wel een uitstekende kans om feedback te krijgen op je antwoorden.

Voor te bereiden: 
5.7: 3

In de oefenzitting: 
5.7: 1,10,6 (zie Definitie 5.14 voor (b)), 4.8: 25

\section{Oefenzitting week 12}

Niet geweest. Ik stond wat achter met theorie verwerken, en heb eerst wat voor/na beschouwingen bekeken. 

\section*{Oefenzitting week 13}



Oefenzitting

    Voor te bereiden: 
\subsection*{6.8: 1 p. 283}
Aantonen dat dit Euclidische ruimte is: beetje raar, er staat in cursus al dat dit het standaard inproduct is (p.248), of toch voor omgekeerde. Dus ik vermoed dat het de bedoeling is om de 4 eigenschappen van definitie 6.1 in detail na te gaan voor dit inproduct. Opfrissing: (1) lineair in de eerste component, symmetrisch, inprod. met zichzelf positief, en inprod. met zichzelf definiet (alleen 0 voor nulvector). 
Heb dit niet verder voorbereid, nog een interessante TODO. 

Dan voor A1, A2: lengte, inproduct, afstand en hoek berekenen. 

Lengte $A_1$: $||A_1|| = \sqrt{ <A_1, A_1>} = \sqrt{ \text{Tr} (A_1^T A_1)  }$. 

$(A_1^T A_1) =\begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix}    \begin{pmatrix} 0 & 0 \\ 1 & 0  \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 0  \end{pmatrix}$. 

Dus is $||A_1|| = \sqrt{ \text{Tr} (A_1^T A_1)} = \sqrt{ 1 } = 1 $ 

Lengte $A_2$:  
$(A_2^T A_2) =\begin{pmatrix} 0 & 2 \\ 1 & 0  \end{pmatrix}  \begin{pmatrix} 0 & 1 \\ 2 & 0  \end{pmatrix} = \begin{pmatrix} 4 & 0 \\ 0 & 1  \end{pmatrix} $, dus is $|| A_2 || = \sqrt{5}$. 

Inproduct $<A_1, A_2>$: 

Eerst berekenen we $ A_1^T  A_2 = \begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix} \begin{pmatrix} 0 & 1 \\ 2 & 0  \end{pmatrix} =\begin{pmatrix} 2 & 0 \\ 0 & 0  \end{pmatrix}  $

Dus is $<A_1, A_2> = \sqrt{2} $

Afstand: $ || A_1 - A_2 || = ||  \begin{pmatrix} 0 & 0 \\ 1 & 0  \end{pmatrix}  - \begin{pmatrix} 0 & 1 \\ 2 & 0  \end{pmatrix}  || = || \begin{pmatrix} 0 & -1  \\ -1 & 0  \end{pmatrix}  || $

... enz. TODO, niet afgemaakt. 

\subsection*{6.8:3 p. 283}

Toon aan dat $\{ (3,-3,0),(2,2,-1),(1,1,4)  \}$ een orthogonale basis is voor de Euclidische ruimte $...,\mathbb{R}^3,...$. 

Hiervoor moeten de 3 vectoren twee aan twee orthogonaal zijn, dus 3 inproducten nakijken: 
$<(3,-3,0),(2,2,-1)>$, $<(2,2,-1),(1,1,4)  >$ , en 
 $< (3,-3,0),(1,1,4) >$. Moeten alle 0 zijn. Zien we snel door rap uit het hoofd uit te rekenen. OK. 

Tiens: hebben we de term "Euclidische ruimte" al gezien? Niet gevonden in mijn nota's, dus ik denk het niet ... 
 
\subsection*{6.8:5 p. 283}

Gram-Schmidt op gegeven basis in $\mathbb{R}^3$. Voorlopig overgeslagen.  


\subsection*{6.8:6 p. 283}
(was geen opgave, maar per ongeluk aan begonnen) 

We beginnen met $\{ v_1=1, v_2=X, v_3=X^2 \}$. 
Eerste vector normeren: $1$ heeft al norm 1 want $\int_0^1 dx = 1$ en $\sqrt{1}=1$. Dus $u_1=1$. 

Dan vectur $u'_2$ vinden: $u'_2 = v_2 - < v_2 , u_1> u_1 = X - <v_2,u_1> 1$.

$<v_2,u_1> = \int_0^1 X dx = \frac{1}{2}$. 
Dus $u'_2 = X - \frac{1}{2}$. 
Normeren. 
$|| u'_2 || = \sqrt{ \int_0^1 (X - \frac{1}{2})^2 } = \frac{1}{\sqrt{12}}$. 
Dus: $u_2 = \sqrt{12}X - \frac{\sqrt{12}}{2} $. 
(nog eens online gecheckt: norm is 1, en staat loodrecht op veelterm $1$). 
Nog verder proberen uit te rekenen, maar weer rekenfout gemaakt (online verificatie), en gestopt. 


\subsection*{bewijs stelling 6.26, p. 259}
Gegeven orthonormale basis $\beta=\{ v_1, v_2, ..., v_n\}$. 
Bewijs voor twee willekeurige vectoren $v,w$ dat $<v,w> = \sum x_i y_i$, met $(x_1,...,x_n)$ de co\"ordinaatvector van $v$ ten opzichte van $\beta$, en de $y_i$ die van $w$. 

We kunnen dus schrijven: $v = x_1v_1 + x_2v_2 +...$. En $w = y_1v_1 + y_2v_2 +...$ (betekenis van een co\"ordinaatvector). 

En dan geldt: $<v,w> = \sum \sum <x_i v_i , y_j v_j > = \sum \sum x_i y_j < v_i , v_j >$ waarbij de beide sommen lopen over de $i$ en de $j$ van $1$ tot $n$. (wegens lineariteit van inproduct). 
Nu staat er in elke term van de som een $<v_i , v_j>$, en wegens orthogonaal zijn deze $0$ voor alle $i \neq j$. Dus blijft over:  $<v,w> = \sum x_i y_i < v_i , v_i >$ voor $i=1,2,...n$. Maar de $v_i$ zijn ook genormaliseerd, dus elke $< v_i , v_i >$ is $1$. QED. 

\subsection*{Oefeningen voor in de oefenzitting: }

\subsection*{6.8: 15} 

\subsection*{6.8: 14(A1)} 
    

\section{Leeg} 

\end{document}