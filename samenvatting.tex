\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{Mijn samenvatting lineaire algebra}
\author{Raf Meeusen}
\date{2023-2024}

% mark paragraphs with empty line instead of indented first line
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}



\maketitle

% H1 
\section{Eerstegraadsvergelijkingen en matrices}

\subsection{Definities, notaties}

\begin{itemize}
    \item p.15-17: drie ERO's, type I ($\lambda R_i$), type II ($R_i \leftrightarrow R_j$), type III ($R_i \rightarrow R_i + \lambda R_j$)
    \item p.20: definitie 1.5 (leidend element, echolonform, trapvorm, rijgereduceerd) 
    \item p.21: definitie 1.6 (gebonden variabelen, vrije variabelen) 
    \item p.28-29: $A=(a_{ij})$ en $(A)_{ij} = a_{ij}$, soms met $1 \leq i \leq m$ etc. erbij
    \item p.29: "we werken analoog voor kolomvectoren" TODO ik snap dit niet goed; is $\mathbb{R}^n$ voor zowel rij- als kolomvectoren? 
    \item p.30 definitie 1.18  
    \item p.32 definitie 1.22 matrixproduct;  TODO eens zelf uitschrijven zonder te spieken
    \item p.32 definitie 1.23: scalaire matrix
    \item p.34 definitie $A^n$, en afspraak $A^0 = \mathbb{I}_n$
    \item p.34 definitie spoort (trace) 
    \item p.35 definitie 1.29 links inverse en rechts inverse van $A \in \mathbb{R}^{m \times n} $; matrix vermenigvuldiging dus dimensies moeten kloppen, en bij niet-vierkante heeft links-inverse andere dimensies dan rechts-inverse! 
    \item p.36 definitie 1.33: inverteerbare = reguliere = niet-singuliere matrix; omgekeerde: niet-inverteerbaar = singulier
    \item definitie 1.36: elementaire matrix van type I, II of III, notatie $E_n$
    \item definitie 1.43: bovendriehoeks en benedendriehoeks: ook voor niet-vierkante matrices! 
\end{itemize}


\subsection{Stellingen}

\begin{itemize}
    \item Propositie 1.8 (voor \emph{elke} matrix) 
    \item Stelling 1.11 (strijdig/1 oplossing/oneindig veel oplossingen) 
    \item Stelling 1.32 (p.36): hier al (eenvoudig) aangetoond dat bij vierkante $A$ de links en rechts inverse (als ze bestaan) gelijk moeten zijn 
    \item Stelling 1.38: goed begrijpen, onthouden
    \item Stelling 1.39: grappig bewijs in de zin dat equivalentie wordt bewezen in vorm van slang die zijn eigen staart bijt. 
    \item Lemma 1.44: product van $L_1$ en $L_2$ is opnieuw een $L$; idem voor $U$ ; $L$ benedendriehoeks, $U$ bovendriehoeks. 
\end{itemize}

\subsection{Redeneringen, inzichten}

p. 15 al: stelsel in matrixvorm: $A\cdot X=B$. Met in $X$ variabelen. Als $B=0$ dan homogeen stelsel. 

ERO's veranderen de oplossingsruimte van het stelsel niet. Voor type I en II redelijk triviaal, voor type III minder triviaal (p.17: kan je dit uitleggen). TODO: ooit als oefening gedaan denk ik. 
Elke ERO kan ongedaan gemaakt worden met inverse ERO, en telkens is inverse ERO van zelfde type. 

De verbanden tussen $m \times n$ matrix in echelonvorm, aantal niet-nul rijen $r$, en oplosbaarheid: TODO eens goed voor mezelf samenvatten. Vraag: kan $r > n$  zijn?? 

TODO: eens goed nakijken; zijn de type III operaties altijd met $j<i$ als we Gauss doen?? (zie verder LU decomp). 

p.31: eigenschappen optelling. Vooral $(A+B)^T = A^T + B^T =  B^T+A^T  $ onthouden en inzien. 

Matrixvermenigvuldiging: 
Stel $A =  \begin{pmatrix} R_1 \\ R_2 \\ ... \\ R_m  \end{pmatrix}$ met $R_i$ de rijen van $A$. En stel $B = \begin{pmatrix}  K_1 & K_2 & ... & K_n \end{pmatrix}$, met $K_i$ kolommen van $B$. 
Dan $A \cdot B = \begin{pmatrix} A\cdot K_1 & A\cdot K_2 & ... & A\cdot K_n \end{pmatrix}$, zie p. 33 punt 2. 
En er is een rij-equivalent die hier niet bij staat: $A \cdot B = \begin{pmatrix} R_1\cdot B \\  R_2\cdot B \\ ... \\ R_m\cdot B \end{pmatrix}$. 
Hieruit is makkelijk in te zien dat als $A$ een nulrij heeft, neem $R_k$, dan heeft ook $A\cdot B$ een nulrij op rij $k$, onafhankelijk van $B$. En dat als $B$ een nulkolom heeft, ook $A\cdot B$ een nulkolom heeft, onafhankelijk van $A$. Inzicht kan nog van pas komen bij inverteerbaarheid van $A\cdot B$. 

Ook: $1 \times n$ maal $n \times 1$ geeft 1 bij 1, maar omgekeerd n bij n. En ook: $n \times 1$ maal $1 \times m$ geeft $n \times m$, niet vierkant dus. En $1 \times m$ maal $B = m \times n$ heeft $n \times m$. 
Soit, dit allemaal nog eens samenvatten misschien? 



p.33 eigenschappen nu we ook vermenigvuldiging hebben. $(A \cdot B)^T = A^T B^T$. En $A \cdot \mathbb{I}_n = \mathbb{I}_n \cdot A$. 

Noot: als we een matrix zien als een "operatie", en een vermenigvuldiging van matrices zien als de opeenvolging van die twee operaties, dan vinden we het ook logisch dat de uitkomst van $AB$ niet zelfde is als $BA$. (schoenen aantrekken, dan kousen, of omgekeerd. 

p.34 puntje 7: toch wel als $A$ inverteerbaar is? Komt normaal nog wel ergens terug... 

TODO: opdracht 1.25 (p.34) voor inzicht

TODO: opdracht 1.27 (p.34) voor inzicht

p. 36, eigenschap 1.31: als $A \times B = \mathbb{I}_m$ dan is  $B^T \times A^T = \mathbb{I}_m$. Evident maar interessant. 

TODO: opdracht 3 en 4 p. 36

Elementaire operatie met elementaire matrix: links vermenigvuldigen! $E_n \times A$. 

Invertereren. 
Procedure inverteren via $(A|\mathbb{I}_n)$: snappen hoe dit in mekaar zit. En ook onthouden: als je nulrij uitkomt, dan niet-inverteerbaar. Als je $\mathbb{I}_n$ uitkomt, dan wel inverteerbaar. 
Eerder gezien dat $A\cdot B$ een nulrij van $A$ of nulkolom van $B$ overneemt. We weten ook dat als $A$ inverteerbaar is, dit eigenlijk wil zeggen dat $A\cdot B = \mathbb{I}$ voor alle $B$. Maar als $A$ een nulrij heeft, heeft $A \cdot B$ een nulrij voor alle $B$, kan $A \cdot B$ nooit gelijk zijn aan $\mathbb{I}$ (want die heeft geen nulrij), en dus kan $A$ niet inverteerbaar zijn. Gelijkaardig voor nulkolom: een matrix met nulkolom is nooit inverteerbaar. (kan beredeneerd worden via getransponeerde etc.) 
Dus niet alleen het uitkomen van een nulrij tijdens het inverteren via ERO's geeft aan dat matrix niet inverteerbaar is, ook starten met nulrij of nulkolom heeft dit al aan. 


Letterlijk in boek: $A$ is inverteerbaar als en slechts als $A = \prod E_i$. 

Oefening 39f (p.54). Als $A$ niet inverteerbaar is, is $AB$ niet inverteerbaar (voor alle $B$). Via determinant is dit makkelijk in te zien, maar nu nog geen determinant gezien. 
Redenering zonder determinant: als $A$ niet inverteerbaar is, dan bestaat er een $E_k \cdot ... \cdot E_2 \cdot E_1 \cdot A$ met een nulrij. Maar nulrij blijft behouden als je rechts vermenigvuldigt met willekeurige $B$, dus $E_k \cdot ... \cdot E_2 \cdot E_1 \cdot A \cdot B$ heeft ook steeds een nulrij. Dus $AB$ kan herleid worden via ERO's tot matrix met nulrij, dus $AB$ niet inverteerbaar voor eender welke $B$. 
Kijken we naar $A\cdot B$ voor een matrix $A$ met nulkolom: als $A$ een nulkolom heeft, dan heeft $A^T$ een nulrij, dus is $A^T$ niet inverteerbaar, dus is $A$ niet inverteerbaar (volgt uit opdracht 1.35-2), dus is $A\cdot B$ niet inverteerbaar voor alle $B$. 

$A$ en $B$ inverteerbaar, dan is $A\cdot B$ inverteerbaar (stelling 1.34). 
Contrapositie: als $A\cdot B$ niet inverteerbaar, dan zijn $A$ en $B$ niet beide inverteerbaar. 
Als $A$ niet inverteerbaar, dan is $A\cdot B$ ook nooit inverteerbaar (oefening 39f, zie eerder). 
Contrapositie: als $AB$ inverteerbaar is, dan is $A$ inverteerbaar. Als $B$ niet inverteerbaar is, is $AB$ niet inverteerbaar. Kan via getransponeerde aangetoond worden. 
Contrapositie van laatste: als $AB$ inverteerbaar, is $B$ inverteerbaar. 

Samengevat: 
\[ AB \text{ inverteerbaar } \Leftrightarrow A \text{ inverteerbaar } \land B  \text{ inverteerbaar }  \]


Vraag die ik me stelde: zijn volgende twee uitspraken equivalent? en geldt dit dan voor alle $B$? Antwoord is denk ik ja, zie verder bij Cramer op p.75. 
\begin{itemize}
    \item $A\times X = 0$ heeft enkel de triviale oplossing $X=\mathbb{O}$
    \item $A \times X = B$ heeft 1 oplossing 
\end{itemize}

LU decompositie. O.a. te onthouden: 
\begin{itemize}
    \item echelon en trapvorm zijn bovendriehoeks
    \item elemenataire matrices type I zijn boven- en benedendriehoeks; type II zijn geen van beide; type III zijn benedendriehoeks als $j<i$
\end{itemize}

Een bovendriehoeksmatrix is rij-equivalent met diagonaalmatrix met dezelfde diagonaalelementen (ALS er geen nullen op de diagonaal staan). 
Als er wel nullen op de diagonaal staan: dan komt er een nulrij bij rij-gereduceerde vorm. Idem voor benedendriehoeks. Welke rij-operaties? Alleen type III nodig! (rijwissels niet nodig, en gewone herschaling van 1 rij ook niet nodig). Van belang bij determinanten. 

TODO: eens kijken naar transponeren van elementaire matrices. Wat voor matrices worden het? Blijven het elemenataire matrices? (van belang bij determinanten) 

TODO: klopt het dat homogeen stelsel nooit strijdig kan zijn? 

% H2
\section{Determinanten}

\subsection{Definities, notaties}

\begin{itemize}
    \item definitie 2.1: definitie van "een" determinantafbeelding, met 3 eigenschappen D1 ($f(\mathbb{I}_n=1$) , D2 (2 rijen wisselen, teken $f$ verandert), D3 ($f(A)$ is lineair in de eerste rij)
    \item (p.60-63)  permutatie, transpositie, inversie, sgn, identieke permuatie $Id$ 
    \item "de determinant" det($A$) of $|A|$ of det $A$
    \item definitie 2.19: cofactor $C_{ij}$ en minor $M_{ij}$ ; verband $C_{ij} = (-1)^{i+j} \cdot \text{det}(M_{ij})$
    \item definitie 2.22: adjunctmatrix = toegevoegde matrix : adj($A$) = $(C_{ij})^T$
\end{itemize}

\subsection{Stellingen}
\begin{itemize}
    \item stelling 2.2: direct gevolg van definitie determinant: D4 (lineair in elke rij) en D5 (nulrij of 2 gelijke rijen: 0) ; opgelet: bewijs voor nulrij vind ik niet zo triviaal; volgens mij gebaseerd op $x = \lambda x$ voor alle $\lambda$. Als $x \neq 0$, dan moet $\lambda=1$ en dus niet voor alle $\lambda$, dus moet $x=0$? of zoiets? 
    \item stelling 2.3: (1) impact op $f(A)$ van type III ERO, (2) beeld $f(E)$ voor elementaire matrices type I, type II en type III, en (3) $f(E\cdot A)= f(E)\cdot f(A)$ met $E$ elemenataire matrix
    \item (p.59) stelling 2.4: (1) $f(U)$ en $f(L)$ (driehoeksmatrices), (2) verband inverteerbaarheid $A$, (3) $f(A\cdot B) = f(A) \cdot f(B)$, en (4) $f(A^T)$.  
    \item gevolg 2.5: (1) $f(A)$ bepaalt inverteerbaarheid, (2) $f(A^{-1}) = \frac{1}{f(A)}$, en (3) eigenschappen D2, D3, D4 en D5 ook voor kolommen. 
    \item stelling 2.7: elke permuatie is samenstelling van transposities (geen bewijs) 
    \item stelling 2.10: willekeurige permutatie $\sigma$ en willekeurige transpositie $\tau$: sgn($\tau\circ\sigma$) = -sgn($ \sigma$). 
    \item (geen stelling) \[ f(A) = \sum_{\sigma \in \mathbb{S}_n} \text{sgn} (\sigma) a_{1\sigma(1)} a_{2\sigma(2)} ... a_{n \sigma(n)} \] 
    \item stelling 2.20 ivm rij-ontwikkeling/kolom-ontwikkeling
    \item stelling 2.23: $A\cdot$ adj($A$) = det($A$)$\mathbb{I}_n$
    \item gevolg 2.24: $A^{-1} = \frac{1}{\text{det}A} \text{adj}A$
\end{itemize}

Ivm bewijs voor 2.4: Bewijs van (1) interessant: gaat via $f(D)$ met $D$ diagonaalmatrix, en via principe dat bovendriehoeks zonder nullen op diagonaal rij-equivalent is met diagonaalmatrix met dezelfde diagonaalelementen! (heb dit inzicht toegevoegd in H1) ; benedendriehoeks: zijn ze vergeten te vermelden in bewijs! maar een benedendriehoeks kan ook via ERO's naar diagonaal omgezet worden via ERO's. $f(D)$ berekenen: herhaaldelijk lineariteit toepassen geeft uiteindelijk product van $d_i$ en $f(\mathbb{I})$. Bewijs voor (2): loopt via $A = E_k \cdot E_{k-1} \cdot ... E_1$, en via $f(E\cdot A) = f(E)f(A)$. Bewijs voor (3): onderscheid tussen inverteerbare $A$ en niet-inverteerbare $A$. Bewijs voor (4): gebaseerd op $f(E)=f(E^T)$. 

\subsection{Redeneringen, inzichten}

Een permutatie schrijven als samenstelling van transposities is niet uniek. Toch is het aantal steeds even of oneven. En ook: $\tau \circ \tau = Id$

Formule voor determinant afleiden: niet zo triviaal, nogal wat gedoe en truken. 
Formule voor rij-ontwikkeling afleiden: ook niet zo triviaal. 
 

% H3
\section{H3 Vectorruimten}

\subsection{Definities, notaties}

\begin{itemize}
    \item definitie 3.2 commutatieve groep (verzameling met 1 bewerking, 4 eigenschappen)
    \item definitie 3.3 re\"ele vectorruimte, notatie $(\mathbb{R},V,+)$, combinatie commutatieve groep met externe scalaire vermenigvuldiging
    \item definitie 3.10 deelruimte = iets dat zowel deelverzameling is, alsook vectorruimte op zichzelf
    \item definitie 3.15 "voortgebracht door"; notatie vct($D$); 
    \item definitie 3.19 
    \item definitie 3.20 
    \item definitie 3.26 (p.104): (een deelverzameling $D$ van $V$ is...) lineaire afhankelijk (niet vrij); lineair onafhankelijke (vrij); "er bestaat een vector in $D$ die een lineaire combinatie is van de andere"; opgelet, dit kan oneindige deelverzameling $D$ zijn! 
    \item definitie 3.31: "voortbrengend" (kan eindig of oneindig zijn!); vct($D$)=$V$ (gelijkheid! geen grotere verzameling!) ; notatie met opsomming tot $v_n$ alleen voor eindige! 
    \item definitie 3.33: "basis $\beta$". (kan eindig of oneidig zijn!) ;notatie met opsomming tot $v_n$ alleen voor eindige! 
    \item definitie 3.38: dimensie van $V$ met eindige basis; en dimensie van $\{ 0\}$ is per definitie $0$
    \item definitie 3.58: complementaire deelruimte
    \item definitie 3.61: rijruimte / kolomruimte / nulruimte 
    \item definitie 3.65: TODO over nadenken 
\end{itemize}

\subsection{Stellingen}
\begin{itemize}
    \item stelling 3.11: deelruimte criterium; niet vergeten: NIET leeg. (en uitaard: elke lin. combin. opnieuw in deelverz.) 
    \item propositie 3.14: doorsnede van 2 deelruimten is opnieuw deelruimte
    \item propositie 3.23: een somruimte is een direct som $\Leftrightarrow$ (het is somruimte) en (doorsnede is $\{0\}$); deel 2: voor meer dan twee verzamelingen, moet de doorsnede van elke afzonderlijke met somruimte van alle overige alleen nulvector bevatten! 
    \item propositie 3.27: $D$ is vrij $\leftrightarrow$ de enige LC van $v_i \in D$ die nulvector geeft, is de LC met alle co\"effici\"enten nul ; 
    \item stelling 3.35: Lemma van Steinitz. reeds voortbrengend deel met m elementen, dan is elke deelverzameling met meer elementen LA; vrij deel met m elementen, dan is elke deelverz. met minder dan m elementen niet voortbrengend. 
    \item gevolg 3.36: als je een willekeurig voortbrengend deel van $m$ elementen hebt, en een willekeurig vrij deel van $n$ elementen, dan geldt: $n \leq m$. 
    \item gevolg 3.37: als er een basis bestaat met $n$ vectoren, dan heeft elke andere basis ook $n$ vectoren
    \item stelling 3.40: elke vrije deelverzameling kan uitgebreid worden tot basis, elke eindige voortbrengende verzameling kan gereduceerd worden tot een basis. 
    \item stelling 3.44: triviaal maar praktisch interessant
    \item stelling 3.45: hetzelfde maar ook voor $\infty$ basis ; interessant bewijs! 
    \item stelling 3.47: 
    \item stelling 3.49: elke vector $v$ is unieke lin. combin. van basisvectoren; co\"ordinaatafbeelding bepaald door basis $\beta$. (definitie co\"ordinaatvector van $v$ ten opzichte van $\beta$) 
    \item stelling 3.53: dim($U+W$) + dim($U \cap W$) = dim($U$) + dim($W$)
    \item stelling 3.56: als $V= U_1 \bigoplus U_2 \bigoplus ...$ dan dim $V = \sum \text{dim} U_i $
    \item stelling 3.60: eindige dimensie: elke deelruimte heeft een complementaire deelruimte 
    \item stelling 3.64: nulruimte blijft behouden bij rijreduceren/gauss-elim. ; en ook de rijruimte ; en dim $N(A)$ + dim $R(A)$ $=n$ met $n$ aantal kolommen 
\end{itemize}

\subsection{Redeneringen, inzichten}

Commutatieve groep is nooit leeg (neutraal element nodig). Dus is vectorruimte nooit leeg. 

TODO: waarom is (C,R,+) geen vectorruimte? Is het omdat elke $\lambda \cdot v$ erin moet zitten?? 

Een deelruimte moet altijd NE bevatten (de nulvector dus). Omgekeerd: elke deelverzameling zonder de nulvector kan nooit een deelruimte zijn. 

Veel bewijzen in dit hoofdstuk zijn niet zo simpel, dus goed op oefenen, studeren. 

Een deelverzameling $D$ met de nulvector erin, is nooit vrij. 

Techniek om na te gaan of vectoren in $\mathbb{R}^3$ lin.onafh. zijn: kan via stelsel (homogeen). Meer algemeen: onderzoek naar oplossingen van homogeen stelsel van 1e graadsvergelijkingen. 
Techniek om na te gaan of vectoren in $\mathbb{R}^3$ voortbrengend zijn: kunnen ze willekeurige vector opleveren, is ook stelsel oplossen (zelfde stelsel als voor LO, maar niet homogeen) 

Bij bewijs van Steinitz staat: je kan inzien dat deel (2) vd stelling equivalent is met deel (1): dit is contrapositie. 

Deel (1) in contrapositie: 
$\neg$(elke deelverz. $> m$ elementen is niet vrij) $\implies$ $\neg$(er bestaat voortbrengend deel met m elementen)

is equivalent met: 

(er bestaat een deelverzameling met $n>m$ elementen die vrij is) $\implies$ (alle deelverzamelingen met $m$ elementen zijn niet voortbrengend)

TODO: die stelling 3.40: was dat letterlijk de examenvraag? 

TODO: bewijs van stelling 3.53: best moeilijk bewijs. 

TODO: paragraaf 3.5.2 lezen 


Belangrijke bedenking: als $\beta$ een basis is van $V$ (eindig-dim), dan kunnen basissen van deelruimten NIET altijd gevonden worden door te schrappen in $\beta$! $\beta$ is namelijk niet voortbrengend voor de deelruimten! Te veel voortbrengen is NIET voorbrengen (zie definitie voortbrengend); voorbeeld: het vlak, eenheidsvectoren op X-as en Y-as, en deelruimte diagonaal y=x. 
Dit is van groot belang bij het bewijs van de dimensiestelling, je kan geen basis van de kern van $L$ vinden door te schrappen uit basis van $V$!! 

% H4
\section{H4 Lineaire afbeelingen en transformaties}

\subsection{Definities, notaties}
\begin{itemize}
    \item lineaire transformatie
    \item def. 4.16: isomorifsme = $L$ die bijectief is; notatie $V \cong W$ als er zo'n $L$ bestaat
    \item definitie 4.31: kern (ker $L$) en beeld (Im $L$) , en rang van $L$! (=dim van Im $L$) ; opgelet rang hoeft niet eindig! 
\end{itemize}

\subsection{Stellingen}
\begin{itemize}
    \item Stelling 4.13: afbeelding van $Hom_{\mathbb{R}}(V,W)$ naar $\mathbb{R}^{m \times n}$. Na kiezen basis. 
    \item stelling 4.14: samenstelling van L en K ook lineaire afbeelding. 
    \item stelling 4.18: not shocking 
    \item stelling 4.20: (eindig dim.) $V \cong W$ asa dim $V$ = dim $W$ ! grappig wel; 1 getal, de dimensie is genoeg om een isomorfisme te hebben
    \item prop. 4.23: $L$ is isomorfisme asa $A$ inverteerbaar. (en matrix van inverse $L^{-1}$ is de inverse matrix) 
    \item prop. 4.32: Im $L$ = vct $\{ L(v_1), ... \} = $ vct $(\beta)$ ;logisch maar moet ik wel goed in hoofd prenten. 
    \item stelling 4.34: $L$ is injectief asa ker $L$ = $\{ 0 \}$. 
    \item stelling 4.36: goed te kennen (o.a. oef 4 p. 186)  
\end{itemize}

\subsection{Redeneringen, inzichten}

TODO: oef. 3.6.11 herbekijken; moet ook voor oneindig-dimensionale vectorruimte of deelruimte kunnen bewezen worden, denk ik; 
als vrij, dan hoogstens 1 manier: lijkt me easy (geval $v \in vct \text{ vs } v \notin vct$. 
Maar omgekeerde geval: ?? 

p.158 opdracht 1: lukte me niet 

p.148 gevolg 4.3 (3): lineaire afbeelding ligt volledig vast door beelden van de basis; maar wat wil dat zeggen? mag je willekeurige beelden $w_i$ uit $W$ aan een basis van $V$ toekennen en dan 
besluiten dat $L$ een lineaire afbeelding is? ergens was er oefening hierover... 

Vraag: als $v_i$ basis vormen, is dan de verzameling $L(v_i)$ ook basis? Ze is in ieder geval LO denk ik, maar to be seen. 

Zeker onthouden: $L_{\beta_V}^{\beta_W}$: kolommen zijn coordinaatvectoren tov $\beta_W$ van de beelden van de basisvectoren uit $\beta_V$. Vooral $V$ en $W$ niet omdraaien hierin, goed inprenten, of goeie kapstok vinden om te onthouden. 

Notaties: $Y = A \cdot X$ met $X$ coord-vector van $v$ en $Y$ coord-vector van $L(v)$ ; 
$v_i$ zijn $n$ basisvectoren van $V$
$w_i$ zijn $m$ basisvectoren van $W$ 
$L(v) = \sum y_i w_i$; 
elke $v = (v_1 ... v_n) \begin{pmatrix} x_1 \\ ... \\x_n \end{pmatrix}$ ; en $L(v) = (w_1 ... w_m) \begin{pmatrix} y_1 \\ ... \\y_m \end{pmatrix}$. 
Ook: $L(v) = ( L(v_1) ... L(v_n) \begin{pmatrix} x_1 \\ ... \\x_m \end{pmatrix}$? 
TODO: dit nog verder uitwerken? Met ook A er verder in gewerkt, en zien dat ik goed en snel kan reproduceren? 

p.156 figuur! 

Truuk om niet in de war te geraken: goed onderscheid maken tussen vector uit $\mathbb{R}^n$ met "re\"ele getallen" op een rij/kolom, en anderzijds een co\"ordinaatvector, die altijd moet gezien worden als coeff. van lin. combinatie van basisvectoren. 

Vraag: welk matrices uit $\mathbb{R}^{m \times n} $ komen overeen met een $L$? Allemaal? Kan ik matrix verzinnen en zeggen: dit is mijn $L_A$ ? En wat betekent dat dan? Tov welke basis? Of kan dit met eender welke basis gecombineerd worden?? 
Wat we weten: gegeven een $L$, een $\beta$ en een $\beta'$: we kunnen altijd de matrix bepalen, er is een techniek hiervoor. 

Vectorruimte van de lineaire afbeeldingen: nog een abstractie verder: alle $L$ van een gegeven $V$ naar een gegeven $W$ vormen zelf een vectorruimte. 
Definitie 4.12: $Hom_{\mathbb{R}}(V,W)$. Homomorfisme. Alle LA van $V$ naar $W$. Je kan hiermee ook bewerkingen doen, dus de LA als vectoren beschouwen. 
En met gekozen basissen zijn het eigenlijk gewoon matrices... 
(LA als abstractie van een matrix? of omgekeerd? ik denk het eerste in de les gezegd) 

Bij matrices van LA: het moet gaan over eindig-dimensionale vectorruimte. dim $V = n$. dim $W = m$. $K+L$ correspondeert met $A+B$, en $\lambda K$ met $\lambda A$. 

Linaire transformatie: vierkante matrix

Als $L$ surjectief is (ttz alle $w$ worden bereikt door $L$), dan is $W$ voorgebracht door de $L(v_i)$. (opdracht 4.19). 
Als $L$ injectief is (ttz elke $v$ heeft unieke $L(v)$), zijn de $L(v_i)$ vrij. 
Als $L$ beide is (bijectief, maw een isomorfisme): dan is $L(\beta)$ een basis van $W$. En $V$ en $W$ hebben dezelfde dimensie geloof ik, kan bijna niet anders. 

TODO: voor mezelf verschillende makkelijk te onthouden voorbeelden van $L$ bedenken die surjectief, injectief, bijectief zijn etc., en erover nadenken, matrices erbij, basissen erbij, ... 

TODO: opdracht 4.22, alle rijen van re\"ele getallen als vectoren; rijen met eindig aantal; waarmee isomorf? 

Sectie 4.4 basisverandering. Op basis van $Id$ afbeelding. Want vectoren blijven zelfde, alleen coord. veranderen! $Id_{\beta}^{\beta'}$. 

Basisverandering matrix: vaak als $P$ aangeduid. En inverse als $P^{-1}$. Is dit altijd inverteerbaar? Is dit altijd vierkant? Definitie gelijkvormige matrices... Ze stellen dezelfde $L$ voor, maar in andere basis. 
Als ene set basisvectoren een standaardbasis is, is ene matrix heel simpel op te schrijven, en andere door inverteren te berekenen. 

Opmerking 4.35: voor EINDIG-dim $V$ en $W$ : verzameling van alle coord. van ker $L$ = nulruimte $N(A)$ van $A = L_{\alpha}^{\beta}$. En gelijkaardig beeld van $L$ en kolomruimte $C(A)$! 

Kern is deelruimte van $V$. Beeld is deelruimte van $W$. 

Dimensiestelling. dim $V$ = dim ker $L$ + dim Im $L$/ 

TODO: praktisch, vlot leren $N(A)$ te vinden incl. basis ervoor, en $C(A)$ en basis ervoor. (herhaling hoofdstuk matrices/vectorruimten??) 

TODO Oef. 9 p. 187: ik zie het niet

oef. 17 p. 189: nog niet kunnen afwerken, altijd in de war met die rij/kolomvectoren, Id, basissen etc. 

Ik stelde me ooit de vraag of elke matrix $A \in \mathbb{R}^{m \times n}$ de voorstelling is van een lineaire transformatie. Natuurlijk is dat zo: gewoon $Y = A \cdot X$ kan altijd gezien worden als afbeelding die $X \ \in \mathbb{R}^n$ afbeeldt op $Y \in \mathbb{R}^m$, en deze afbeelding is lineair (matrixproduct distributief, en scalairen kunnen binnen en buiten gebracht worden). 
Omgekeerd is ook elke $L : V \to W$ als matrix te bekijken door met co\"ordinaten te werken. 

Als elke willekeurige matrix een $L$ voorstelt, wat dan met willekeurige inverteerbare matrices? Inverteerbare matrices $n \times n$ hebben sowieso rijen en kolommen die lineair onafhankelijk zijn (zie die samenvattende stellingen ergens einde H3 geloof ik). Dus de kolommen van een inverteerbare matrix vormen steeds een basis voor $\mathbb{R}^n$. 
Maar als kolommen van een matrix een basis vormen, dan stelt deze matrix eigenlijk ook een basisverandering voor op $\mathbb{R}^n$ van basis gegeven door de kolommen (bvb. $\alpha$) naar de standaardbasis $\epsilon$. Dus elke inverteerbare matrix $A$ is eigenlijk een basistransformatie $Id_{\alpha}^{\epsilon}$ met $\alpha=\{ \text{kolommen van A} \}$ en $\epsilon$ standaardbasis. 
Inderdaad, geven we co\"ordinaat $(1,0,0...)$ aan deze basisverandering, dan komt er kolom 1 uit, wat de coordinaat tov $\epsilon$ van het beeld van de eerste basisvector van  $\alpha$ is, wat klopt want de co\"effici\"enten van een kolommatrix zijn gewoon de coordinaten tov van de standaardbasis. Wel verwarrend altijd, die coordinaten en die co\"effici\"enten door mekaar. 
De omgekeerde basistransformatie is dan $A^{-1}$. 
Samengevat: elke inverteerbare matrix kan gezien worden als een basistransformatiematrix van $\alpha$ naar de standaardbasis. Of omgekeerd (want toch inverteerbaar). Verderop gebruik ik dit nog bij diagonaliseerbaarheid. 



% H5
\section{H5 Eigenwaarden, eigenvectoren, diagonaliseerbaarheid}

Intro: bij een $L$ op $V$ ziet de matrix $A$ er anders uit naargelang de basiskeuze. Als de basis zodanig kan gekozen worden dat de matrix $A$ een diagonaalmatrix is, zou dat "ideaal" zijn. Bvb. product van twee diagonaalmatrices (of kwadraat van een diag.matrix) zijn heel eenvoudig te berekenen. 

We hebben bij basisveranderingen gezien dat de nieuwe matrix er als volgt uitziet: $B=P^{-1}AP$ dus als we dat zodanig kunnen dat $B$ een diagonaalmatrix is, dan noemen we enerzijds $A$ diagonaliseerbaar (en bijhorende $L$), en weten we ook dat we nog steeds dezelfde transformatie hebben, alleen met coordinaten ten op zichte van een andere basis. 

Bij het uitrekenen wanneer een $P^{-1}AP=\Lambda$ diagonaal is (dan is namelijk $AP=P\Lambda$, en kan je wat met kolommen van $P$ redeneren), kom je automatisch op de eigenwaarde- en eigenvectordefinities. 


\subsection{Definities, notaties}
\begin{itemize}
    \item eigenwaarde en eigenvectoren horende bij die eigenwaarde (van een matrix $A$): $Av=\lambda v$
    \item karakteristieke veelterm van een matrix $A$
    \item eigenwaarde en bijhorende eigenvectoren  van ee lineaire transformatie $L(v)=\lambda v$
    \item definitie spectrum van $L:V \to V$, Spec($L$) $\subset \mathbb{R}$
    \item definitie diagonaliseerbare matrix: er bestaat een inverteerbare $P$ zodat ... 
    \item definitie diagonaliseerbare transformatie $L$ op $V$: er bestaat een basis van $V$ die volledig bestaat uit eigenvectoren van $L$ (!!); heel andere definitie dus; niet via bijhorende matrix of zo. 
    \item algebra\"ische multipliciteit  $m(\lambda)$, en meetkundige $d(\lambda) $; om te onthouden: m multipliciteit, en d dimensie (en dimensie is een meetkundig gegeven) 
    \item enkelvoudig spectrum: spectrum van een $L$ op $V$ met dim $n$ is enkelvoudig als \#Spec($L$)=$n$ (dus precies $n$ verschillende eigenwaarden) 
\end{itemize}

\subsection{Stellingen}
\begin{itemize}
    \item de enige eigenwaarden $\lambda$ van matrix $A$ zijn de nulpunten van de karakteristieke veelterm
    \item stelling 5.8: $A$ diagonaliseerbaar asa $\mathbb{R}^n$ heeft een basis die volledig bestaat uit eigenvectoren van $A$ / bewijs al best pittig om te volgen! (zie nota's redeneringen) 
    \item stelling 5.16: makkelijker te onthouden notatie dan in boek:   $1 \leq d(\lambda) \leq m(\lambda)$
    \item stelling 5.18: eigenvectoren bij verschillende eigenwaarden zijn lineair onafhankelijk 
    \item p.215: $L$ met enkelvoudig spectrum is diagonaliseerbaar
    \item lemma 5.22: als $L$ diagonaliseerbaar, dan moet $\phi_L$ volledig ontbinden als product van eerstegraadstermen; bewijs niet moeilijk, maar vraagt toch mind-flex. 
    \item stelling ?.? : gegeven $V$ met dim $n$, een $L$ op $V$, en een $\phi_L$ met $n$ nulpunten in $\mathbb{R}$: L is diagonaliseerbaar \emph{asa} voor alle $\lambda_i$ geldt dat $m(\lambda_i) = d(\lambda_i) $
    \item ergens een stelling dat gelijkvormige matrices dezelfde eigenwaarden, eigenvectoren en wat meer hebben!! 
\end{itemize}

\subsection{Redeneringen, inzichten}

Eigenvectoren altijd niet-nul. Eigenwaarden kunnen nul zijn. (bijhorende eigenvectoren zijn oplossingen van homogeen stelsel). 

Scalair veelvoud van een eigenvector is steeds een andere eigenvector bij dezelfde eigenwaarde. 

Twee verschillende definities: eigenwaarde matrix en eigenwaarde lineaire transformatie. (hoewel je al kan aanvoelen dat ze overeen gaan komen). 

Wat nadenken over spiegelingen, rotaties etc. kan zeker geen kwaad! Spiegeling tov bisectrice (y=x). Als $y=x$ : die vectoren op zichzelf afgebeeld, dus eigenwaarde 1. Andere vectoren $(x,y)$ afgebeeld op $(-x,-y)$, namelijk de loodlijn door 0 op die bisectrice, dus er is een eigenwaarde -1 ook. Ander voorbeeld: projecties van hele ruimte op vlak of as. 

$A=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$: 
  Spec($A$) = $\{\lambda_1=1,\lambda_2=-1\}$,
  $E_{\lambda_1}=\{ r \cdot \begin{pmatrix} 1\\ 1 \end{pmatrix}  \}$, 
  $E_{\lambda_2}=\{ s \cdot \begin{pmatrix} -1\\ 1 \end{pmatrix}  \}$

$B=\begin{pmatrix} 0&-1 \\ 1&0 \end{pmatrix}$: 
  Spec($B$) = $\{  \lambda_1=i,  \lambda_2=-i \}$,  
  $E_{\lambda_1} = \{ c \cdot \begin{pmatrix} i\\ 1 \end{pmatrix}  \}$, 
  $E_{\lambda_2} = \{ d \cdot \begin{pmatrix} -i\\ 1 \end{pmatrix}  \}   $


$C=\begin{pmatrix} 1&1 \\ 0&1 \end{pmatrix}$: 
  Spec($C$) = $\{ \lambda_1=1 \}$. 
  $E_{\lambda_1} = \{ r \cdot \begin{pmatrix} 1\\ 0 \end{pmatrix}  \}$, dim $E_{\lambda_1}=1$ 
  
$D= \mathbb{I}_2 = \begin{pmatrix} 1&0 \\ 0&1 \end{pmatrix}$: 
  Spec($D$) = $\{ \lambda_1=1 \}$. 
  $E_{\lambda_1} = \{ \begin{pmatrix} r\\ s \end{pmatrix}  \}$, dim $E_{\lambda_1} = 2 $



Als $Ap_i = \lambda_i p_i$, waarom zijn deze kolommen $p_i$ dan een basis van $\mathbb{R}^n$? Kolommen van een inverteerbare matrix zijn altijd lineair onafhankelijk. En aangezien er $n$ zijn, is het een basis. (nodig in bewijs stelling 5.8). 

Als een vector $v$ in $E_{\lambda_1}$ zit, kan die niet in $E_{\lambda_2}$ zitten, en omgekeerd. Dit is evident omdat $L(v)$ niet tegelijk gelijk kan zijn aan $\lambda_1 v$ en aan $\lambda_2 v$ met verschillende eigenwaarden. 

$E_{\lambda}$ , de eigenruimte, is wel degelijk een deelruimte! Ik dacht eerst dat er geen $0$ in zat, maar er zit zeker wel een $0$ in, want het is per definitie een ruimte. Ook de ruimte met basis $\{ (1,0), (0,1) \}$ heeft $0$ erin natuurlijk... 


In de les iets gezegd van: lineaire afbeeldingen zijn een abstractie van matrices. Snap niet helemaal waarom lineaire afbeeldingen abstracter zijn dan matrices, ik vind ze eigenlijk concreter. 

Uit theorieles: wanneer heeft $A$ of $L$ genoeg eigenvectoren? Genoeg in de zin van: om een basis te vormen, dus $n$ lineair onafhankelijke eigenvectoren. En dit is dezelfde vraag als: wanneer is $A$ of $L$ diagonaliseerbaar. En er zijn twee obstakels: eerst moeten we genoeg nulpunten hebben van $\phi_A$ zodat we genoeg eigenwaarden hebben; en dan als er genoeg eigenwaarden zijn, moeten we nog een basis vinden van eigenvectoren. 
(TODO: vraag: waarom moeten we genoeg eigenwaarden hebben? waarom niet binnen eenzelfde eigenwaarde eigenvectoren te vinden? ik denk dat Vaes zijn formulering een beetje te vaag is, en je eigenlijk met 1 eigenwaarde nog altijd kan diagonaliseren, zie eenheidsmatrix; betere formulering zou zijn: we moeten genoeg multipliciteit hebben). 

Redenering: vectorruimte $V$ met dimensie $n$, karakteristiek vergelijking van bijhorende matrix heeft maximum graad $n$, dus er zijn maximum $n$ re\"ele nulpunten (en exact $n$ complexe nulpunten). Dus de som van de algebraische multipliciteiten is maximaal $n$: $m(\lambda_1) + m(\lambda_2) + ... + m(\lambda_k) \leq n $ (en $=n$ bij complexe). 

In de les een variant op stelling 5.8 gezien: als we een $L: V \to V$ hebben en een basis $\beta=\{ v_1, v_2, ... v_n \}$. Dan bestaat $\beta$ uit eigenvectoren van $L$ \emph{asa} $L_\beta^\beta$ is een diagonaalmatrix. Bewijs loopt als volgt: kolommen van $L_\beta^\beta$ zijn de co\"ordinaten van de beelden van de basisvectoren $v_i$. Maar als $v_i$ eigenvectoren zijn, dan zijn de geldt voor elke $i$: $L(v_i) = \lambda_i v_i$. En dus is de co\"ordinaat $(0, ... \lambda_i, ... 0)$. Dus staan die co\"ordinaten als kolommen in $L_\beta^\beta$ en hebben we een diagonaalmatrix. 

Enkelvoudig spectrum: uit definitie volgt rechtstreeks dat alle $m(\lambda_i)=1$. 

Als enkelvoudig spectrum, dan diagonaliseerbaar (een stelling of Lemma p.215). Maar omgekeerd: indien geen enkelvoudig spectrum?? Wel dan kan alles nog... bvb. eenheidmatrix heeft geen enkelvoudig spectrum, maar is al diagonaal dus zeker diagonaliseerbaar... 

Complexe vectorruimten, complexe eigenwaarden/eigenvectoren: $\mathbb{C}$. Definities $A \in \mathbb{C}^{n \times n}$ diagonaliseerbaar over $\mathbb{C}$, $L: (\mathbb{C},V,+) \to (\mathbb{C},V,+)$ diagonaliseerbaar, en voorwaarde \emph{asa} $d(\lambda_i) = m(\lambda_i)$: eigenlijk hetzelfde als re\"ele varianten. Maar scalairen zijn dan complex (dus ook vermenigvuldigen met $i$ van bvb vector $\begin{pmatrix} 1\\ 0 \end{pmatrix}$ is een scalair veelvoud! 

Ik probeer in eigen woorden het verband te zien tussen de definitie van een diagonaliseerbare matrix en een diagonaliseerbare transformatie. $A$ diagonaliseerbaar, is per definitie het bestaan van een $P, P^{-1}$ zodat $ P^{-1}AP= \Lambda$. 
Een diagonaliseerbare $L$ betekent het bestaan van een basis $\beta$ in $V$ bestaande uit eigenvectoren van $L$. Maar het bestaan van een basis bestaande uit eigenvectoren, betekent net hetzelfde als zeggen dat de matrix van $L$ tov. basis $\beta$ een diagonaalmatrix is: $L_{\beta}^{\beta}$ = \begin{pmatrix} \lambda_1 & 0 &... \\ 0 & \lambda_2 & ... \\ &...&\\ ... & 0 & \lambda_n \end{pmatrix}. (lemma in de les gezien als intermezzo). 
Nu kunnen we bij een $L$ altijd onze basis kiezen, en aan de hand daarvan de matrix van $L$ opschrijven. Kiezen we voor dezelfde diagonaliseerbare $L$ een andere basis $\alpha$, dan weten we dat $L_{\alpha}^{\alpha}$ en $L_{\beta}^{\beta}$ dezelfde lineaire transformatie voorstellen, maar ten opzichte van een andere basis. Deze basisverandering is ook een transformatie, en is bijectief, dus is er een inverteerbare matrix $P$ zodat $L_{\alpha}^{\alpha} = P^{-1} L_{\beta}^{\beta} P $. Dat wil ook zeggen dat matrix $L_{\alpha}^{\alpha}$ diagonaliseerbaar is. Dus voor willekeurige basis $\alpha$ is de matrix van een diagonaliseerbare transformatie $L$ ook diagonaliseerbaar. 
Omgekeerd: nemen we een willekeurige matrix $A$ die diagonaliseerbaar is. Dan is $A = P^{-1} \Lambda P$ of $\Lambda=P A P^{-1}  $. Deze matrix $A$ stelt ook een lineaire transformatie voor $X \mapsto AX$ op $\mathbb{R}^n$. En elke inverteerbare matrix $P$ kan gezien worden als een basistransformatiematrix van $\alpha$ naar de standaardbasis of omgekeerd (zie eerder mijn redenering ergens).  Dus onze $\Lambda$ stelt dezelfde $L$ voor als onze $A$, maar ten opzichte van een andere basis. Wegens ons intermezzo (er bestaat basis uit eigenvectoren asa matrix $L$ tov deze basis is diagonaalmatrix): onze $L$ is ook diagonaliseerbaar. 





=================
p.231: oef. 1a: truuk nodig; $Av=\lambda v$ dus $AAv= A\lambda v$ dus ... $\lambda v = \lambda^2 v$, ofte  $\lambda v - \lambda^2 v = 0$ voor niet-nul $v$, dus $\lambda(1-\lambda)=0$

oef. 1b: $Av=\lambda v$ dus $A^{-1}Av = A^{-1} \lambda v$, ... $\frac{1}{\lambda}v= A^{-1}v$, dus ... 




% H6 
\section{Inproductruimten en Euclidische ruimten}



Hoogtepunt cursus: p.273: spectraalstelling i.v.m symnmetrische of Hermitische matrices. 
Legt verband tussen: 

\begin{itemize}
    \item Euclidische ruimte (dus vectorruimte met inproduct)
    \item lineaire transformatie
    \item matrices
    \item orthonormale basis
    \item eigenvectoren
\end{itemize}


\subsection{Definities, notaties}
\begin{itemize}
    \item 
\end{itemize}

\subsection{Stellingen}
\begin{itemize}
    \item 
\end{itemize}

\subsection{Redeneringen, inzichten}


\section{Overkoepelende beschouwingen}

\subsection{Matrix}

\begin{itemize}
    \item tabel getallen $m$ bij $n$
    \item m rijen of ook: m rijvectoren uit $\mathbb{R}^n$
    \item n kolommen of ook: n kolomvectoren uit $\mathbb{R}^m$
    \item model van lineair stelsel (homogeen of uitgebreide matrix); 0, 1 of oneindig veel oplossingen       
    \item $n \times n$: inverteerbaar asa det $\neq 0$ asa rijvectoren LO asa kolomvectoren LO asa rang $A=n$ 
    \item zelf element van vectorruimte  $\mathbb{R}^{m \times n}$ 
    \item rijen brengen rijruimte $R(A)$ voort, kolommen de kolomruimte $C(A)$ 
    \item oplossingen homogeen stelsel vormen nulruimte $N(A)$ 
    \item rang matrix = min ( aantal LO rijen, aantal LO kolommen) 
    \item als lineaire afbeelding: $X \mapsto A \cdot X$ (of lin. transformatie indien $n \times n$) 
\end{itemize}

Rang van een matrix, met nieuwe info in H4 na definitie LA : ergens beetje eigenaardig dat "de rang" van een matrix pas wordt aangetoond als lineaire afbeeldingen zijn behandeld. 
\begin{itemize}
    \item H3: kolomrang en rijrang gedefineerd (als dim van $C(A)$ resp. $R(A)$) 
    \item H3: dimensiestelling matrix en geassocieerde vectorruimtes: dim $N(A)$ + dim $R(A)$ = $n$ ($n$ = aantal kolommen) 
    \item H4: dimensiestelling LA: dim $V$ = dim(ker $L$) + dim Im $L$, met dim $V=n$=aantal kolommen bijhorende matrix, dim ker $L$ = dim $N(A)$, en dim Im $L$ per definitie rang van $L$, maar ook dim Im $L$ = $C(A)$ = kolomrang
    \item H4: uit voorgaande: dim $N(A)$ + rijrang = n = dim $N(A)$ + kolomrang ; dus kolomrang = rijrang = rang afbeelding $L_A$  
\end{itemize}

\subsection{Lineaire afbeelding}
\begin{itemize}
    \item matrix voorstelling ervan als er basis(sen) zijn gekozen (bij lin trans op $V$ is 1 basis genoeg)
    \item basisverandering binnen 1 ruimte is ook een transformatie, eigenlijk de identieke $Id$ 
    \item $L$ is bijectief (en dus isomorfisme) asa matrixvoorstelling is inverteerbaar     
    
\end{itemize}


\end{document}