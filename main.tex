\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{Nota's lineaire algebra}
\author{Raf Meeusen}
\date{2023-2024}

% mark paragraphs with empty line instead of indented first line
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section{Les 1, 26 sept}

Toepassingen matrices: 
\begin{itemize}
\item google search (eigenwaarden/eigenvectoren worden berekend) 
\item jpeg compressie (projecties uit lin. alg.) 
\end{itemize}


Lineair: zelfs voor niet-lineaire problemen wordt vaak een lineaire benadering (ook: eerste orde benadering) gebruikt voor een oplossing; bvb: slinger waarbij sin theta = theta wordt gesteld

Examen: 
\begin{itemize}
\item duurt 3 uur
\item gesloten boek
\item schriftelijk
\item 2 oefeningen + 1 theorievraag
\item er zijn voorbeeldexamens 
\end{itemize}

Boek: editie 2 ook bruikbaar ; maar er is een editie 3 


Makkelijke verbanden tussen lin. alg. en andere topics: 
\begin{itemize}
    \item fysisch netwerk met knooppunten (elek, stroom auto's, vloeistof): knooppuntvergelijkingen geven stelsel; stelsel = lin. algebra
    \item vlakke meetkunde: 2 vgln. stellen rechte voor; stelsel = alle punten die op beide rechten liggen
    \item Ruimte-meetkunde: vergelijkbaar, vlakken / rechten
\end{itemize}

Vrijheidsgraden als er oneindig veel oplossingen zijn. Oplossingen te schrijven in functie van "vrije parameters". 

"operaties die de oplossingen van een stelsel bewaren" 
(zie ook oefening nabeschouwing met S en S') 
Deze operaties zijn per definitie omkeerbaar.  

Termen: 
\begin{itemize}
\item uitgebreide matrix van een stelsel (met de $b_i$) 
\item achterwaartse subsitutie (bij oplossen stelsel) 
\end{itemize}

Stelsels waarbij oplossing bijna rechtstreeks af te lezen is:  voorbeelden gezien in de les. Allemaal met een matrix die in specifieke vorm is. Dan link gelegd met echelonvorm / trapvorm. 

Termen:
\begin{itemize}
\item echelonvorm
\item trapvorm
\item gebonden variabelen (kolommen met leidende 1) 
\item vrije variabelen (zonder leidende 1); als parameter te kiezen
\item strijdig stelsel
\end{itemize}

Gewoonte: parameter $\lambda$, $\mu$, ...

Elk stelsel kan naar echelon geconverteerd worden. MAAR: een echelonvorm is NIET uniek. 

Methode: algoritme, technieken. 

Dan nog gezien: stelling 1.11. (ivm. 1 oplossing, 0 oplossingen, oneindig veel oplossingen)

Notatie: $\mathbb{R}^{nxk}$ , voor n rijen / k kolommen. 

Voorbeelden bewijzen eigenschappen matrices: onthouden dat een matrix defini{\"e}ren erop neerkomt om elk element te bepalen (notatie Aij = element op rij i en kolom j). 

Definitie: transponeren v/e matrix ($A^T$) . Is spiegelen over diagonaal. 

Termen:
\begin{itemize}
\item vierkante matrix
\item symmetrische matrix ($A = A^T$) ; moet dus vierkant zijn
\item diagonaalmatrix
\end{itemize}

\section{Les 1 nabeschouwing}

Oefening 1, S1 en S2 gemaakt. S3 begonnen, maar niet helemaal afgewerkt.

Oefening 2: gelezen, niet gemaakt. 

Oefening 3: 0, 1 of oneindig. Leg uit adhv prop 1.8 / st.1.11. Even opgezocht: prop 1.8: elke matrix is rij-equivalent met een matrix in echelonvorm, en zelfs met een matrix die rijgereduceerd is. (p. 21). Stelling 1.11: p. 24
stelling zegt dat een stelsel waarvan matrix in echelonvorm, altijd 0, 1 of oneindig oplossingen heeft, en geeft ook de voorwaarden adhv echelonvorm eigenschappen. 

Oefening 4: maak opdracht 1.2 in boek. "toon aan dat  elke vd elementaire rijoperaties ongedaan gemaakt kan worden door opnieuw...". Geef telkens correct notatie voor de inverse elem.rij-op. 

elementaire rij-operaties zijn: 
\begin{itemize}
\item $Ri \rightarrow  lambda * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri + lambda*Rj$
\end{itemize}

inverse: 
\begin{itemize}
\item $Ri \rightarrow  (1/lambda) * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri - lambda*Rj$
\end{itemize}

Oefening 5: gemaakt (zonder helemaal uit te schrijven) 
...
Stel daarom eerst dat ... oplossing is van S. Omdat 
$
(ai1 + lambda*aji) * x1 + ... 
= 
[ ai1 * x1 + ... + ain xn  ] + lambda* [ aj1 * x1 + ... ajn * xn] $
en dat we uit S weten dat 
eerste deel gelijk is aan $b_i$ 
en tweede deel gelijk aan $lambda*bj$
is ook aan deze vergelijking (nieuwe $Ri$) voldaan. 

Omgekeerd: stel dat $(x1,...xn)$ oplossing is van S' 
Omdat $lambda*(aj1x1 + ...) = b_j $ (zie j-de rij van S', en we hebben een oplossing),
kunnen we vergelijking van nieuwe Ri herschrijven door links en rechts gelijke delen te schrappen,
en blijft nog gewoon oude Ri uit S over. 

Oefening 6: 
eens gedaan op papier; niet heel moeilijk, maar wel opletten welke notatie juist welke betekenis heeft (volgorde van $_{ij}$ en $T$ en $lamba*$ etc.). 

Oefening 7 (extra): 
Vraag gesteld aan An Speelman: kolom van de constanten telt niet mee! 

kolomoperaties: 
\begin{itemize}
    \item kolommen verwisselen (die NIET de constanten zijn): gewoon variabelen van naam veranderen. 
    \item kolommen maal een lambda: 
kolom van niet constanten: uitkomst van corresponderende var is gedeeld door lambda
\item kolom plus lambda keer andere kolom: ik zie geen makkelijke redenering van wat er met oplossingen gebeurt. 

\end{itemize}




\section{Oefenzitting 1, 3 okt}

(10u30, lokaal 01.14; Tom geeft oefenzittingen). 

Oefeningen 1.7 pagina 47-54. 

Oefening 1 matrix A1 gemaakt. 
Oefening 2 matrix B1 gemaakt. 
Oefeningen 4a en 4b gemaakt. Denk wel juist. 
Oefening 5d gemaakt. 
Oefening 6d gemaakt, maar niet zo heel netjes opgeschreven. 
Oefening 10 linkse stelsel gemaakt. Denk redelijk juist gemaakt. 
Oefening 11b gemaakt. Wel juist in grote lijnen. 
Oefening 12 gemaakt. Maar niet goed aangepakt, en nog eens opnieuw gedaan. Kwam derdegraadvergelijking in b in, die je moet oplossen door nulpunt (b=1) te raden, en dan veelterm te delen. 



Oefening 14 begonnen. Maar gestopt, te veel schrijfwerk. 


\section{Les 2 voorbereiding}

Gelezen. 
2a) toon aan dat I.A = A = A.I . Voor 2x2. 
\[I = 
1  0
0  1 \]
\[
A = 
a11  a12 \\
a21 a22\]

(onthouden: rij vd linkse matrix x kolom vd rechtse matrix) 

\[(I x A)11 = 1.a11 + 0.a21 =a11\]
\[(I x A)12 = 1.a12 + 0.a22 = a12\]
\[(I x A)21 = 0.a11 + 1.a21  =a21\]
\[(I x A)22 = 0.a12 + 1.a22 =a22\]


idem voor andere. 

2b) 
Zoek voorbeeld van 2x2 matrices waarbij A . B niet gelijk is aan B. 
\[A 
1  0
1  1 
en
0  1
0  0 \]

heb ik eens uitgeteld; verschillend. 


Appendix over sommatieteken. 
TODO: Nieuwe notatie hier: $(ai)_{ni}=1$  
(bij puntje 1. ) 

bewijs de eigenschappen
\begin{itemize}
\item lambda/mu : OK, heb ik eens uitgeschreven
\item dubbele som van ai.bj : OK, heb ik ook eens uitgeschreven. 
\end{itemize}


herschrijf uitdr. met sommatieteken; OK kan ik 
omgekeerd, wat drukt .. uit  ; OK, kan ik 

toon aan dat ... : OK, gedaan; is wat schrijfwerk en lastig, maar geen rocket science



\section{Les 2, 3 okt}


Matrices vermenigvuldigen. Te onthouden: rij links, kolom rechts. Een rij x een kolom = 1 getal. (makkelijk te onthouden, want ik weet ook nog dat nxm matrix maal mxp matrix een nxp matrix geeft, en nxm matrix heeft n rijen met lengte m, en de lengte van de kolommen van de tweede is ook m. Logisch. Aantal rijen = lengte van de kolommen, en vice versa. 

% even vooruitlopen waarom dat matrixproduct zo gedefinieerd is: 
% 
%Functie $L_A(x)$ is een afbeelding van $\mathbf{R}$ $$ R^k naar R^n, met 
%L_A(x) = som (Aij . xj) 
%(elke matrix A definieert een afbeelding La van ...) 
%En dan is La . Lb = L_A.B 
%(en dit maakt de definitie van matrixproduct logisch) 


\begin{itemize}
    \item veel eigenschappen en rekenregels
    \item een nulmatrix $O_{nxk}$
    \item een eenheidsmatrix $I_n$ (vierkante matrix) 
    \item MAAR: vermenigvuldiging niet-commutatief (logisch want niet eens gedefinieerd voor alle nxk en kxl dimensies. 
    \item $AB$ kan nul zijn terwijl $A \neq O$ en $B \neq O$
\end{itemize}


Noot: als we een matrix zien als een "operatie", en een vermenigvuldiging van matrices zien als de opeenvolging van die twee operaties, dan vinden we het ook logisch dat de uitkomst van $AB$ niet zelfde is als $BA$. (schoenen aantrekken, dan kousen, of omgekeerd. 

Matrices waar volgorde van operaties wel omwisselbaar is: noemt men commuterende matrices. 

Vraag: kunnen we matrices delen? Aangezien product van matrices nul kan zijn zonder dat een van de matrices nul is, is het delen niet zo evident als bij re{\"e}le getallen. Bvb. uit $ab=0$ volgt, als $b=0$, dat $a=0$, omdat we mogen delen door $b$. 

Inverteren: een matrix vinden zodanig dat vermenigvuldiging de eenheidsmatrix geeft. Ook hier zijn er verschillende opties: de linkerinvers en de rechterinvers. 

Opmerking: we beperken ons hier tot vierkante matrices wat betreft inverse, hoewel je ook inverse kan berekenen voor niet-vierkante matrices. 

Stelling: voor een vierkante matrix die links inverse B heeft en rechts inverse C, geldt dat $B=C$. (bewijs in les gezien, heel kort). 

Definitie 1.33. We noemen een nxn matrix ofwel inverteerbaar, ofwel niet-inverteerbaar, indien ... Synoniemen: regulier, niet-singulier/singulier. Opgelet: op dit moment hebben we nog niet bewezen dat linker inverse en rechter inverse gelijk zijn! Dat komt later! 

Stelling 1.34: inverse van product (als beide inverteerbaar zijn) =  product van de inversen in omgekeerde volgorde. Het bewijs is gebaseerd op: aantonen dat een gegeven matrix een inverse van een andere, is bewijzen dat zowel links-vermenigvuldigen als rechts-vermenigvuldigen resulteert in de eenheidsmatrix. 

Noot: niet evident hier om de te volgen wat al bewezen is, en wat we al weten, maar nog niet bewezen is (en dus: wat volgt eigenlijk waaruit). Bvb. nog steeds niet bewezen dat linker en rechter inverse gelijk gaan zijn. 

Rap een eigenschap tussendoor: als $AB=0$ en $A$ is inverteerbaar, dan is $B=0$. (bewijs redelijk makkelijk) 

Term/concept: elementaire matrix $E_n$. Komt voort uit oplossen van stelsels met matrix rij-operaties, gecombineerd met matrix-vermenigvuldigen. We kunnen namelijk rij-operaties voorstellen als matrix-vermenigvuldiging met zogenaamde elementaire matrices. 

Interessant: twee rijen van plaats wisselen: ongedaan maken is opnieuw hetzelfde doen! Dus inverse van $E_n$ voor rijen verwisselen, is gewoon dezelfde $E_n$! 

Noteer ook: alle elementaire matrices zijn inverteerbaar. 

ERO: elementaire rij-operaties. Bij elke ERO hoort een EM (elementaire matrix), noemen we $\Sigma$, en die EM bekom je door rij-operaties toepassen op de eenheidsmatrix. ERO toepassen op matrix A komt overeen met matrix-vermenigvuldiging $\Sigma A$. Links-vermenigvuldigen dus. 

Noot: we weten dat twee rijen wisselen, ongedaan gemaakt wordt door opnieuw hetzelfde te doen. Dus: de matrix $\Sigma$ van rij-wissel operatie, moet wel de inverse van zichzelf zijn. 

Grote stelling: 1.39, met 6 equivalente beweringen. De matrix A heeft een links inverse B; het stelsel $AX=0$ heeft enkel de evidente oplossing $X=0$ (dus stelsel met 1 oplossing!); etc. etc. Puntje 4: matrix A is product van elementaire matrices, is een soort sleutel-formulering, en ook een sleutel in het grote bewijs. 

Bewijs: formulering 2 volgt uit 1 door links met B te vermenigvuldigen en uit te werken. Formulering 3 volgt uit 2, doordat stelsels met 1 oplossing in rij-echelon-vorm de eenheidmatrix hebben, en altijd daartoe te herleiden zijn met ERO's. Bewijs van 4 uit 3 (A is product van EM's): op bord geschreven tijdens de les. Ook redelijk makkelijk (moet wel weten dat EM's inverteerbaar zijn, en opnieuw EM geven). 

Tussenstelling: als $C$ inverteerbaar, en $CA=I_n$, dan is A inverteerbaar en $A^{-1}=C$. Het bewijs: als $CA=I_n$, dan is A rechterinvers van C, en dus $A=C^{-1}$

Noot: tijdens de les effe de draad kwijt tussen wat al bewezen was, en wat niet, en de definities inverteerbaar etc. Nog eens nakijken! 

Noot2: belangrijk is: als $AB=I_n$, dan kan je dat op 2 manieren lezen: A is linkerinvers van B, of B is rechterinvers van A. Die twee manieren van lezen heb je nodig om bewijs sluitend te krijgen. 

Bovendien volgt uit deze stelling een algoritme om de inverse te berekenen: doe dezelfde rijoperaties om A te rij-herleiden naar $I_n$, ook gewoon telkens op $I_n$, bijvoorbeeld door de matrix $(A|I_n)$ te gebruiken voor alle rij-operaties. Dan krijgen we rechts $A^{-1}$, want in bewijs hadden we gezien dat $A^{-1} = \Sigma_k \Sigma_{k-1} ... \Sigma_1$. Als bij A een nul-rij ontstaat, is A niet inverteerbaar. 

Dan nog even over LU-decompositie gehad. (lower / upper triangular = benedendriehoeksmatrix en bovendriehoeksmatrix). Benedendriehoeksmatrix heeft niet-nullen op en beneden diagonaal. Het kunnen schrijven van een matrix A als product van een L en een U, heeft interessante toepassingen. Ook interessant: product van twee bovendriehoekmatrices is opnieuw bovendriehoeks. Een diagonaalmatrix is zowel bovendriehoeks als benedendriehoeks. Kijk ook eens naar de EM's en of ze L of U zijn. Echelonvorm is bovendriehoeks. 

Ook "de spil" vernoemd hier ivm rij-herleiden. Dus ook leerstof (spil/gauss-eliminatie...). En volgende redenering: als je A in echelonvorm kan zetten zonder rijen te verwisselen, en we weten dat echelonvorm een U-vorm is. Dus $U= (\Sigma_k ... \Sigma_1) A$, met $\Sigma_k ... \Sigma_1$ inverteerbaar, en inverse hiervan is een L-vorm (indien geen rij-wissels). 

\section{Les 2 nabeschouwing}

1. stelsel herleiden naar trapvorm, dan elementaire matrices hiervoor opschrijven, en dan A herschrijven als product van elementaire (die dan inverse zijn van oorspronkelijke elementaire!) en de trapvorm zelf. 
OK, alleen twijfelde ik wat bij inverteren van de elementaire. Niet slecht om eens voorbeeld van op te schrijven (1/lambda, en +lamba Ri wordt -lambda Ri). 

2. stelling 1.38 bewijzen; had ik al gedaan voor paar getallenvoorbeelden, moet in deze oefening eigenlijk gewoon met variabele lambda en zo, dus heb ik geskipt. Wel goeie oefening om die inversen van elementairen eens uit te schrijven. 

3. Die bewijzen zijn veel schrijfwerk, en met al die verschillende indices en mxn, nxp etc. dimensies, ... Eerst eens proberen definitie 1.22 zelf op te schrijven zonder te spieken. 

4. Doenbaar, wat rekenwerk. Gevolg 1.40 is pagina 40. Maar nog niet gedaan. 

5. Oefening 1.35 uit boek (is op p. 53, idempotente matrix). Nog niet gedaan. 

\section{Les 3 voorbereiding}

A4 lesvoorbereiding, paar stukjes in boek lezen en nadenken. Duurde ongeveer halfuur. 


\section{Oefenzitting 2, 10 okt}

\subsection{Voorbereiding}

Oefening 1.7-19 p.51 gemaakt. Deel 2 $C C^T$ kwam symmetrische matrix uit. Misschien altijd zo? Element $c_{ij}$ van product komt uit rij $i$ van linkse matrix en kolom $j$ van rechts. Element $c_{ji}$ komt uit rij $j$ van linkse en kolom $i$ van rechts. Rij $i$ van linkse matrix is kolom $i$ van rechtse. En kolom $j$ van linkse is rij $j$ van rechtse matrix. Dus inderdaad altijd zo. 


Oefening 1.7-20 p.51. Volgens mij is de clou dat na herschrijven volgt dat $A-A^T = -2B^T$, dus dat we hieruit makkelijk $-2B^T$ kunnen halen, en dan ook $B$. 

Oefening 1.7-24 matrix A1 p.52. OK, inverse berekend met truuk $(A|I)$. Uitkomst: 
\begin{math}
\begin{pmatrix}
-\frac{2}{7} & 1 & \frac{6}{7}\\
-\frac{1}{7} & 1 & \frac{3}{7} \\
\frac{4}{7} & -1 & -\frac{5}{7} 
\end{pmatrix}
\end{math}

Nagekeken met online calculator, klopte. Ook nog eens gekeken en nagedacht hoe te schrijven als product van elementaire. Niet vergeten dat het ook inverse van elementaire worden op een gegeven moment! Niet helemaal uitgeschreven, maar ik snap het principe wel. 

Oefening 1.7-34 p.53. Ok, gemaakt. Deel a) met inductie en ook uit ongerijmde (als k-de macht verschillend van nul is, is ook (k+1)-de macht verschillend van nul), en dan gezien dat dat niet kan. Deel b) gewoon uitgerekend. Die inverse die voorgesteld wordt kan altijd berekend worden, dan vermendigvuldigd met $I-A$ en uitgerekend.


\subsection{Oefenzitting}

Oefening 27(a) p.52: OK uitgerekend, klopte. 

Opdracht 1.27 op p.34: OK, klopt na uitschrijven van Tr(A-B) en dan te hergroeperen. 

Oefening 1.7-21 p.51
21a) $AA^T$: OK. Element $c_{ij}$ uitschrijven als som, dan meepakken dat $b_{kj} = a_{jk}$ voor de getransponeerde, en dan ook eens element $c_{ji}$ uitschrijven, en dan zie je dat ze hetzelfde zijn. Matrix met $c_{ij} = c{ji}$ is een symmetrische. Voor $A+A^T$ is zelfs nog makkelijker. 
Andere manier: definitie van symmetrisch: $A=A^T$. Dus als we $(A+A^T)^T$ uitrekenen en opnieuw $A+A^T$ uitkomen, dan is deze inderdaad symmetrisch. En er is een eigenschap van de getransponeerde van een som, dus kan ook makkelijke zo aangetoond worden. Ook voor product moet dat waarschijnlijk zo kunnen (eigenschap $(AB)^T$), maar niet verder uitgewerkt. 
22b) is ook redelijk triviaal. Dan 22c), gewoon uitgerekend dat som klopt. Dan 22d), was wat moeilijker. Gesteld dat $A=B+C$ met $B=B^T$ en  $C=-C^T$. Dan kan je berekenen dat die B en C gelijk zijn aan de uitdrukkingen uit vorige deeloefening door uitdrukkingen voor $A$ en $A^T$ verder uit te schrijven en op te lossen naar $B$ en $C$. 
21e geskipt. 


Oefening 1.7-35 p.53. Idempotente matrix. Ene vinden: eenheidsmatrix, en eerste rij maal -1 doen. Of tweede rij, etc. Deel b is makkelijk te berekenen. 

Oefening 1.7-39 p.54. 39a is vals. Tegenvoorbeeld is ooit in de les gegeven. 39b is waar. Uit gegeven volgt dat $A=A^{-1}$ en $B=B^{-1}$. Dan is $(AB)^{-1} = (A^{-1}B^{-1})^{-1}$. Via eigenschap inverse van product volgt het gevraagde. 
39c: vals. Stel $B=-A$, dan is $A+B=0$; tegenvoorbeeld.
39d: waar. 

39e: moeilijk, ik zie hem niet. Blijkbaar vals, moet paar keer proberen met bvb. 2x2 matrix. 

39f: makkelijk, ongerijmde. Stel AB inverse wel, dan moet A wel inverteerbaar zijn. 

39g: vals. Bvb. twee rijen wisselen, en een rij met  lambda vermenigvuldigen; operaties omkeren, en het klopt niet meer. 

\section{Les 3, 10 okt}

Determinanten. Vorige les: verband inverteerbaarheid matrix en oplosbaarheid (uniek) van homogeen stelsel. Voor 2x2 matrix kan je matrix beginnen inverteren via rijoperaties, en bekom je dat matrix inverteerbaar is als en slechts als $ad-bc$ niet nul is, en we kunnen dit defini\"eren als de determinant-functie $f(A)$ van een 2x2 matrix. 

Paar eigenschappen van de 2x2 determinant: rijen van plaats wisselen, determinant van eenheidsmatrix is 1, twee matrices met verschillende eerste rij combineren. Deze drie eigenschappen worden als basis genomen van een algemene determinant-afbeelding. Zie boek. 

Eigenschappen van een determinantfunctie: er volgt direct dat die lineariteit ook geldt voor alle andere rijen (want je mag rijen wisselen, alleen maar minteken erbij). Nul-rij geeft nul-determinant. Twee gelijke rijen ook nul (teken verandert maar moet hetzelfde blijven, dus moet 0 zijn). 

Impact van elementaire rij-operaties (ERO) op een determinant-afbeelding: type III in stelling 2.3. Bewijs in de les gezien. Ook voor andere ERO's de impact gezien. 

Stelling 2.4 gezien, vier onderdelen gecombineerd in een stelling: driehoeksmatrices, verband met inverteerbaarheid, determinant van product van matrices, determinant van getransponeerde matrix (dus lineariteit geldt ook voor de kolommen!). In de les het bewijs gezien van deel 2 van de stelling (verband inverteerbaarheid). Ene richting bewijs via ontbinding elementaire matrices. Andere richting van bewijs via contrapositie, en boven- en benedendriehoeksmatrices. 

Gevolg 2.5: o.a. determinant van de inverse, ... 

Nu gaan we bewijzen dat determinant-afbeelding bestaat. Gaat via permutaties, dus eerst heleboel wiskunde rond permutaties, transposities, ... 

Notatie van permutatie $\sigma$ via matrix met bovenste rij (1 2 3 ...), en onderste rij de beelden onder $\sigma$. Demonstratie dat om van een gegeven  permutatie naar de identieke permutatie te gaan via "wissels": altijd oneven aantal wissels nodig. Een wissel noemt men een transpositie. Stelling: elke permutatie kan geschreven worden als een samenstelling van transposities. Het aantal transposities in zo'n gegeven ontbinding is altijd even of altijd oneven. 

Definitie van inversies van een permutatie: koppels van indices $(i,j)$ waarvoor het afgebeelde koppel de orde omkeert. Voorbeeld gezien (alle inversies van een gegeven permutatie).  Definitie van het teken van de permutatie $\sigma$, gedefinieerd aan de hand van aantal inversies. Verband tussen aantal transposities in een ontbonden permutatie en het teken: $sgn(\sigma)=(-1)^m$. 

Afleiding van de/een determintantafbeelding voor 3x3 matrices. Gedaan via lineaire combinaties van eenheidsvectoren $e_1$, $e_2$ en $e_3$. Dan definitie determinant gebruikt, eerst op rij 1, dan rij 2 etc. Je komt dan uiteindelijk op afbeelding op een matrix met alleen $e_i$ als rijen, die die hebben als waarde altijd 1 (eenheidsmatrix), -1 (afgeleid van eenheidsmatrix met oneven aantal rijwissels) of 0 (twee dezelfde rijen). Alleen kijken naar de niet-nul termen in de grote sommatie, dit zijn alleen de waarden $(j,k,l)$ die permutaties zijn van $(1,2,3)$ (dus verschillende j, k en l), dus equivalent te schrijven dat we sommeren over alle permutaties in plaats van de sommeren over alle combinaties van $j,k,l$ waarbij ze van 1 tot 3 gaan. 

De afleiding toont aan dat de determinantafbeelding uniek is. Maar is de formule ook OK voor de andere twee delen van de definitie?  En komt dit overeen met hoe we vroeger determinanten geleerd hebben voor een 3x3? 


\section{Les 3 nabeschouwing}

1. Een determinant op 3 (twee??) manieren berekenen. OK, gedaan via trapvorm (en stelling 2.4 voor driehoeksmatrix), en dan gedaan via formule 2.2 pagina 65 met $\sigma$ en $sgn(\sigma)$ etc. Kwam beide 1 uit. 

2. Bewijs van stelling 2.3 p. 58 verder bewijzen, dus puntje 2 en 3 i.v.m. $f(E)$. Puntje 2 letterlijk uit definitie te halen door D-1 en D-2 samen te nemen. Puntje 3 is gewoon D-3 op eenheidsmatrix, met $\mu=0$, gecombineerd met puntje 1 dat het op elke rij geldt.  

3. gevolg 2.5 punt 2 bewijzen (det van inverse). Niet zo moeilijk: via product van $AA^{-1}$, we weten $f( AA^{-1}) = 1$ en dat determinant van product van matrices gelijk is aan product van determinanten. 

4. redelijk eenvoudige uitbreiding van wat er al staat voor een 2x2, gewoon wat gelezen. 

5. formule 2.2 pagina 65: aantonen dat $f(\mathbb{I})=1$ met deze formule. OK, even op moeten denken. De term onder sommatie $sgn(\sigma)a_{1\sigma(1)}...a_{n\sigma(n)}$ is alleen verschillend van nul, als \emph{alle} $a_{ij}$ tegelijk verschillend zijn van nul, en dat is alleen het geval als bij \emph{alle} $a_{ij}$ geldt dat $i=j$ (eenheidsmatrix, alleen diagonaal heeft 1). Dit krijg je alleen als $\sigma(i)=i$, dus bij de identieke afbeelding. Voor deze afbeelding is $sgn()$ gelijk aan 1. Dus 1 term blijft over van de som, en alle factoren erin zijn 1. 

6. definitie 2.8 pagina 61. Nog eens goed moeten lezen, want er wrong iets. Ik had het wel juist gedaan, maar eigenlijk niet goed begrepen. Via stelling 2.10 p. 62 is het pas helemaal compleet: elke transpositie toepassen op een permutatie, verandert het teken. De identieke permutatie heeft 0 inversies, dus is even. En vanaf dan gaat elke "wissel" het teken veranderen. Maar het aantal inversies is niet gelijk aan het aantal wissels dat je gedaan hebt, je kan namelijk blijven wisselen. En dan snap ik wel beter dat je makkelijk 12 even en 12 oneven permutaties haalt uit $\{1,2,3,4\}$

7. Had ik eerst heel veel moeite mee. Ik dacht dat die permutaties of transposities gingen over het van plaats veranderen van posities van elementen in koppels of vectoren. Maar dat is niet. Het zijn afbeeldingen, dus functies, met zowel domein en bereik dezelfde verzameling $\{1,2,3,...\}$. Het gaat dus niet over het van plaats verwisselen an sich. Dus eventueel tekenen als verzamelingen met pijlen, en zo meerdere keren na mekaar. NIET tekenen als 5-tuples waarbij je telkens elementen van plaats verwisselt! 
7b is dan triviaal, want $(-1)^3=-1$. 7c is dan ook simpel. 





\section{KEEP ME TO JUMP TO END}

\end{document}
