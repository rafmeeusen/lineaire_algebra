\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{Nota's colleges lineaire algebra}
\author{Raf Meeusen}
\date{2023-2024}

% mark paragraphs with empty line instead of indented first line
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section*{Les 1, 26 sept}

Toepassingen matrices: 
\begin{itemize}
\item google search (eigenwaarden/eigenvectoren worden berekend) 
\item jpeg compressie (projecties uit lin. alg.) 
\end{itemize}


Lineair: zelfs voor niet-lineaire problemen wordt vaak een lineaire benadering (ook: eerste orde benadering) gebruikt voor een oplossing; bvb: slinger waarbij sin theta = theta wordt gesteld

Examen: 
\begin{itemize}
\item duurt 3 uur
\item gesloten boek
\item schriftelijk
\item 2 oefeningen + 1 theorievraag
\item er zijn voorbeeldexamens 
\end{itemize}

Boek: editie 2 ook bruikbaar ; maar er is een editie 3 


Makkelijke verbanden tussen lin. alg. en andere topics: 
\begin{itemize}
    \item fysisch netwerk met knooppunten (elek, stroom auto's, vloeistof): knooppuntvergelijkingen geven stelsel; stelsel = lin. algebra
    \item vlakke meetkunde: 2 vgln. stellen rechte voor; stelsel = alle punten die op beide rechten liggen
    \item Ruimte-meetkunde: vergelijkbaar, vlakken / rechten
\end{itemize}

Vrijheidsgraden als er oneindig veel oplossingen zijn. Oplossingen te schrijven in functie van "vrije parameters". 

"operaties die de oplossingen van een stelsel bewaren" 
(zie ook oefening nabeschouwing met S en S') 
Deze operaties zijn per definitie omkeerbaar.  

Termen: 
\begin{itemize}
\item uitgebreide matrix van een stelsel (met de $b_i$) 
\item achterwaartse subsitutie (bij oplossen stelsel) 
\end{itemize}

Stelsels waarbij oplossing bijna rechtstreeks af te lezen is:  voorbeelden gezien in de les. Allemaal met een matrix die in specifieke vorm is. Dan link gelegd met echelonvorm / trapvorm. 

Termen:
\begin{itemize}
\item echelonvorm
\item trapvorm
\item gebonden variabelen (kolommen met leidende 1) 
\item vrije variabelen (zonder leidende 1); als parameter te kiezen
\item strijdig stelsel
\end{itemize}

Gewoonte: parameter $\lambda$, $\mu$, ...

Elk stelsel kan naar echelon geconverteerd worden. MAAR: een echelonvorm is NIET uniek. 

Methode: algoritme, technieken. 

Dan nog gezien: stelling 1.11. (ivm. 1 oplossing, 0 oplossingen, oneindig veel oplossingen)

Notatie: $\mathbb{R}^{nxk}$ , voor n rijen / k kolommen. 

Voorbeelden bewijzen eigenschappen matrices: onthouden dat een matrix defini{\"e}ren erop neerkomt om elk element te bepalen (notatie Aij = element op rij i en kolom j). 

Definitie: transponeren v/e matrix ($A^T$) . Is spiegelen over diagonaal. 

Termen:
\begin{itemize}
\item vierkante matrix
\item symmetrische matrix ($A = A^T$) ; moet dus vierkant zijn
\item diagonaalmatrix
\end{itemize}

\section{Les 1 nabeschouwing}

Oefening 1, S1 en S2 gemaakt. S3 begonnen, maar niet helemaal afgewerkt.

Oefening 2: gelezen, niet gemaakt. 

Oefening 3: 0, 1 of oneindig. Leg uit adhv prop 1.8 / st.1.11. Even opgezocht: prop 1.8: elke matrix is rij-equivalent met een matrix in echelonvorm, en zelfs met een matrix die rijgereduceerd is. (p. 21). Stelling 1.11: p. 24
stelling zegt dat een stelsel waarvan matrix in echelonvorm, altijd 0, 1 of oneindig oplossingen heeft, en geeft ook de voorwaarden adhv echelonvorm eigenschappen. 

Oefening 4: maak opdracht 1.2 in boek. "toon aan dat  elke vd elementaire rijoperaties ongedaan gemaakt kan worden door opnieuw...". Geef telkens correct notatie voor de inverse elem.rij-op. 

elementaire rij-operaties zijn: 
\begin{itemize}
\item $Ri \rightarrow  lambda * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri + lambda*Rj$
\end{itemize}

inverse: 
\begin{itemize}
\item $Ri \rightarrow  (1/lambda) * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri - lambda*Rj$
\end{itemize}

Oefening 5: gemaakt (zonder helemaal uit te schrijven) 
...
Stel daarom eerst dat ... oplossing is van S. Omdat 
$
(ai1 + lambda*aji) * x1 + ... 
= 
[ ai1 * x1 + ... + ain xn  ] + lambda* [ aj1 * x1 + ... ajn * xn] $
en dat we uit S weten dat 
eerste deel gelijk is aan $b_i$ 
en tweede deel gelijk aan $lambda*bj$
is ook aan deze vergelijking (nieuwe $Ri$) voldaan. 

Omgekeerd: stel dat $(x1,...xn)$ oplossing is van S' 
Omdat $lambda*(aj1x1 + ...) = b_j $ (zie j-de rij van S', en we hebben een oplossing),
kunnen we vergelijking van nieuwe Ri herschrijven door links en rechts gelijke delen te schrappen,
en blijft nog gewoon oude Ri uit S over. 

Oefening 6: 
eens gedaan op papier; niet heel moeilijk, maar wel opletten welke notatie juist welke betekenis heeft (volgorde van $_{ij}$ en $T$ en $lamba*$ etc.). 

Oefening 7 (extra): 
Vraag gesteld aan An Speelman: kolom van de constanten telt niet mee! 

kolomoperaties: 
\begin{itemize}
    \item kolommen verwisselen (die NIET de constanten zijn): gewoon variabelen van naam veranderen. 
    \item kolommen maal een lambda: 
kolom van niet constanten: uitkomst van corresponderende var is gedeeld door lambda
\item kolom plus lambda keer andere kolom: ik zie geen makkelijke redenering van wat er met oplossingen gebeurt. 

\end{itemize}



\section{Les 2 voorbereiding}

Gelezen. 
2a) toon aan dat I.A = A = A.I . Voor 2x2. 
\[I = 
1  0
0  1 \]
\[
A = 
a11  a12 \\
a21 a22\]

(onthouden: rij vd linkse matrix x kolom vd rechtse matrix) 

\[(I x A)11 = 1.a11 + 0.a21 =a11\]
\[(I x A)12 = 1.a12 + 0.a22 = a12\]
\[(I x A)21 = 0.a11 + 1.a21  =a21\]
\[(I x A)22 = 0.a12 + 1.a22 =a22\]


idem voor andere. 

2b) 
Zoek voorbeeld van 2x2 matrices waarbij A . B niet gelijk is aan B. 
\[A 
1  0
1  1 
en
0  1
0  0 \]

heb ik eens uitgeteld; verschillend. 


Appendix over sommatieteken. 
TODO: Nieuwe notatie hier: $(ai)_{ni}=1$  
(bij puntje 1. ) 

bewijs de eigenschappen
\begin{itemize}
\item lambda/mu : OK, heb ik eens uitgeschreven
\item dubbele som van ai.bj : OK, heb ik ook eens uitgeschreven. 
\end{itemize}


herschrijf uitdr. met sommatieteken; OK kan ik 
omgekeerd, wat drukt .. uit  ; OK, kan ik 

toon aan dat ... : OK, gedaan; is wat schrijfwerk en lastig, maar geen rocket science



\section{Les 2, 3 okt}


Matrices vermenigvuldigen. Te onthouden: rij links, kolom rechts. Een rij x een kolom = 1 getal. (makkelijk te onthouden, want ik weet ook nog dat nxm matrix maal mxp matrix een nxp matrix geeft, en nxm matrix heeft n rijen met lengte m, en de lengte van de kolommen van de tweede is ook m. Logisch. Aantal rijen = lengte van de kolommen, en vice versa. 

% even vooruitlopen waarom dat matrixproduct zo gedefinieerd is: 
% 
%Functie $L_A(x)$ is een afbeelding van $\mathbf{R}$ $$ R^k naar R^n, met 
%L_A(x) = som (Aij . xj) 
%(elke matrix A definieert een afbeelding La van ...) 
%En dan is La . Lb = L_A.B 
%(en dit maakt de definitie van matrixproduct logisch) 


\begin{itemize}
    \item veel eigenschappen en rekenregels
    \item een nulmatrix $O_{nxk}$
    \item een eenheidsmatrix $I_n$ (vierkante matrix) 
    \item MAAR: vermenigvuldiging niet-commutatief (logisch want niet eens gedefinieerd voor alle nxk en kxl dimensies. 
    \item $AB$ kan nul zijn terwijl $A \neq O$ en $B \neq O$
\end{itemize}


Noot: als we een matrix zien als een "operatie", en een vermenigvuldiging van matrices zien als de opeenvolging van die twee operaties, dan vinden we het ook logisch dat de uitkomst van $AB$ niet zelfde is als $BA$. (schoenen aantrekken, dan kousen, of omgekeerd. 

Matrices waar volgorde van operaties wel omwisselbaar is: noemt men commuterende matrices. 

Vraag: kunnen we matrices delen? Aangezien product van matrices nul kan zijn zonder dat een van de matrices nul is, is het delen niet zo evident als bij re{\"e}le getallen. Bvb. uit $ab=0$ volgt, als $b=0$, dat $a=0$, omdat we mogen delen door $b$. 

Inverteren: een matrix vinden zodanig dat vermenigvuldiging de eenheidsmatrix geeft. Ook hier zijn er verschillende opties: de linkerinvers en de rechterinvers. 

Opmerking: we beperken ons hier tot vierkante matrices wat betreft inverse, hoewel je ook inverse kan berekenen voor niet-vierkante matrices. 

Stelling: voor een vierkante matrix die links inverse B heeft en rechts inverse C, geldt dat $B=C$. (bewijs in les gezien, heel kort). 

Definitie 1.33. We noemen een nxn matrix ofwel inverteerbaar, ofwel niet-inverteerbaar, indien ... Synoniemen: regulier, niet-singulier/singulier. Opgelet: op dit moment hebben we nog niet bewezen dat linker inverse en rechter inverse gelijk zijn! Dat komt later! 

Stelling 1.34: inverse van product (als beide inverteerbaar zijn) =  product van de inversen in omgekeerde volgorde. Het bewijs is gebaseerd op: aantonen dat een gegeven matrix een inverse van een andere, is bewijzen dat zowel links-vermenigvuldigen als rechts-vermenigvuldigen resulteert in de eenheidsmatrix. 

Noot: niet evident hier om de te volgen wat al bewezen is, en wat we al weten, maar nog niet bewezen is (en dus: wat volgt eigenlijk waaruit). Bvb. nog steeds niet bewezen dat linker en rechter inverse gelijk gaan zijn. 

Rap een eigenschap tussendoor: als $AB=0$ en $A$ is inverteerbaar, dan is $B=0$. (bewijs redelijk makkelijk) 

Term/concept: elementaire matrix $E_n$. Komt voort uit oplossen van stelsels met matrix rij-operaties, gecombineerd met matrix-vermenigvuldigen. We kunnen namelijk rij-operaties voorstellen als matrix-vermenigvuldiging met zogenaamde elementaire matrices. 

Interessant: twee rijen van plaats wisselen: ongedaan maken is opnieuw hetzelfde doen! Dus inverse van $E_n$ voor rijen verwisselen, is gewoon dezelfde $E_n$! 

Noteer ook: alle elementaire matrices zijn inverteerbaar. 

ERO: elementaire rij-operaties. Bij elke ERO hoort een EM (elementaire matrix), noemen we $\Sigma$, en die EM bekom je door rij-operaties toepassen op de eenheidsmatrix. ERO toepassen op matrix A komt overeen met matrix-vermenigvuldiging $\Sigma A$. Links-vermenigvuldigen dus. 

Noot: we weten dat twee rijen wisselen, ongedaan gemaakt wordt door opnieuw hetzelfde te doen. Dus: de matrix $\Sigma$ van rij-wissel operatie, moet wel de inverse van zichzelf zijn. 

Grote stelling: 1.39, met 6 equivalente beweringen. De matrix A heeft een links inverse B; het stelsel $AX=0$ heeft enkel de evidente oplossing $X=0$ (dus stelsel met 1 oplossing!); etc. etc. Puntje 4: matrix A is product van elementaire matrices, is een soort sleutel-formulering, en ook een sleutel in het grote bewijs. 

Bewijs: formulering 2 volgt uit 1 door links met B te vermenigvuldigen en uit te werken. Formulering 3 volgt uit 2, doordat stelsels met 1 oplossing in rij-echelon-vorm de eenheidmatrix hebben, en altijd daartoe te herleiden zijn met ERO's. Bewijs van 4 uit 3 (A is product van EM's): op bord geschreven tijdens de les. Ook redelijk makkelijk (moet wel weten dat EM's inverteerbaar zijn, en opnieuw EM geven). 

Tussenstelling: als $C$ inverteerbaar, en $CA=I_n$, dan is A inverteerbaar en $A^{-1}=C$. Het bewijs: als $CA=I_n$, dan is A rechterinvers van C, en dus $A=C^{-1}$

Noot: tijdens de les effe de draad kwijt tussen wat al bewezen was, en wat niet, en de definities inverteerbaar etc. Nog eens nakijken! 

Noot2: belangrijk is: als $AB=I_n$, dan kan je dat op 2 manieren lezen: A is linkerinvers van B, of B is rechterinvers van A. Die twee manieren van lezen heb je nodig om bewijs sluitend te krijgen. 

Bovendien volgt uit deze stelling een algoritme om de inverse te berekenen: doe dezelfde rijoperaties om A te rij-herleiden naar $I_n$, ook gewoon telkens op $I_n$, bijvoorbeeld door de matrix $(A|I_n)$ te gebruiken voor alle rij-operaties. Dan krijgen we rechts $A^{-1}$, want in bewijs hadden we gezien dat $A^{-1} = \Sigma_k \Sigma_{k-1} ... \Sigma_1$. Als bij A een nul-rij ontstaat, is A niet inverteerbaar. 

Dan nog even over LU-decompositie gehad. (lower / upper triangular = benedendriehoeksmatrix en bovendriehoeksmatrix). Benedendriehoeksmatrix heeft niet-nullen op en beneden diagonaal. Het kunnen schrijven van een matrix A als product van een L en een U, heeft interessante toepassingen. Ook interessant: product van twee bovendriehoekmatrices is opnieuw bovendriehoeks. Een diagonaalmatrix is zowel bovendriehoeks als benedendriehoeks. Kijk ook eens naar de EM's en of ze L of U zijn. Echelonvorm is bovendriehoeks. 

Ook "de spil" vernoemd hier ivm rij-herleiden. Dus ook leerstof (spil/gauss-eliminatie...). En volgende redenering: als je A in echelonvorm kan zetten zonder rijen te verwisselen, en we weten dat echelonvorm een U-vorm is. Dus $U= (\Sigma_k ... \Sigma_1) A$, met $\Sigma_k ... \Sigma_1$ inverteerbaar, en inverse hiervan is een L-vorm (indien geen rij-wissels). 

\section{Les 2 nabeschouwing}

1. stelsel herleiden naar trapvorm, dan elementaire matrices hiervoor opschrijven, en dan A herschrijven als product van elementaire (die dan inverse zijn van oorspronkelijke elementaire!) en de trapvorm zelf. 
OK, alleen twijfelde ik wat bij inverteren van de elementaire. Niet slecht om eens voorbeeld van op te schrijven (1/lambda, en +lamba Ri wordt -lambda Ri). 

2. stelling 1.38 bewijzen; had ik al gedaan voor paar getallenvoorbeelden, moet in deze oefening eigenlijk gewoon met variabele lambda en zo, dus heb ik geskipt. Wel goeie oefening om die inversen van elementairen eens uit te schrijven. 

3. Die bewijzen zijn veel schrijfwerk, en met al die verschillende indices en mxn, nxp etc. dimensies, ... Eerst eens proberen definitie 1.22 zelf op te schrijven zonder te spieken. 

4. Doenbaar, wat rekenwerk. Gevolg 1.40 is pagina 40. Maar nog niet gedaan. 

5. Oefening 1.35 uit boek (is op p. 53, idempotente matrix). Nog niet gedaan. 

\section{Les 3 voorbereiding}

A4 lesvoorbereiding, paar stukjes in boek lezen en nadenken. Duurde ongeveer halfuur. 

\section{Les 3, 10 okt}

Determinanten. Vorige les: verband inverteerbaarheid matrix en oplosbaarheid (uniek) van homogeen stelsel. Voor 2x2 matrix kan je matrix beginnen inverteren via rijoperaties, en bekom je dat matrix inverteerbaar is als en slechts als $ad-bc$ niet nul is, en we kunnen dit defini\"eren als de determinant-functie $f(A)$ van een 2x2 matrix. 

Paar eigenschappen van de 2x2 determinant: rijen van plaats wisselen, determinant van eenheidsmatrix is 1, twee matrices met verschillende eerste rij combineren. Deze drie eigenschappen worden als basis genomen van een algemene determinant-afbeelding. Zie boek. 

Eigenschappen van een determinantfunctie: er volgt direct dat die lineariteit ook geldt voor alle andere rijen (want je mag rijen wisselen, alleen maar minteken erbij). Nul-rij geeft nul-determinant. Twee gelijke rijen ook nul (teken verandert maar moet hetzelfde blijven, dus moet 0 zijn). 

Impact van elementaire rij-operaties (ERO) op een determinant-afbeelding: type III in stelling 2.3. Bewijs in de les gezien. Ook voor andere ERO's de impact gezien. 

Stelling 2.4 gezien, vier onderdelen gecombineerd in een stelling: driehoeksmatrices, verband met inverteerbaarheid, determinant van product van matrices, determinant van getransponeerde matrix (dus lineariteit geldt ook voor de kolommen!). In de les het bewijs gezien van deel 2 van de stelling (verband inverteerbaarheid). Ene richting bewijs via ontbinding elementaire matrices. Andere richting van bewijs via contrapositie, en boven- en benedendriehoeksmatrices. 

Gevolg 2.5: o.a. determinant van de inverse, ... 

Nu gaan we bewijzen dat determinant-afbeelding bestaat. Gaat via permutaties, dus eerst heleboel wiskunde rond permutaties, transposities, ... 

Notatie van permutatie $\sigma$ via matrix met bovenste rij (1 2 3 ...), en onderste rij de beelden onder $\sigma$. Demonstratie dat om van een gegeven  permutatie naar de identieke permutatie te gaan via "wissels": altijd oneven aantal wissels nodig. Een wissel noemt men een transpositie. Stelling: elke permutatie kan geschreven worden als een samenstelling van transposities. Het aantal transposities in zo'n gegeven ontbinding is altijd even of altijd oneven. 

Definitie van inversies van een permutatie: koppels van indices $(i,j)$ waarvoor het afgebeelde koppel de orde omkeert. Voorbeeld gezien (alle inversies van een gegeven permutatie).  Definitie van het teken van de permutatie $\sigma$, gedefinieerd aan de hand van aantal inversies. Verband tussen aantal transposities in een ontbonden permutatie en het teken: $sgn(\sigma)=(-1)^m$. 

Afleiding van de/een determintantafbeelding voor 3x3 matrices. Gedaan via lineaire combinaties van eenheidsvectoren $e_1$, $e_2$ en $e_3$. Dan definitie determinant gebruikt, eerst op rij 1, dan rij 2 etc. Je komt dan uiteindelijk op afbeelding op een matrix met alleen $e_i$ als rijen, die die hebben als waarde altijd 1 (eenheidsmatrix), -1 (afgeleid van eenheidsmatrix met oneven aantal rijwissels) of 0 (twee dezelfde rijen). Alleen kijken naar de niet-nul termen in de grote sommatie, dit zijn alleen de waarden $(j,k,l)$ die permutaties zijn van $(1,2,3)$ (dus verschillende j, k en l), dus equivalent te schrijven dat we sommeren over alle permutaties in plaats van de sommeren over alle combinaties van $j,k,l$ waarbij ze van 1 tot 3 gaan. 

De afleiding toont aan dat de determinantafbeelding uniek is. Maar is de formule ook OK voor de andere twee delen van de definitie?  En komt dit overeen met hoe we vroeger determinanten geleerd hebben voor een 3x3? 


\section{Les 3 nabeschouwing}

1. Een determinant op 3 (twee??) manieren berekenen. OK, gedaan via trapvorm (en stelling 2.4 voor driehoeksmatrix), en dan gedaan via formule 2.2 pagina 65 met $\sigma$ en $sgn(\sigma)$ etc. Kwam beide 1 uit. 

2. Bewijs van stelling 2.3 p. 58 verder bewijzen, dus puntje 2 en 3 i.v.m. $f(E)$. Puntje 2 letterlijk uit definitie te halen door D-1 en D-2 samen te nemen. Puntje 3 is gewoon D-3 op eenheidsmatrix, met $\mu=0$, gecombineerd met puntje 1 dat het op elke rij geldt.  

3. gevolg 2.5 punt 2 bewijzen (det van inverse). Niet zo moeilijk: via product van $AA^{-1}$, we weten $f( AA^{-1}) = 1$ en dat determinant van product van matrices gelijk is aan product van determinanten. 

4. redelijk eenvoudige uitbreiding van wat er al staat voor een 2x2, gewoon wat gelezen. 

5. formule 2.2 pagina 65: aantonen dat $f(\mathbb{I})=1$ met deze formule. OK, even op moeten denken. De term onder sommatie $sgn(\sigma)a_{1\sigma(1)}...a_{n\sigma(n)}$ is alleen verschillend van nul, als \emph{alle} $a_{ij}$ tegelijk verschillend zijn van nul, en dat is alleen het geval als bij \emph{alle} $a_{ij}$ geldt dat $i=j$ (eenheidsmatrix, alleen diagonaal heeft 1). Dit krijg je alleen als $\sigma(i)=i$, dus bij de identieke afbeelding. Voor deze afbeelding is $sgn()$ gelijk aan 1. Dus 1 term blijft over van de som, en alle factoren erin zijn 1. 

6. definitie 2.8 pagina 61. Nog eens goed moeten lezen, want er wrong iets. Ik had het wel juist gedaan, maar eigenlijk niet goed begrepen. Via stelling 2.10 p. 62 is het pas helemaal compleet: elke transpositie toepassen op een permutatie, verandert het teken. De identieke permutatie heeft 0 inversies, dus is even. En vanaf dan gaat elke "wissel" het teken veranderen. Maar het aantal inversies is niet gelijk aan het aantal wissels dat je gedaan hebt, je kan namelijk blijven wisselen. En dan snap ik wel beter dat je makkelijk 12 even en 12 oneven permutaties haalt uit $\{1,2,3,4\}$

7. Had ik eerst heel veel moeite mee. Ik dacht dat die permutaties of transposities gingen over het van plaats veranderen van posities van elementen in koppels of vectoren. Maar dat is niet. Het zijn afbeeldingen, dus functies, met zowel domein en bereik dezelfde verzameling $\{1,2,3,...\}$. Het gaat dus niet over het van plaats verwisselen an sich. Dus eventueel tekenen als verzamelingen met pijlen, en zo meerdere keren na mekaar. NIET tekenen als 5-tuples waarbij je telkens elementen van plaats verwisselt! 
7b is dan triviaal, want $(-1)^3=-1$. 7c is dan ook simpel. 


\section{Les 4 voorbereiding} 
Gedaan. 

\section{Les 4, 17 okt}
Niet live, maar video bekeken. Was ziek. 

Eerst nog verder over determinanten. De formule (2.2) p. 65 verder bestudeerd. Deze (2.2) is niet echt een definitie, denk ik, maar resultaat van rekenwerk aan de hand van definitie (2.1) p. 57. Best wel iets speciaal: we defini\"eren een determinant als een afbeelding met 3 eigenschappen, en komen dan tot besluit dat er maar 1 oplossing bestaat hiervoor. 

In de les gedemonstreerd hoe je van (2.2) komt tot rij- of kolomontwikkeling, eigenlijk paragraaf 2.2.4 p.68 uit boek. Introductie $S_n$ als verzameling van alle permutaties van $\{1,2,...,n\}$. Kwam erop neer gewoon eens te proberen alle $a_{1j}$ uit de rest van de formule af te zonderen, en dan te kijken wat er overblijft (voorbeeld voor ontwikkeling eerste rij). Prof introduceerde ook $S_{n,1}, S_{n,2},...$, deelverzamelingen van $S_n$ waarbij 1 afgebeeld wordt op 1, resp. 2 op 2, etc. In boek iets andere notatie gebruikt. Dan interpretatie van de overschot: wat blijft er nog over nadat bvb. $a_1j$ buitengebracht is? Dit wordt $C_{ij}$ genoemd, en door goed te bekijken zien we dat dit ook een determinant is. Voor $C_{11}$ is het vrij makkelijk, maar voor $C_{12}$ is het subtieler. $S_{n,2}$ is eigenlijk verzameling van alle permutaties van $\{1,3,4,...n\}$. En er is nog een belangrijke nuance: er staat daar nog een kolom 2 tussen die geschrapt wordt, waardoor het teken verandert. Het volledige bewijs van hoe dat juist werkt met die tekens, is iets complexer, en moeten we niet kennen. Maar je kan het "onthouden" door in te zien dat je eerst nog kolommen moet omwisselen om de geschrapte kolom uit beeld te krijgen, en we weten dat elke kolom-omwisseling de determinant van teken doet veranderen. Bvb. kolom 4 naar links schuiven, is eerst een wissel kolom 4 met kolom 3, dan kolom 3 met 2, dan 2 met 1; dan pas staat geschrapte kolom uit beeld en is de overblijvende matrix een "geheel". 
Besluit is dan: $C_{1i} = (-1)^{i-1} det (M_{1i})$, en dat $det A=  \sum_{i=1}^{n} a_{1i}C_{1i}$. 

Definitie (2.19) besproken (cofactor en minor), en verband (cofactor is determinant minor, op teken na). Paar toepassingen gezien, redelijk triviaal. Dan terug naar hoe we aan determinanten waren gekomen: inverteren, inverteerbaarheid, en concept adjunt matrix gezien. Definitie 2.22, eigenschap van stelling 2.23, en daaruit volgt dus dat $A^{-1}= \frac{1}{det(A)} \cdot adj(A)$. Opmerking: niet effici\"ent om inverse te berekenen! Ook bewijs gezien van stelling 2.23 voor \'e\'en deel, in twee stappen: diagonaal elementen gelijk aan $det(A)$, en dan niet-diagonaal elementen gelijk aan nul. Truuk in het bewijs: matrix met 2 gelijke rijen introduceren, waarvan we weten dat determinant 0 is. Als je die ontwikkelt naar de j-de rij (met j-de rij gelijk aan i-de rij), dat krijg je een ontwikkeling met resultaat nul, maar die ontwikkeling komt ook overeen met een stuk van de formule in het bewijs ergens. 

(pauze) 

Vectorruimten: iets nieuws. Een niveau van abstractie. Begonnen met voorbeelden: oplossingen-verzameling van homogene stelsels, matrices met optelling en scalaire vermenigvuldiging, de veeltermen met re\"ele co\"effici\"enten. Sprake van zogenaamde "evidente rekenregels" voor optelling en scalaire vermenigvuldiging, en ook distributiviteit van ene op andere. 

Nog een voorbeeld: continue functies. Kan je ook optellen en scalaire vermenigvuldigen. 

Dus: twee bewerkingen die we telkens hebben, optelling en scalaire vermenigvuldiging. 
Eerst even focus op die optelling: verzameling met 1 binaire bewerking die commutatief is: commutatieve groep (def. 3.2). Noot: er zijn ook niet-commutatieve groepen, bvb. samenstellen van permutaties, de vermenigvuldiging bij de inverteerbare matrices (jaja, die ene bewerking moet geen klassieke optelling zijn!). Anderzijds is er natuurlijk de gewone optelling in $\mathbb{Z}$ die wel een commutatieve groep is. 
Niet-commutatieve groepen hebben vaak een $\times$ of $\cdot$ symbool voor hun operatie, en het neutraal element wordt dan vaak als $1$ geschreven (en niet als $0$ zoals bij commutatieve groep). Tegengesteld element is nodig bij commutatieve groep, daarom is ($\mathbb{Z}, \cdot$) geen commutatieve groep: wel associatief, wel een neutraal element ($1$), en ook commutatief, maar geen tegengesteld element (inverse). Rationale getallen met vermenigvuldiging zijn wel commutatieve groep, maar je moet 0 weglaten (want je kan met $0 \cdot x$ nooit neutraal element $1$ uitkomen!). 

Even uitwijding naar aanleiding van een vraag: verschil axioma's en definities. Iemand vond het geen definities die werden gegeven, maar axioma's. Prof gaf hem ongelijk, zei dat axioma's vooral uit de verzamelingenleer komen. 

Principe vectorruimten: paar definities, voor de rest een wit blad. En dan gaan we stellingen bewijzen. Beginnen met paar eenvoudige stellingen: lemma 3.7 en lemma 3.8 in de les even bewezen. 
Opgelet: symbool $0$ is soms een re\"eel getal, en soms de nulvector. Zou uit context moeten blijken, maar toch belangrijk om bij stil te staan bij die bewijzen. Ook symbool $-$ heeft die dubbele betekenis: voor een getal is het het negatieve getal (bvb. $-1$), maar voor een vector bvb. $-v$ betekent het notatie van tegengestelde. Gelukkig is $(-1)v = -v$, zie lemma 3.8. 

Concept deel-vectorruimte: komt vaak voor! Stelling 3.11 i.v.m. deelruimtecriterium. Voorbeeld: $\mathbb{R}^n$ is een vectorruimte, en de oplossingen van een stelsel $AX=0$ is een deelvectorruimte van $\mathbb{R}^n$. 

\section{Les 4, nabeschouwing}

\subsection {Kolom/rij-ontwikkeling}

Oefening rap gemaakt, lukte wel. Opgelet: tekens! denk aan $(-1)^{i+j}$

Oefening 2 extra:  Leg zorgvuldig uit waarom in 2.2.6 in het bewijs van de formule van Cramer geldt
dat ... . Boek pagina 75. 
\[ 
det \begin{pmatrix}
a_{11} & ... & b_1 & ... & a_{1n} \\ 
a_{21} & ... & b_2 & ... & a_{2n} \\
...    & ... & ... & ... & ... \\ 
a_{n1} & ... & b_n & ... & a_{nn} 
\end{pmatrix} 
= b_1 C_{1i} + b_2 C_{2i} + ... 
\] 
Antwoord: gewoon kolomontwikkeling naar kolom $i$ (waar de $b_i$ staan). De kolom zelf waarnaar ontwikkeld wordt, komt niet voor in de minoren/cofactoren, dus zijn het de gewone $C_{ij}$ van de oorspronkelijke matrix, want de rest van de matrix is ongewijzigd. 


\subsection {Vectorruimten}

3. Bekijk Voorbeeld 3.6(3), pagina 93. 
(a) Toon voor de vectorruimte ($\mathbb{R}, \mathbb{R}[X], +$) aan dat de eigenschappen van neutraal element en tegengesteld element uit Definitie 3.2 gelden en toon 1 eigenschap naar keuze aan uit Definitie 3.3.

NE: er bestaat element 0 zodat $v+0=0=0+v$; voor ($\mathbb{R}, \mathbb{R}[X], +$) is dit de veelterm met alle  $a_i=0$. (noot: veelterm van graad 0). Triviaal dat uit definitie optelling (i.e. co\"effici\"enten van gelijke graad bij elkaar optellen), dat $v+0 = 0 = 0+v$. 

TE: bestaat ook, namelijk de veelterm $v'$ met co\"effici\"enten $a'_i = -a_i$. Ook hier is het triviaal uit definitie optelling, dat $v + v'$ een veelterm geeft met alle co\"effici\"enten gelijk aan $0$. 

Eigenschap naar keuze uit definitie 3.3 (p. 91): eigenschap 4 i.v.m. scalar 1: $1 . v=v$. Vermenigvuldiging met scalar voor veeltermen doe je door al zijn co\"effici\"enten met die scalar te vermenigvuldigen. Evident dat je dan opnieuw $v$ krijgt voor scalar 1. 

(b) Waarom vormen de monische veeltermen geen vectorruimte?
Monische veeltermen: hoogstegraadsco\"effici\"ent is 1. Bvb. $X^3 + 3X^2 -7X + 31$. 
Heel eenvoudig: tel voorbeeld op bij zichzelf. En hij is niet monisch meer, dus som zit niet meer in de deelverzameling $U$ (zoals in stelling 3.11 voor deelruimtecriterium), dus is het geen deelruimte, dus geen vectorruimte (wel een deelverzameling van alle veeltermen).  



4. Werk de opmerking op het einde van Voorbeeld 3.6(4) uit. (p. 93). De opmerking is: veeltermen van graad precies $n$ vormen geen vectorruimte. Ga na hoe dit komt. Opnieuw kijken naar criteria deelvectorruimten: is die optelling nog inwendig? Ook hier niet: neem hoogste co\"effici\"ent, zet er een $-$ voor, en tel op bij oorspronkelijke veelterm: de graad wordt eentje lager, niet meer precies $n$ dus, maar $n-1$. 

5. Bewijs Lemma 3.7. En bewijs het derde puntje van Lemma 3.8. 
Lemma 3.7: tiens, was ook oefening. Zie oefeningen. 
Lemma 3.8, punt 3: $(-\lambda)v = -(\lambda v) = \lambda(-v)$. Bvb. gebruikmakend van 
\[ (-\lambda)v = [(-1)\lambda]v = [\lambda (-1)]v = \lambda(-1)v = \lambda(-v) \]
(achtereenvolgens: gemengde associativiteit, commutativiteit ($\mathbb{R},\cdot$), Lemma 3.8 puntje 2), hebben we al de gelijkheid van de linkse en de rechtse uitdrukking aangetoond. 
Voor de middelste uitdrukking: 
\[
-(\lambda v) = (-1)(\lambda v) = (-1 \lambda) v = (- \lambda)v
\]
(achtereenvolgens puntje 2 Lemma 3.8, gemengde associativiteit, en \emph{gewone} tekenverandering in $\mathbb{R}$). 

6. Toon aan dat er geen re\"ele vectorruimten bestaan met eindig veel vectoren.
Raar. Volgens boek bestaat dat wel, namelijk de triviale vectorruimte $V=\{0\}$. 
Stel dat bedoeling van vraag is: vectorruimten met meer dan 1 element. Dan begon ik te lezen in definitie, en zit met een vraag: waar uit definitie leid je af dat elke mogelijke $\lambda$ maal elke mogelijke $v$ opnieuw een vector is in $V$?? 


7. Bekijk Voorbeeld 3.6(6), p.94. Hier staan voorbeelden van complexe vectorruimten. 
Bekijk dan Voorbeeld 3.6(2) op p. 93 en leg uit waarom ($\mathbb{C}, \mathbb{R}, +$) geen (complexe) vectorruimte is.
Ook dit lijkt gebaseerd op de aanname dat elke mogelijke $\lambda$ maal elke mogelijke $v$ steeds een vector is in $V$. (aanname wordt precies vaak gemaakt, bvb. bij deelruimten, maar ik snap niet goed waar ze vandaan komt). 


\section{Les 5, voorbereiding}
OK, gelezen en over nagedacht. (ivm de voortgebrachte ruimte van een deelverzameling $D$ van de commutatieve groep van een vectorruimte). 

\section{Les 5, 24 okt}

Voorbeelden deelruimten. 

Voorbeeld 1: veeltermen $P$ met  graad $\leq n$, is een deelruimte van $\mathbb{R}[X]$. 

Voorbeeld 2: veeltermen met graad exact $n$ zijn geen deelruimte. (zie oefening ergens; van zichzelf aftrekken = graad minder). Overigens zit nul-veelterm hier ook niet in. 

Voorbeeld 3: wat zijn alle deelruimten van het vlak? Alle rechten door de oorsprong (nul zit erin, en veelvouden blijven op rechte liggen). En dan nog de twee triviale: {0}, de ganse ruimte. 

Voorbeeld 4: deelruimten van $\mathbb{R}^3$? Vlakken door de oorsprong, ook alle rechten door de oorsprong, en de twee triviale deelruimten. We voelen hier al aan: de dimensie is verschillend voor al deze deelruimten. 

Nieuw concept dat vaak terugkomt: lineaire combinaties. (\emph{de} prototypische operatie bij vectorruimten). De vector $\alpha_1 v1 + \alpha_2 v2 + ... \alpha_n v_n$ noemen we een lineaire combinatie van $v_1, ..., v_n$. Andere notatie: $\sum_{i-1}^n \alpha_i v_i $. 
We voelen hier al aan voor $\mathbb{R}^3$: alle lineaire combinaties van twee vectoren (die niet op 1 rechte liggen), die vormen een vlak. Dit kan veralgemeend worden. 

Definitie 3.15. Deelruimte voorgebracht door $D$. Noteer: dit is de kleinst mogelijke deelruimte waarin alle vectoren van $D$ in zitten. In woorden uitgelegd in de les waarom dit een deelruimte is. Notatie $vct(D)$, en andere notaties zie boek. 

Bekijken we deelruimten van de ruimte, "een rechte" en "een vlak". Ze gaan door de oorsprong (dus niet evenwijdig tenzij rechte in vlak ligt). Bekijken we twee rechten (die niet samenvallen), de voortgebrachte ruimte is het vlak door die twee rechten. Wat valt er op: de veelvouden van de vectoren op de rechten zitten allemaal in de deelruimten, voor de "nieuwe" vectoren van de voortgebrachte ruimte moeten we alleen kijken naar de sommen van vectoren. Weer een interessant concept: 

Definitie somruimte: Als $V_1, ... V_n \subset V$ allemaal deelruimten, dan is $V_1 + V_2 + ... V_n = \{ v_1 + ... + v_n | v_i \in V_i \}$  terug een deelruimte. Notatie $V_1 + V_2 + ... V_n = vct(V_1 \cup V_2 ... \cup V_n)$ . Nagaan: waarom is dat een deelruimte? (via eigenschappen voor deelruimte). 

Definitie "directe som" (eerst voor twee deelruimten gezien, verderop voor meer dan twee): is ook een somruimte, maar hierbij kunnen we elke vector van de somruimte op \emph{unieke} wijze kunnen schrijven als som van iets in $U$ en iets in $V$. Voorbeeld: somruimte van vlak en rechte, vs. somruimte van twee rechten. Beide geven volledige 3D ruimte, maar alleen de somruimte van twee rechten is een directe som. 

Propositie: $U+W$ is een directe som als en slechts als $U \cap W = \{ 0 \} $. Bewijs van een "asa", dus twee implicaties. Stap 1 de pijl terug: via veronderstelling dat $v=u_1 + w_1$ maar ook $v=u_2 + w_2$, en dan bewijzen dat $u_1=u_2$ en $w_1=w_2$. (met gegeven dat doorsnede $V$ en $W$ alleen de nulvector is). We komen tot een gelijkheid van twee vectoren waarvan we weten dat ene in $V$ zit en andere in $W$, dus ze zitten alle twee in de doorsnede, dus ze moeten beide $0$ zijn, en dus ... 
Stap 2 de pijl naar rechts. Gegeven direct som, bewijzen dat doorsnede alleen $0$ bevat. Of via contrapositie: als doorsnede niet gelijk is aan $\{ 0 \} $, bewijzen dat er een vector is in $U+W$ die twee schrijfwijzen heeft. In les helemaal uitgeschreven. 

Opnieuw even bekijken in termen van dimensies. We verwachten dat voor direct som, de som van de dimensies gelijk is aan de dimensie van de somruimte. Voor niet-directe som: we gaan te veel hebben, hoeveel te veel? Ah, de dimensie van de doorsnede moeten we er misschien van aftrekken? Zie volgende les. 

Definitie 3.20: directe som voor meer dan 2 deelruimten. Uitbreiding propositie in verband met doorsnede: kijken naar doorsnede van 1 van de deelruimten met de somruimte van al de rest! En vooral: moet gelden voor elke van de deelruimten die je apart neemt! 
Niet zo evident, maar denk aan drie rechten door 0 in hetzelfde vlak. Somruimte van twee rechten is al het vlak, en doorsnede van dat vlak met derde rechte is opnieuw rechte. Bewijs is quasi zelfde als voor som van twee deelruimten. 

(pauze; voorstelling POC, permanente onderwijscommissie, studentenvertegenwoordiging hierin) 

Volgende (fundamenteel) concept: lineaire afhankelijkheid, onafhankelijkheid. Hangt ook samen met begrip dimensie. Zolang je van een groep vectoren een vector kan schrijven als lineaire combinatie van de andere vectoren, dan is hij eigenlijk overtollig wat betreft de somruimte. Hij is dan ook niet lineair onafhankelijk. Als we zo'n groep vectoren in een deelverzameling $D$ steken, dan spreken we van de lineaire afhankelijkheid of onafhankelijkheid van die deelverzamling $D$. Definitie 3.26. Ook "vrij" als concept! (lineair onafhankelijk). 

Als $D$ maar 1 vector bevat: uiteraard lineair onafhankelijk. Tweede vector erbij: lineair onafhankelijk als hij geen veelvoud is van de eerste. Derde erbij: als hij niet in het vlak ligt (als we over ruimte spreken). Vraag: kunnen we een vierde vinden in $\mathbb{R}^3$? We voelen aan dat dat niet kan. 

Met voorbeelden gezien: hoe nagaan of vectoren lineair onafhankelijk zijn. Met veeltermen gezien. We voelen al dat het moeilijk kan zijn om heel zeker te zijn van ons stuk bij bvb. $\{ 1, 1+X, 1+X+X^2 \}$. En zeker bij  $\{ X,X^2-X^3,  1+X+X^3, 1-X+X^2 \}$

Propositie 3.27: voldoende om te bewijzen dat enige lineaire combinatie die nulvector oplevert, diegene is wanneer de co\"effici\"enten nul zijn. Opgelet: er staat iets voor eindige en niet per se eindige $D$ in de propositie, dat had ik niet helemaal door. 

Dan eerst de propositie toegepast op paar voorbeelden. Stellen dat een lineaire combinatie 0 is, en dan bewijzen dat hieruit volgt dat alle co\"effici\"enten nul zijn, is tegelijk een bewijs dat de vectoren lineair onafhankelijk zijn. 
En dat is vrij gemakkelijk bij veeltermen. 

Bewijs in de les gedaan. Maar alleen voor eindig aantal vectoren in $D$. Opnieuw twee richtingen van de "asa" apart te bewijzen. Pijl rechts: te bewijzen dat alle $\lambda_i=0$. Uit ongerijmde: stel een $\lambda_j \neq 0$. We brengen in dat bewijs die $\lambda_j v_j$ apart uit de vergelijking. Dan alles delen door $\lambda_j$, en dan kunnen we die $v_j$ schrijven als lineaire combinatie van de andere $v_i$. Maar dat kon niet, was de aanname, ... tegenspraak. 
Dan pijl terug. Stel ze zijn lineair afhankelijk. T.B. we kunnen $\lambda_i$ vinden niet alle 0 en toch lineaire combinatie gelijk aan 0. Bewijs van dit deel ook in les uitgeschreven gezien. Vrij eenvoudig om dan een lin. combinatie te schrijven met minstens een niet-0 co\"effici\"ent. 

Zeer belangrijke propositie, gaan we vaak gebruiken als criterium voor lineaire onafhankelijkheid van gegeven vectoren. 

Volgende concept: wanneer hebben we genoeg vectoren om de hele ruimte op te spannen. Concept heet "voortbrengend". Definitie 3.31. Voortbrengend gaat over "genoeg". Concept "vrij" gaat over "niet te veel". Combinatie vrij en voortbrengend: 

Concept "basis". Definitie 3.33: een basis is vrij en voortbrengend. 

Voorbeeld basis voor veeltermen met graad $\leq n$. Dat het voortbrengend is, is triviaal, want volgt uit definitie van de verzameling van die veeltermen (is al geschreven als lineaire combinatie van die basis). Dat die basis vrij is, is ook vrij logisch (hoewel ik bewijs in de les niet heel formeel vond). 
Ander voorbeeld, van een basis voor de veeltermen met graad hoogstens 2. Je voelt wel aan dat het kan. 

We zien dus al onmiddellijk met die voorbeelden: een basis is niet per se uniek. Wel is het aantal elementen in een basis precies altijd hetzelfde. Dat gaan we later de dimensie noemen. 




\section{Les 5, nabeschouwing}

\subsection{Deelruimten} 
NB 5.1: Maak oefening 3.6.2 (p. 128-129) voor W2, W5, W6 en W8: zie oefenzittingen. 

NB 5.2:  Teken Venn-diagram met $V, D, vct(D)$. OK, getekend. Als $D$ een deelruimte is, dan is $vct(D)=D$. 

NB 5.3a:  Neem propositie 3.14 p.98. Toon aan voor oneindig veel deelruimten. 
Propositie: algemene doorsnede van deelruimten $U_i$ is opnieuw een deelruimte (ook voor oneindig veel deelruimten). 
Bewijs: de nulvector zit in alle deelruimten, dus ook in de algemene doorsnede,  dus aan dit criterium voor deelruimten is voldaan. Volgend deel criterium: zit een willekeurige $rx +sy$ met $x,y \in \bigcap$ en $r,s \in \mathbb{R}$ ook in $\bigcap$? Antwoord is ja, want voor elke $U_i$ geldt dat $x,y \in U_i$, en elke $U_i$ is een vectorruimte, dus elke $rx +sy$ zit in $U_i$, en dat geldt voor alle $i$. Dus zit $rx +sy$ ook in $\bigcap$. QED. 

NB 5.3b:  opdracht 3.18, p.100. Toon aan dat $vct(D)$ de doorsnede is van alle deelruimten die $D$ bevatten. 
Definitie $vct(D)$: de vectorruimte voortgebracht door $D$, of ook: de verzameling van alle lineaire combinaties van $x_i \in D$. Wat over nagedacht, maar uiteindelijk geskipt, ik vond het een lastige. Er stond ook nog een hint (je mag gebruiken dat vct(D) de kleinste deelruimte is van $V$ die $D$ omvat). 
Opnieuw bekeken: de hint wil eigenlijk zeggen: noem $\{ U_i | i \in I, D \subset  U_i \subset V \}$  de verzameling van alle deelruimten van $V$ die $D$ omvatten. Dan geldt: $vct(D) \subset U_i$ voor alle $i \in I$, want $vct(D)$ is een deelruimte, en is de kleinste van alle. ?? is dit zo? wat is dat juist, "de kleinste". opnieuw in boek lezen... 


\subsection{Som/direct som} 
NB 5.4:  Venn-diagram getekend. Met in achterhoofd: $V=\mathbb{R}^3$, $U$ is x-as, $W$ is y-as, dan is $U+W$ het vlak door x-as en y-as. En dan in Venn-diagram paar punten gezet $(0,0,0), (x_1,0,0), (0,y_1,0), ... (x_1,y_2,0)$. $U+W$ omspant duidelijk $U \cup W$. 

NB 5.5:  oefening 3.6.28 p. 132. Zie oefeningen. 

NB 5.6:  
Propositie 3.23, p. 102. Puntje 2. Voorbeeld: neem $V=\mathbb{R}^2$, $U_1$ de x-as, $U_2$ de y-as, en $U_3$ de diagonaal tussen x- en y-as. Dan gelden dat alle paarsgewijze doorsneden gelijk zijn aan $\{0\}$. En er geldt ook $U_1+U_2+U_3=W$. Toch is het geen directe som, want je kan elke vector op meerdere manieren schrijven als $u_1+u_2+u_3$. 
Kijken we naar de echte voorwaarde van propositie 3.23: $U_1 \cap ( U_2 + U_3 ) = U_1 \neq \{0\}$. Verschil tussen de beweringen: in propositie wordt uit de som 1 deelruimte uitgelicht, en dan gezien dat de overblijvende som niks gemeenschappelijk heeft met de uitgelichte deelruimte (behalve nulvector). Dat is iets helemaal anders dan twee aan twee doorsnedes bekijken. 


\subsection{Lin. (on)afh.}
NB 5.7, opdracht 3.30 p. 106, puntje 1: 
Uitgerekend: $\lambda_1 v_1 + \lambda_2 v_2 + \lambda_3 v_3$ gelijkgesteld aan nulmatrix, stelsel van gemaakt, en opgelost naar $\lambda_i$. Drie $\lambda_i$ zijn effectief 0. Dus lineair onafhankelijk. 


\section*{Les 6, voorbereiding}

\subsection*{1}
Geef een basis voor vectorruimte $\mathbb{R}^{n \times m}$ , dus alle $(n \times m)$-matrices.

Definitie basis terug opgezocht: een basis is vrij (lin. onafh.) en voortbrengend $vct(basis) = V$.
Mij lijkt dat je $m\timesn$ matrices moet hebben van de vorm \begin{pmatrix}
    0 & ... & 0 \\
      & ... & \\ 
    0 &  a_{ij} =1 & 0 \\
      & ... & \\
    0 & ... & 0 
\end{pmatrix} met $i = \{1,2,...m\} $ en $j=\{ 1,2,...n\}$.

Dus voor $2 \times 2$ matrices: $B = \left\{  \begin{pmatrix} 1 & 0\\ 0&0 \end{pmatrix}, \begin{pmatrix} 0 & 1\\ 0&0 \end{pmatrix}, \begin{pmatrix} 0 & 0\\ 1&0 \end{pmatrix}, \begin{pmatrix} 0 & 0\\ 0&1 \end{pmatrix}    \right\} $ is een basis. 

\subsection*{2}
De vectorruimte voortgebracht door $(1, 2, 3)$ en $(4, 5, 6)$. 
Aan welke voorwaarden moeten $a, b, c \in \mathbb{R} $ voldoen opdat de vector $(a, b, c)$ in die voorgebrachte ruimte zit? 

Voortgebrachte ruimte: alle lineaire combinaties $(x+4y, 2x+5y, 3x+6y)$, of $a=x+4y$, $b=2x+5y$ en $c=3x+6y$. 
Nu willen ze dit graag geschreven zien als vergelijking van het vlak door die twee punten en door $(0,0,0)$. 
Eigenlijk staat dit er toch al: parameters $x$ en $y$ te kiezen, en er komen 3 co\"ordinaten uit. 
Algemeen vlak: $lx + my + nz = 0$. Dus stelsel is: 
$\left\{ \begin{aligned}
l1 + m2 + n3 &= 0 \\
l4 + m5 + n6 &= 0 
\end{aligned} \right.$ 
Of 
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
4l + 5m + 6n &= 0 
\end{aligned} \right.$ 
Als en slechts als
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
   - 3m - 6n &= 0 
\end{aligned} \right.$ 
of
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
    m + 2n &= 0 
\end{aligned} \right.$ 

Noem $n=\lambda$, dan: 
$\left\{ \begin{aligned}
l  &=  \lambda \\
m &= -2 \lambda
\end{aligned} \right.$ 
Dus het vlak: $x -2  y + z = 0$

Djuu, ik maak hier een rekenfout, of erger: redeneerfout, en ik vind ze niet. Later opnieuw bekijken! 

\section*{Les 6}

Herhaling concepten voor 6de-jaars in de les. Vectorruimte, vrij of lineair onafhankelijk, lineair afhankelijk. Concept voortbrengend (deel, of verzameling). Concept basis. 

Interessante vraag: is de nulvector dan altijd lineair afhankelijk? Bij conventie, is een verzameling vectoren met de nulvector erbij altijd "niet vrij". 

Vandaag gaan we zien: in een vectorruimte heeft elke basis even veel elementen. (en dat aantal gaan we dan dimensie noemen). 
Basis voor veeltermen: oneindig grote basis, dus dimensie oneindig. Basis: $\{1, X, X^2, X^3,...  \}$

Stelling 3.35, Lemma van Steinitz. I.v.m. voortbrengend deel, dat voldoende heeft, meer elementen zijn lineair afhankelijk, en i.v.m. vrij deel, dat uit vrij deel een vector weghalen, of algemeen minder elementen hebben in een deel, kan nooit voortbrengen. 
Anders gezegd: aantal elementen in een vrij deel is altijd $leq$ van aantal elementen in een voortbrengend deel. 
Bewijs in de les gezien (al op examen gevraagd). Eerst op een manier waarop je zelf bewijs zou proberen te vinden door wat te proberen en te rekenen. 
Stel $\{v_1, ... v_n \}$ is voortbrengend, en stel ik neem een $\{u_1, ... u_k \}$ met $k > m$, dan bewijzen we dat $\{u_1, ... u_k \}$ lineair afhankelijk zijn. Of ook: te bewijzen dat er $\lambda_i$ bestaan zodat $\sum \lambda_i u_j =0$. Bewijs te vinden via som uit te werken (dubbele sommering, en dan omdraaien en wat herwerken).  Wat we dan vinden, en willen zoeken, is eigenlijk een stelsel waarvoor we een niet-nul oplossing zoeken. Het zijn $m$ vergelijkingen (elke sommatie is een vergelijking), met $k$ onbekenden (de $\lambda_j$). En dit soort stelsel heeft altijd een oplossing verschillend van de nuloplossing. Cool bewijske. 
Het rechtstreekse bewijs loopt eigenlijk omgekeerd, en dan komt het stelsel natuurlijk uit de lucht gevallen. 

Veel voorkomende verwarringen: 
\begin{itemize}
    \item de "voldoende" in het bewijs; het is voldoende om $\lambda_1 ... \lambda_k$ te vinden, niet alle 0, zodat voor alle $i=1,...m$ geldt: ... 
    \item verwarring rond $m$ en $k$ ; $k$ is groter, we nemen een of meerdere extra vectoren in onze tweede. 
\end{itemize}


Gevolgen 3.36 en 3.37. Vraag die nog niet beantwoord is: is er wel altijd een basis? (!! dat hadden we nog niet bewezen).
Kan ook aangetoond worden voor eindig voortgebrachte ruimten. 

Eerst een oefening: vectoren $\{ v_1,...,v_n \}$ zijn L.A. als en slechts als er een $v_k$ is die een lineaire combinatie is van de vectoren $\{ v_1,...,v_{k-1} \}$. Dit principe komt terug in komende bewijzen. Niet heel moeilijk te bewijzen (in vergelijking van LC die nul wordt, alles delen door de eerste $\alpha_k \neq 0$). 

Nu beperken we ons tot eindig voortgebrachte vectorruimten. 
Er zijn verschillende manieren om tot een basis te komen: ofwel vertrekkende van lineair onafhankelijke vectoren, extra vectoren toevoegen. Ofwel van voortbrengende set vectoren schrappen. 
En dankzij het lemma weten we dat dit proces altijd stopt. 

Pak een voortbrengend deel. Loop van links naar rechts. Bij elke vector: is hij LC van voorgangers? Zo ja: schrappen. We weten dat hij voortbrengend blijft, want hij was een LC van de vorige. Blijven doen, uiteindelijk krijg je automatisch een vrij deel. 

Opgelet: als er een voortbrengend deel is, dan is er een basis. Maar het is niet zeker dat er altijd een voortbrengend deel is. 

Vraag: hebben alle vectorruimten een basis? Niet zo simpel te beantwoorden. 
Voorbeeld: VR van continue functies op interval 0-1, de basis bestaat, maar niemand kan hem neerschrijven. 
De kwestie hangt samen met het zogenaamde keuze-axioma in de verzamelingenleer. 

Stelling 3.45: aantal equivalente beweringen. Niet schokkend. 

Noot: ben niet meer helemaal mee met wanneer juist een basis te vinden, wanneer ze bestaat, of ze eindig is, etc. Nog eens laten bezinken. 

(pauze)

De co\"ordinatenafbeelding. Als er een eindige basis is, kan kunnen we elke vector op \emph{unieke} wijze beschrijven als LC van de basisvectoren. Het bestaan is triviaal (voortbrengend). Het unieke: uniciteitsbewijs in les gezien. (heel simpel, te maken met het vrij karakter van een basis). 
Notatie: de co\"ordinaat of de co\"ordinaten van vector $v$ ten opzichte van $\beta$, wat een afbeelding is van $V$ naar $\mathbb{R}^n$. Ze is bijectief, en ze bewaart de som en het scalair product. 
Goed over nadenken: er is een \"e\"en op \"e\"en verband tussen $\mathbb{R}^n$ en elke eindig-dimensionale vectorruimte! We noemen dit isomorf. Dus alles wat je kan bewijzen in $\mathbb{R}^n$, geldt ook voor een willekeurige eindig-dimensionale vectorruimte. 

Terug naar somruimte/direct som. (direct som als en slechts als doorsnede alleen de nulvector bevat). Vraag: wat is de dimensie $dim(U+W)$. Voor directe som is dit som van de dimensies, voelen we aan: vrijheidsgraden apart, want elke som te kiezen met ene in $U$ en ene in $W$. Voor algemene som: dimensie doorsnede aftrekken van som van dimensies. Zie stelling 3.53. Heel belangrijke stelling. 

Stelling 3.47 gezien. Evident, maar nog niet bewezen. Bewijs van eerste deel in woorden gezien in de les. Constructief, begin met 1 vector en voeg toe zolang er vectoren zijn die geen LC zijn van onze set vectoren. Dit proces stopt omdat $V$ eindig dimensionaal is. 

Bewijs van stelling 3.53. Dimensiestelling voor somruimten. (later: dimensiestelling voor ...). Het principe bij die bewijzen is: goede basis construeren en tellen hoeveel vectoren daar in zitten. Start: kies een basis voor de doorsnede $U \cap W$. Uitbreiden tot basis voor $U$, dat kan. Je kan ook uitbreiden tot basis van $W$, dan krijg je andere set. We hebben dus drie basissen nu, met ene een deelverzameling van de twee uitgebreide. De unie van die twee uitgebreide is een basis voor de somruimte. (denk bvb. aan 3D ruimte met rechten en vlakken). Nu nog bewijzen dat dit een basis is voor $U+W$. Want als dit zo is, hebben we een basis voor elke ruimte in de formule, en blijkt ook dat de formule klopt. 
Dan bewijs zelf dat die voorgestelde verzameling een basis is van $U+W$. Moet dus vrij zijn, en voortbrengend. Het voortbrengend stuk: makkelijk aan te tonen. Het bewijs dat deze vrij is, is moeilijker. 
Het vrij zijn van de voorgestelde basis voor $U+W$: neem een lineaire combinatie die nul is, en toon aan dat alle co\"effi\"enten nul zijn. Stap 1 is de $\sum w_i$ naar andere kant brengen. Dan staat links een vector in $U$ want LC van basis van $U$. Rechterlid is vector in $W$. Een vector in $U$ gelijk aan een vector in $W$, moet in doorsnede zitten. We hebben een basis voor die doorsnede, dus die vector kan ook geschreven worden als LC van die basisvectoren. Opnieuw alles aan een kant in die vergelijking, zodat we een LC krijgen gelijk aan nul. Het is een LC van $u_i$ en $w_i$, allemaal vrije vectoren. Kan alleen nul zijn, als alle co\"effi\"enten nul zijn. Invullen in eerdere vergelijking, geeft dat ook een hoop andere co\"effi\"enten nul zijn. QED. 

\section*{Les 6, nabeschouwing}

1. Toon aan dat het omgekeerde van Stelling 3.49 ook geldt. 

Gegeven: $\beta = \{e_1, \dots , e_n\}$ is een eindige deelverzameling van $V$, en elke vector $v$ kan op een
unieke manier geschreven kan worden als een lineaire combinatie van de vectoren van $\beta$. 

Te bewijzen: $\beta$ is een basis van $V$.

Bewijs: 
We moeten 2 zaken bewijzen:  (1) $\beta$ is voortbrengend, en (2) $\beta$ is vrij. 
Voor (1): aangezien gegeven is dat elke vector $v$ geschreven kan worden als LC van de vectoren in $\beta$, is $\beta$ voortbrengend, dus dit deel is triviaal. Voor (2): we bewijzen dat $\beta$ uit het ongerijmde. Stel $\beta$ niet vrij, bestaan er $\lambda_i$ niet alle $0$ zodat $\sum_{i=1}^n \lambda_i e_i = 0$ (vgl.1). Gegeven is dat een willekeurige $v \in V$ kan geschreven worden als $v=\sum_{i=1}^n a_i e_i$ (vgl.2) met alle $a_i$ uniek. Echter, tellen we (vgl.1) op bij (vgl.2), kan krijgen we: $v=\sum_{i=1}^n (a_i + \lambda_i) e_i $. Aangezien $\lambda_i$ niet alle $0$ zijn, hebben we een tweede manier gevonden om $v$ te schrijven als LC van $e_i$, dus waren onze $a_i$ niet uniek, wat een tegenspraak is met ons uitgangspunt. Bijgevolg klopt onze aanname dat $\beta$ niet vrij is, niet. En dus moet $\beta$ vrij zijn. QED. 

2. Maak Opdracht 3.52 (p. 118).  
Ik zie wel dat als er een lineair afhankelijke vector bij zit, dat er dan door opeenvolgende rijoperaties een nulrij kan gemaakt worden, en dat dan de determinant nul is. Maar ik ben niet zeker dat "det = 0" en "er kan een nulrij gemaakt worden" equivalent zijn.  "er kan een nulrij gemaakt worden" => "det = 0" (want det verandert niet van niet-nul naar nul door rij-operaties). Omgekeerd: geldt "det = 0" => "er kan een nulrij gemaakt worden"?? Toch nog wat moeilijk, zo vlot nadenken over stelsels, determinanten etc. 

3. Controleer alle beweringen van Voorbeeld 3.32(2) op p. 107.
Raar, het woordje "controleer", want je moet iets bewijzen. Bewijs: stel dat we een eindige $D$ hebben die $V$ (alle veeltermen) voortbrengt. Dan kunnen we voor elke $d \in D$ de hoogste graad bepalen, en $D$ ordenen volgens graad: $(d_1, d_2, ..., d_n)$, met $d_n$ de vector in $D$ met de hoogste graad. Stel dat de graad van $d_n$ gelijk is aan $k$. 

\section*{Les 7, voorbereiding}

niet helemaal gedaan; best nog eens bekijken want zijn interessante opdrachten. 

1. Zij $v1 = (1, 1, 1), v2 = (1, 0, 1), v3 = (0, 1, 0)$ vectoren in $\mathbb{R}^3$.

(a) Laat zien dat $vct\{v1, v2, v3\} = \{(x, y, z) \in \mathbb{R}^3 \mid x = z\}$. 
Voortgebrachte ruimte is de rijruimte van 
\[
\begin{pmatrix}
    1&1&1\\
    1&0&1\\
    0&1&0
\end{pmatrix}
\] 
maar ook van (rij-operaties bewaren de rijruimte): 

\[
\begin{pmatrix}
    1&0&1\\
    0&1&0\\
    0&0&0
\end{pmatrix}
\]

Alle vectoren in $vct\{v1, v2, v3\}$ zijn dus van de vorm: 
$\lambda_1 (1,0,1) + \lambda_2 (0,1,0) = (\lambda_1, \lambda_2, \lambda_1)$. QED. 

\section*{Les 7}

Uitdunnen van elk voortbrengend deel tot een basis, we weten dat dat kan, maar hoe doe je dat? We willen graag een algoritme. Of omgekeerd: gegeven een vectorruimte, hoe vinden we op een effici\"ente manier een basis? 

Definitie 3.61. Wat we doen hier: associ\"eren van vectorruimten met matrices, via rijruimten, kolomruimten, nulruimte. 
Voor elke $vct$ definitie kan je de voortgebrachte ruimte schrijven als een rij- of kolomruimte. 

We gaan zien: kolomruimte kunnen we uitdunnen tot een basis. Rijruimte kunnen we ... (stukje gemist). 

Definities: rijrang en kolomrang. En we gaan zien: ze zijn altijd gelijk! Dus we gaan ze gewoon rang noemen. 


Stelling 3.64: nulruimte en rijruimte veranderen niet door rijreductie. Punt 1 van stelling kennen we al: stelsel oplossen doen we via rij-operaties. De twee eerste ELO's: evident dat de rijruimte niet verandert. Maar de 3e ELO iets minder evident: moet je in twee richtingen bekijken, de rijruimte na ELO3 is sowieso deelruimte van de rijruimte voor de ELO. Maar we kunnen rijoperaties ongedaan maken, dus geldt het in andere richting ook. 
Derde punt van stelling: over de dimensies. 
Redenering om derde punt te bewijzen. Houd de echelonvorm voor ogen. Dimensie $N(U)$. Bij een stelsel: aantal vrijheidsgraden is van belang! Vrije variabelen en gebonden variabelen. In termen van vrije/gebonden variabelen: aantal vrije variabelen is de dimensie van de nulruimte. En de dimensie van $R(U)$: in de echolonvorm zie je een basis staan: de rijen die niet-nul rijen zijn. De basis van de rijruimte = de niet-nul rijen van $U$. Dus 
dimensie van $R(U)$ is aantal gebonden variabelen. Som hiervan is $n$ (aantal rijen). We hebben dus al gezegd hoe we die basis krijgen: gewoon de rijen van echolonvorm zijn een basis. 

Dan naar kolomruimte gekeken. De basis hiervoor zijn de kolomen die horen bij een gebonden variabele. MAAR: de kolomruimte blijft \emph{NIET} behouden bij rij-operaties. De gebonden/ongebonden variabelen blijven wel dezelfde door rij-operaties. Maar de kolomruimte verandert bij de ELO's. 

Goeie vraag in hoorcollege: zegt aantal nulrijen iets? Antwoord: nee, je kan er altijd toevoegen, en er verandert niks aan de dimensies. 

Nu over naar afbeeldingen tussen structuren. We zijn ge\"interesseerd in afbeeldingen tussen vectorruimten die structuur van optelling en scalaire vermenigvuldiging bewaren. Lineaire afbeeldingen. 

Eerste paar voorbeelden van lineaire afbeelding. O.a. co\"ordinaatafbeelding $co_\beta$, die elke vector in $V$ afbeeldt op een vector in $\mathbb{R}^n$. Ook deze afbeelding is lineair. 

Definitie 4.1. Ook: lineaire transformatie, lineaire vorm gedefineerd. 

Nog een voorbeeld van een lin. afb. De rotatie rond $o$ in het vlak (of de ruimte). Een spiegeling rond/langs rechte door $o$ in vlak of ruimte. Wat zijn niet-lineaire transformaties: een verschuiving bvb. Want: de nulvector moet de nulvector blijven. 

Nog helemaal ander voorbeeld uit de analyse. Vectorruimte $V$ van afleidbare functies van $\mathbb{R}$, en vectorruimte $W$ van alle functies. $D: V \to W: D(f) = f'$ (afleiden) : is een linaire afbeelding. Want afgeleide van som van functies is som van afgeleiden, ... 
Ook integreren: afbeelding van continue functies naar afleidbare functies. Voorbeeld op bord: functies van $[0,1]$ naar $\mathbb{R}$. Afbeelding $I: V \to W: f \mapsto I(f)$ en $I(f)(x) = \int_0^x f(t)dt$ is een lineaire afbeelding. En $D \circ I $ is de identieke functie. 

Lemma 4.2, gevolg 4.3: basiseigenschappen van lineaire afbeeldingen. O.a. een lineaire afbeelding ligt volledig vast door beelden van een basis!! Heel belangrijke eigenschap. Willekeurige afbeeldingen/functies hebben deze eigenschap niet. 

Heel fundamenteel voorbeeld: lineaire afbeelding van $\mathbb{R}^n$ naar $\mathbb{R}^k$: $L_A(X)=A \cdot X$, met $A \in \mathbb{R}^{kxn}$. 

Na de pauze: nog een veralgemening van elke lineaire afbeelding van $\mathbb{R}^n$ naar $\mathbb{R}^k$: kan altijd door een matrix voorgesteld worden. 

pauze: \url{https://www.youtube.com/watch?v=uqwC41RDPyg} 

Conventie: vectoren uit $\mathbb{R}^n$ en $\mathbb{R}^k$ stellen we voor als kolommen zodat die matrixvermenigvuldiging klopt qua dimensies. 

Eigenschap: elke lineaire afbeelding $L: \mathbb{R}^n \to \mathbb{R}^k$ is van de vorm $L=L_A$ (matrixvermenigvuldiging van kolommatrix naar kolommatrix). 
Bewijs op bord gezien. 
Vluchtig samengevat: via standaardbasis $\{ e_1, ...\}$, en ..., en definitie van $A$ als matrix met kolommen $L(e_1) ... L(e_n)$. Dan staat daar bij die sommatie dus een matrixvermenigvuldiging. 

Lineaire afbeeldingen en matrices zijn dus echt wel zeer nauw verwant, om niet te zeggen: hetzelfde. 

Algmeen: $V$ vectorruimte met gegeven basis, en $W$ andere vectorruimte met gegeven basis, en een $L: V \to W$. Definitie: matrix $L_{\beta_V}^{\beta_W}$ die zo'n transformatie defineert. En die hangt af van de basis (vandaar die $\beta$ in de notatie bij $L$.  
We gebruiken hiervoor co\"ordinaten van vectoren, eigenlijk van de getransformeerde basisvectoren van $V$. Die co\"ordinaten zetten we in de kolommen van die matrix. We hadden gezien dat de transformatie van basisvectoren de volledige lineaire transformatie bepaald, dus we kunnen inzien dat alle informatie voor $L$ in deze matrix zit. 

Voorbeeld van transformatie tussen twee dezelfde vectorruimten van veeltermen graad $\leq$ 2, afbeelding die afgeleide van veelterm geeft. (bijgevolg is beeld een deelruimte, graad $\leq$ 1). Voorbeeld eerst gezien met basis zelfde voor $V$ en $W$:  $(1,X,X^2$. Dan eens met andere basis om te zien dat er andere matrix uitkomt. Basis $V$ behouden van vorige voorbeeld, basis $W$ nu $\{ 1+X^2, 1+X, X \}$. Geeft andere matrix. 

Recap: we maken een matrix van een lineaire afbeelding, maar de matrix verandert met de basis die we kiezen. Maar de vraag is: hoe verandert die matrix? Want het is dezelfde afbeelding die we beschrijven. Dat zien we volgende week. 

Voor lineaire transformaties (afbeeldingen op een ruimte), nemen we typisch dezelfde basis voor $V$ en $W$. We zullen volgende week zien dat de determinant dezelfde blijft voor alle transformaties, ook al verandert de basis. 

Eigenschap van $A = L_{\beta_V}^{\beta_W}$. Voor alle $v \in V: co_{\beta_W}(L(v)) = A \cdot co_{\beta_V}(v)$. Voorstelling in vierkant met pijlen beetje zoals op de kaft van boek, met hoekpunten $V$, $W$, $\mathbb{R}^n$ en $\mathbb{R}^k$. Je van van $V$ naar $\mathbb{R}^k$ op twee manieren: eerst $L$, dan co\"ordinaten nemen, of eerst co\"ordinaten nemen en dan $L_A$ toepassen. 
Bewijs in de les gezien. Heel kort, is maar paar lijnen. Maar complex met $V$ en $W$ met verschillende basissen en co\"ordinaten. 

Recap: 
elke lineaire afbeelding heeft een matrix. Matrices hebben veel eigenschappen, bvb. we kunnen ze optellen. We gaan zien dat we ook lineaire afbeeldingen kunnen optellen en zo. En we kunnen ook de verzameling van alle lineaire afbeeldingen beschouwen, en dit is ook een vectorruimte, want ook hier kunnen we optellen en scalair vermenigvuldigen. 

Propositie 4.11 vat dit samen. 

\section*{Les 7, nabeschouwing}

Voorbeeld 3.62 gelezen en over nagedacht. Ik snap het wel, maar kan niet direct bewijzen of zeggen welke uitspraak van welke definitie of stelling af te leiden is. 

Nulruimte: basis vinden, stukje op p. 125 gelezen, en voorbeeld op p. 126. Gegeven stelsel (x,y,z,t,u) niet gedaan, maar lijkt me doenbaar. 

Dan 3a: matrix $A$ is heel simpel, gewoon die drie vectoren als rijen nemen en klaar, want is definitie van $R(A)$. Voor 3b: uitleg p. 126 gelezen. Maar niet uitgerekend, klassiek matrixrekenen, zal wel lukken. 

Puntje 4: niet gedaan maar moet ik zeker nog eens doen! TODO. 

Puntje 5: Maak oefening 4.8.1a: L3 en L5. (p.185). L3 niet lineair (neem $-(x,y)$, beeld gaat niet $-L((x,y))$ zijn. L5: lijkt me wel lineair, alleen lineaire operaties. Wel niet helemaal uitgeschreven voor optelling en scalair product. 

Puntje 6: nog niet gedaan.

Puntje 7: nog niet gedaan. Deze best eens doen! TODO. 

\section*{Les 8, voorbereiding}
vergeten te doen, wel snel gelezen voor de les, en snap wel waar het over gaat. 

\section*{Les 8, 21 nov}
Abstracte variant van matrices gezien: de lineaire afbeeldingen. 

Even recap wat matrix van lineaire afbeelding is. 
Per definitie is de matrix $L_{\beta_V}^{\beta_W}$ met als kolommen de co\"ordinaten van de beelden van de basisvectoren van $V$. En in het vierkante schema met $V, W, \mathbb{R}^n, \mathbb{R}^m$ is de matrix de afbeelding van $\mathbb{R}^n$ naar $\mathbb{R}^m$.

Propositie 4.11. Verzameling van alle lineare afbeeldingen van $V$ naar $W$ ook een re\"ele vectorruimte. Notaie voor deze verzameling: $Hom(V,W)$. 

We hebben nu heel veel verbanden gezien tussen lineaire afbeeldingen en matrices. Definitie 4.12 en stelling 4.13 gaan over dit homomorfisme. $Hom_{\mathbb{R}}$. Dit is redelijk logisch. Alleen hebben we het product van twee matrices nog niet gezien in dit homomorfisme. 

Stelling 4.14: samenstelling van twee lineaire afbeeldingen is opnieuw lineair. Bewijs in de les gezien. 
V.T.B. (voldoende te bewijzen): samenstelling respecteert lineaire combinaties: $(K \circ L) (\alpha v + \beta w) =...$. Via $K(L(...))=...$ en wat herschikken. Eenvoudig bewijs. 

Eigenschap: de matrix van $K \circ L$ is het matrixproduct van de individuele afbeeldingen. Hier komen we eigenlijk op het punt dat verklaart waarom de matrixvermenigvuldiging is zoals ze is. Bewijs is kort, maar best abstract. Neem vector $v \in V$. Dan is $co_{\beta_U}( (K \circ L) (v) ) =... $. Gaan we op twee manieren uitrekenen: eerst door $K \circ L$ als \'e\'en lineaire afbeelding te zien en te schrijven als matrix-vermenigvuldiging. Dan uitrekenen door te schrijven als $co$ van $K (L(v))$. Door associativiteit matrixproduct komt een gelijkheid te voorschijn, maar je kan hier niet direct uit afleiden dat nieuwe matrix van samenstelling gelijk is aan product (je kan kolomvector niet inverteren, dus kolomvector niet wegwerken). Omweg nodig: neem de basisvectoren stuk voor stuk in de vergelijking, dan komt telkens de $i$-de kolom te voorschijn, dus zijn alle kolommen van deze matrices gelijk, dus is hele matrix gelijk. Punt is: vierkante matrix maal kolommatrix heeft opnieuw kolom, en voor 1 bepaalde kolommatrix in een vergelijking is er niet 1 oplossing! Dus sowieso andere redenering nodig. Ik dacht eerst dat het sowieso duidelijk was, maar dat is niet! 

Definitie 4.16. Stelling 4.18. Isomorfisme: bijectieve afbeelding die alle structuur bewaart. Het isomorfisme dat we al hele tijd gebruiken: de co\"ordinaatafbeelding. Stelling 4.18 vult het gebrek aan symmetrie aan in de definitie van 4.16 (om daar van $V$ naar $W$ wordt gewerkt en niet omgekeerd, terwijl uiteraard omgekeerd ook het isomorfisme geldt). 

Bewijs stelling 4.18 op bord gezien. Ook interessant. Eerst via definitie lineariteit van $L^{-1}$ een gelijkheid genoteerd.  V.T.B. is dat $L$(linkerlid) = $L$(rechterlid), en dit is voldoende omdat $L$ injectief is. Aantonen dat twee vectoren in $V$ gelijk zijn, kan ook door aan te tonen dat hun beelden door injectieve functie gelijk zijn. Dan gewoon uitrekenen, waarbij $L(L^{-1})$ gaat wegvallen. 

Voorbeelden. 

Het woord standaardbasis is paar keer gevallen (ook in oefenzitting). Maar hadden we dit gezien? Spreekt wel voor zich eigenlijk, maar toch... 

Stelling 4.20. Als isomorf, dan dimensies gelijk. Waarom? Eigenlijk, alle eigenschappen blijven bewaard, dus dit is evident. Omgekeerd. Als dimensies gelijk, zijn ze beide isomorf met $\mathbb{R}^n$ met zelfde $n$, dus ook onderling isomorf. 

Vraag: kan re\"ele vectorruimte isomorf zijn met complexe vectorruimte? In principe valt dat buiten de definitie. Maar: je kan voor complexe vectorruimten altijd jezelf beperken tot scalairen uit $\mathbb{R}$ en daar wel mee aan de slag.

Propositie 4.23. Verband tussen isomorfismen en inverteerbaarheid van matrices. En de inverse afbeelding $L^{-1}$ heeft als matrix de inverse matrix. In les alleen in woorden uitgelegd. 

(pauze) 

Basisverandering. Stel twee basissen $\beta_1 en \beta_2$. Verband tussen de twee co\-\"or\-di\-naat\-afbeeldingen? Vraag twee: stel we hebben een $L$ en we veranderen de basis, wat is het verband tussen de matrices van de lineaire afbeeldingen? 

I.v.m. vraag 1: bekeken via de identieke afbeelding, en de matrix t.o.v. de verschillende basissen. Matrix $P$ gevonden die dan verband geeft tussen de twee co\"ordinaten. Voorbeeld gegeven. (maar we hadden al zoiets in de oefeningen gedaan). 

Dan naar vraag 2: lineaire transformaties, en basisverandering. Uitgewerkt op bord, vertrekken van definitie van toepassen matrix van $L$ op een $co_{\beta_2}(v)$. Dan erbij geschreven formule uit vraag 1 ($P$ maal $co_{\beta_1}(v)$). Dan verder gerekend ... Ook gebruik gemaakt van feit dat $P$ inverteerbaar is. Waarom? Omdat identieke afbeelding (waar die $P$ vandaan kwam) inverteerbaar is. We bekomen opnieuw (zoals voor de pauze bij samenstellen van afbeeldingen) een gelijkheid van matrixproducten met rechts een kolom, en die geldig is voor alle mogelijke kolomvectoren. Dus zijn die matrices voor de kolommatrix gelijk. Resultaat: $L_{\beta_2}^{\beta_2}  = P \cdot L_{\beta_1}^{\beta_1} \cdot P^{-1}$. Naam van dit soort eigenschap: gelijkvormigheid. Definitie 4.26. Gevolg 4.27. 
Veralgemening: met vier basissen in plaats van twee. Maar in de praktijk niet vaak nodig. Zie boek. 

Toepassing: determinant en spoor van een matrix blijven behouden als $A$ en $B$ gelijkvormige matrices zijn. Bewijzen kort op bord, via definitie, eigenschap det(product matrices), eigenschap det(inverse matrix), etc. Voor spoor: via eigenschap $Tr(AB)=Tr(BA)$. 

Even voorbereiden volgende les: de kern. En het beeld. En de rang.  Gaat over lineaire afbeeldingen (matrix equivalent kern: nulruimte, matrix equivalent beeld: kolomruimte). Logisch: de beelden van de basisvectoren zitten in die matrix, dus dat beeld van de afbeelding equivalent is met de kolomruimte is logisch. 
Definitie 4.31. Even stilgestaan bij feit dat dit deelruimten zijn (lineaire combinatie blijft behouden in ker en Im). 

Propositie 4.34. I.v.m. injectiviteit van $L$ asa $ker(L)=\{0\}$. Bewijs op bord gezien. 

\section*{Les 9, 28 nov}

Vandaag: dimensiestelling voor lineaire afbeeldingen bewijzen. 
Stelling 4.36. 

Bij matrices: verschillende deelruimten gezien (C, R, N), en lineaire afbeeldingen zijn abstractie van matrices, dus deze dimensiestelling gaat ook verband houden met rang van matrices. 
Denk bvb. aan: dimensiestelling van deelruimten van $V$, en hoe die bewezen is door vinden van een basis. 

Bewijs voor 4.36. Begin met basis voor de kern. ($Ker L$ is deelruime van $V$). Basis deelruimte kunnen we uitbreiden tot basis van hele ruimte $V$. (dit kan, want elke vrij deel in $V$ kan uitgebreid worden). Noemen we deze basis $\{ v_1 ... v_n\} $, dan zal gelden dat $Im L = vct\{ L(v_1), ..., L(v_n)  \}$. Maar $L(v_1)=0, etc.$ (definitie kern). Kunnen we weglaten uit $vct$, dus we hebben voortbrengend deel voor $Im L$. Nu nog aantonen dat deze LO (vrij) zijn, en we zijn er. Ook dit staat er al bijna. Neem LC die nul is, en bewijs dat co\"effi\"enten nul zijn. 
$L()$ operator kan buiten gebracht worden, dus die LC vector zit in de kern. Elke vector in de kern kan als LC geschreven worden van kernvectoren. Dit uitschrijven geeft opnieuw LC gelijk aan nul, met alle vectoren $v_1,...,v_n$ erin, en hiervan weten we dat deze LO zijn, dus alle $\lambda_i$ zijn nul, ook de $\lambda_i$ van onze "te bewijzen". 

Interpretatie van deze stelling voor matrices. Neem matrix $A$ en bijhorende lineaire afbeelding $L_A$. De kern zijn dan de oplossingen van het homogene stelsel, of ook: de nulruimte $N(A)$. Wat is $Im(L_A)$? Neem standaardbasis van $\mathbb{R}^n$. Dan is $Im(L_A) = vct \{ L_A(e_1), ..., L_A(e_n)  \} = vct \{ 
 \text{kolom 1 van }A, \text{kolom 2 van } A, ... \} = C(A)$.  
Dus, dimensiestelling zegt: $n = dim(\mathbb{R}^n) = dim(Ker L_A) + dim(Im L_A) = dim(N(A)) + \text{kolomrang } A$. 
Eerder gezien: $n = dim( N(A)) + \text{rijrang } A$. 
En dus: kolomrang = rijrang. 
Even over redeneren: voor $k \times n$ matrix, wat is hoogst mogelijke rang? Antwoord: kleinste van de twee. Logisch: dim rijruimte nooit groter dan aantal rijen. Maar dim kolomruimte nooit grotere dan lengte van kolommen, dit is ook aantal rijen.   

Verder op redeneren. Neem $A \in \mathbb{R}^{k \times n}$ met rang $k$. Dit kan alleen als $C(A) =\mathbb{R}^{k} $. Dit kan alleen als $ \text{Im } L_A = \mathbb{R}^{k}$. Asa voor elke $b \in  \mathbb{R}^{k}$ er een $X$ bestaat met $A \cdot X = b$. Asa elke $AX=b$ heeft minstens 1 oplossing. 
Maar dit alles is ook equivalent met: rijen van $A$ zijn lineair onafhankelijk. Bovenstaande redenering is stelling 4.45. 

Andere geval. Neem $A \in \mathbb{R}^{k \times n}$ met rang $n$.
Uit dimensiestelling volgt dan: $dim(N(A)=0$ , of $N(A)=\{ 0 \}$. Of $Ker (L_A) = \{ 0 \}$. Of: $L_A$ is injectief. Dan heeft $AX=b$ voor elke $b$ hoogstens \"e\"en $X$. Asa kolommen van $A$ zijn lineair onafhankelijk. Bovenstaande is stelling 4.46. 

Stelling 4.47 voor $n \times n$ met rang $n$. 

Interessant gevolg: voor $L_A$ in eindig-dimensionale vectorruimte (lineaire transformatie dus), is injectiviteit hetzelfde als surjectiviteit! Zie gevolg 4.40. 
"Tegenvoorbeeld" voor oneindig-dimensionale ruimte: veeltermen van willekeurige graad. En neem $L: V \to V: L(P) = P \cdot X$. Dus elke term met graad verhogen. Dit is injectief maar niet surjectief, omdat constante veeltermen niet bereikt worden. 

(pauze) 

Nieuw hoofdstuk: eigenwaarden/eigenvectoren. Veel toepassingen. Bvb. page rank Google is een eigenvectorprobleem van gigantische matrix. Ander voorbeeld: goed passende ellips rond een puntenwolk met hoge correlatie, die ellips bepalen kan met eigenwaarden. 

Voorbeeld evolutie van marktaandeel bij 2 telecomoperatoren. Gelijkaardig: populatiemodel met prooien en jagers en voortplanting. Populatiemodel marktaandeel: klanten die van operator veranderen: 99\% van P blijft trouw, 1\% stapt over. Bij T: 94\% trouw, en 6\% stap over. Hoe evolueert aantal gebruikers/klanten maand na maand. Dit kan makkelijk gemodeleerd worden met linaire algebra. Twee rijen variabelen $x_1, x_2, ...$ klanten van P in maand $1, 2, ...$. En $y_1, y_2, ...$. Dus elke maand kan nieuwe $(x_2, y_2)$ berekend worden door matrixproduct met vorige $(x_1, y_1$. En na 10 maanden: $A^10$, ... Machten van matrices dus. Wat we met eigenwaarden gaan kunnen doen, is ook machten van matrices berekenen! 
Vraagstuk convergeert. En eigenlijk was evenwicht te voorspellen: als er evenveel overlopers zijn aan elke kant, dan verandert het niet meer. Evenwicht: $A \begin{pmatrix} 6 \\ 1\end{pmatrix} = \begin{pmatrix}  6 \\ 1 \end{pmatrix}$. Dus $(x,y)$ verandert niet meer. 

Beperking: het gaat alleen over vierkante matrices. $n$ variabelen in en $n$ out van onze transformatie. 

Definitie eigenwaarde $\lambda$ van matrix $A$: ... $A \cdot v = \lambda v$, $v \neq 0$, en deze $v$ noemen we een eigenvector van $A$. Voor lineaire transformaties: zelfde definitie via equivalent matrices en lineaire transformaties. Het evenwicht daarstraks was met een eigenwaarde 1. 

Vinden van alle eigenwaarden van $A$ (en bijhorende eigenvectoren). $A \cdot v = \lambda v$ asa $(\lambda \mathbb{I}_n - A)\cdot v = 0$ asa stelsel $(\lambda \mathbb{I}_n - A)\cdot v = 0 $ heeft niet alleen nuloplossing, asa $det(\lambda \mathbb{I}_n - A) = 0 $. 

Voorbeeld. Neem vierkante matrix $ \begin{pmatrix} a & b \\ c & d \end{pmatrix}$. Uitwerken geeft: $\lambda^2 - (a+d)\lambda + (ad-bc)=0$.

Voor $A \in \mathbb{R}^{n \times n}$: we bekijken $\phi_A(X) = det(X \cdot \mathbb{I}_n -A)$, een $n$-de graadsveelterm, en we zoeken hiervan de re\"ele nulpunten. Definitie: $\phi_A(X)$ is de karakteristieke veelterm van $A$. 

Meetkundige betekenis: lineaire transformaties in $\mathbb{R}^2$, dus  $2 \times 2$ matrices,  kan je zien als transformaties in het vlak. Bvb. spiegeling t.o.v. x-as. We zien al dat er eigenwaarde 1 gaat zijn (punten op as naar zichzelf), en andere punten naar spiegelbeeld die geen eigenwaarde hebben. 
Andere transformatie: rotatie. Nul telt niet mee, dus nooit op zichzelf afgebeeld, dus geen eigenwaarden hier. (geogebra.org). 
Heel interessant om wat inzicht te krijgen. Afgebeelde vector moet in verlengde komen van originele (maal re\"eel getal $\lambda$). 

Karakteristieke vergelijking voor matrices: kan zelfs met re\"ele elementen in matrix complexe eigenwaarden uitkomen, en bijhorende complexe eigenvectoren. Maar tellen niet mee hier. En bij lineaire transformaties overigens ook geen betekenis. (?? niet helemaal gesnapt hier). 

Intermezzo, in verband met matrices met veel eigenwaarden. Neem transformatie $L: V \to V$ en basis $\beta$. Dan bestaat $\beta$ uit eigenvectoren asa $L_\beta^\beta$ is een diagonaalmatrix. 
Bewijs: op bord gezien. Eigenlijk logisch, diagonaalmatrix vermenigvuldigt elke $v_i$ met $\lambda_i$ (uit diagonaalmatrix). 

Definitie 5.7. Diagonaliseerbaarheid. Belangrijk concept! Heeft te maken met "genoeg" eigenwaarden te hebben om de diagonaalmatrix te vullen. 

Extreme gevallen: eenheidsmatrix: elke vector is een eigenvector voor de eenheidsmatrix, met eigenwaarde 1. 


\section*{Les 10, voorbereiding}
niet gedaan

\section*{Les 10, 5 dec}

Even herhaling concepten: def. 5.3 eigenvector/eigenwaarde van $L$, en $L$ is abstractie van matrix dus geldt dit ook voor matrices, def. 5.7 diagonaliseerbaar: te maken met "genoeg eigenvectoren" in de betekenis "genoeg om een basis te maken". Noot: het hebben van een matrix met basisvectoren is hetzelfde als diagonaliseerbaarheid. Prof heeft veel inzichtelijke uitleg gegeven, maar ik vond het moeilijk te volgen. 

Vandaag eerst: hoe begin je aan diagonaliseren? Welk algoritme hebben we? We hebben een $L$ op $V$, met 1 basis $\beta$, en met $dim V=n$. En de matrix $A = L_{\beta}^{\beta}$. Eigenwaarden van $L$ vinden: nulpunten van karakteristieke veelterm $\phi_A(X)$. Dit is een n-de graadsveelterm. Stelling 5.9 zegt dan: die veelterm is karakteristiek voor $L$, want staat los van de basis. Andere basis is andere matrix $A$, maar er zal dus blijken dat gelijkvormige matrices dezelfde hebben. Vergelijk met: gelijkvormige matrices hebben dezelfde determinant. Noot: $X \cdot \mathbb{I}_n$ kan je zien als $X$ keer de identieke afbeelding. 
Dan de eigenvectoren vinden: dat doen we voor elk nulpunt $\lambda$ van $\phi_A$, door de niet-nul oplossingen te vinden van het stelsel $(\lambda \cdot \mathbb{I}_n -A) \cdot v = 0$. 

Nu paar voorbeelden met 2x2 matrices. Eerst $det( X \cdot \mathbb{I}_2 - A)$, dan re\"ele nulpunten zoeken. Eigenvectoren dan per eigenwaarde door eenvoudigweg een stelsel op te lossen. In voorbeeld waarbij we twee eigenwaarden hadden voor een 2x2 matrix: de matrix is dan diagonaliseerbaar. Eerste voorbeeld: geen re\"ele eigenwaarden. Tweede voorbeeld: twee verschillende re\"ele eigenwaarden. Derde voorbeeld: slechts 1 re\"ele eigenwaarde. Eerste en derde voorbeeld: niet diagonaliseerbaar. 
Noot: deze voorbeelden zijn zeer instructief. Om te diagonaliseren hebben we twee lineair onafhankelijke vectoren nodig in $\mathbb{R}^2$. 
Dan nog 4e voorbeeld gegeven: $A = \begin{pmatrix}
    2 & 0 \\ 0 & 2
\end{pmatrix}$. Alle vectoren verschillend van $0$ zijn hier nu eigenvectoren. 

Nieuw jargon, maar geen nieuw concept: def. 5.12: spectrum van $L$. 

Definitie 5.13: alg. multipliciteit $m$ van een eigenwaarde (op basis van multipliciteit van een nulpunt van de KV). Te maken met meervoudige nulpunten. Ook te maken met schrijven een veelterm $P \in \mathbb{R}[X]$ als $P(X) = (X-\lambda_1)^{n_1}(X-\lambda_2)^{n_2} ...$. Anders gezegd: $(X-\lambda)$ buiten brengen tot overblijvende $Q(X)$ geen nulpunt $\lambda$ meer heeft. 

Definitie 5.14: meetkundige multipliciteit $d$: de dimensie van de eigenruimte van de eigenwaarde. 

We hebben dus per $\lambda$ twee multipliciteiten: de algebra\"ische $m$ en de meetkundige $d$. 

Waar we naartoe werken: stelling 5.16 bewijzen. Voor elke eigenwaarde is $d \leq m$. 

Bewijs. Gegeven is $L: V \to V$ en $\lambda$ is een eigenwaarde. T.B. $1 \leq d(\lambda) \leq m(\lambda)$. 
Bewijs: eerste ongelijkheid is triviaal (er is minstens een oplossing anders was het geen eigenwaarde). Tweede ongelijkheid: neem basis voor eigenruimte $E_\lambda$. Breid deze basis uit tot basis van $V$. Stel dan $A=L_\beta^\beta$. En dan beschouwen we $A$ als kolomvectoren die co\"ordinaten zijn van de beelden van de basisvectoren. Omdat we basisvectoren uitdrukken in co\"ordinaten van basisvectoren, zijn deze vectoren allemaal met nullen en $\lambda$! Die hele $A$ wordt in blokken uitgedrukt. Dan wordt karakteristieke veelterm uitgerekend, en truuk van een determinant met blokken met een nulblok (ooit oefening geweest), gelijk aan product van determinanten van de diagonale blokken. Dan kunnen we $(X-\lambda)^k$ buitenbrengen, en onze T.B.  neerschrijven. 

(pauze) 

Vraag: kunnen eigenvectoren lineair afhankelijk zijn? Antwoord komt nu. Uiteraard: die bij eenzelfde eigenwaarde horen natuurlijk wel lineair afhankelijk.

Stelling 5.18.  Eigenvectoren die bij verschillende eigenwaarden horen: die zijn lineair onafhankelijk. Bewijs per inductie op aantal vectoren. Start met $k=1$. Trivaal, \'e\'en niet-nulvector is per definitie lineair onafhankelijk. Dan inductiehypothese: het geldt voor $k$. T.B. het geldt voor $k+1$. 
Bewijs: we nemen aan dat  $v_1,...,v_k$ zijn LO.  
Neem een LC (co\"ef. $\alpha$) van $k+1$ vectoren gelijk aan 0. Bewijs in twee gevallen. Geval 1: $\alpha_{k+1}=0$, geval 2: $\alpha_{k+1} \neq 0$. In geval 1: laatste term niet aanwezig, dus via inductiehypothese volgt dat alle andere $\alpha_i$ nul zijn. Geval 2: som herschrijven met $v_{k+1}$ links, en delen door $\alpha_{k+1}$. Dan lineaire afbeelding $L$ toepassen op die vergelijking. Maar uit dezelfde eerste vergelijking kan je ook links en rechts met constante $\lambda_{k_1}$ vermenigvuldigen, en we krijgen gelijkheid van twee sommen. Naar zelfde kan vergelijking brengen, sommen samenbrengen: nieuwe LC gevonden die nul oplevert. Uit inductiehypothese (LO van die vectoren), volgt dat al die nieuwe co\"effici\"enten nul zijn. Maar hierin zit verschil van $\lambda_i$ die verschillend zijn, dus die zijn niet nul, en moet andere deel (de $\beta$) nul zijn. Maar dit is een contradictie. Dus geval 2 kan niet voorkomen. QED. Best complex bewijske. Niet moeilijk maar complex (inductie, gevalsonderscheid en ongerijmde allemaal gebruikt). 

Definitie 5.19: enkelvoudig spectrum. (alle eigenwaarden hebben multipliciteit 1). Gevolg 5.20. een $L$ met enkelvoudig spectrum is diagonaliseerbaar. 

Lemma 5.22 en stelling 5.23. Eerder in de les gesproken van twee obstructies om niet te kunnen diagonaliseren. 
Lemma 5.22: we moeten eerstegraadsfactoren hebben (genoeg verschillende eigenwaarden). Stelling 5.23: maar zelfs als hebben we volledige ontbinding hebben van KV, dan nog moeten we gelijkheid hebben tussen alg. en meetk. multipliciteit. 

Bewijs stelling, twee richtingen van de asa. 

Richting "als diagonaliseerbaar, dan ...". Neem basis van eigenvectoren. Groepeer per eigenwaarde via twee indices. Totaal aantal vectoren gelijk aan dimensie. Opgelet: notatie-nachtmerrie. Bijhorende matrix is een diagonaalmatrix, met verschillende $\lambda_i$ die meerdere keren voorkomen. De karakteristieke veelterm kan dan uitgeschreven worden. En dan kan bijna afgelezen worden dat de $d$ gelijk zijn aan de $m$. 

Bewijs in andere richting: gegeven dat $\phi_L(X)$ volledig ontbindt als $\phi_L = (X-\lambda_1)^{m_1}...(X-\lambda_k)^{m_k}$ met alle $\lambda_i$ verschillend, en dat $d=m$ voor alle $\lambda$, en $m_1+...+m_k = dim V$. 
Te bewijzen: $L$ diagonaliseerbaar. 
Bewijs: kies voor elke $i$ een basis voor $E_{\lambda_i}$. Dit kan omdat $dim(E_{\lambda_i})=d(\lambda_i)=m_i$. Bewering is dan: alle $v_{i,j}$ met $1 \leq j \leq m_i$ en $1 \leq i \leq k$ vormen samen een basis van $V$. Aantal van die vectoren: $m_1+m_2+... + m_k$. Dus dat klopt. Dan nog gewoon bewijzen dat deze vectoren LO zijn. Zoals bijna altijd: we maken LC die nulvector oplevert. Dubbele sommatie over de twee indices. Dan bewijzen dat alle co\"effici\"enten nul zijn. Binnenste som hernoemen naar $w_i$, dan zit elke $w_i$ in $E_{\lambda_i}$. En we weten ook dat som van alle $w_i$ nul is. Dan moeten alle $w_i$ nul zijn, want som van eigenvectoren, en die zijn lineair onafhankelijk. Dan volgt uit andere vergelijking: dat die oorspronkelijke co\"effici\"enten nul. QED. 
Vond ik moeilijk te volgen, dit bewijs, ging nogal snel op einde van de les. 

Dan nog rap stelling 5.27 gezien in verband met vectorruimten over $\mathbb{C}$. Deze hoofdstelling van de algebra is moeilijk te bewijzen! (analyse nodig). Gevolg: de eerste obstructie voor diagonaliseerbaarheid vervalt bij complexe nulpunten. 

\section*{Les 10, nabeschouwing}

\subsection*{(1) oefening 5.7.4(A3)}
Niet gemaakt, zou moeten lukken. Karakteristieke vergelijking opstellen (det van matrix met diagonalen min $X$), gelijk aan nul en oplossingen voor $X$ zoeken. Dat zijn de eigenwaarden dan. 
Eigenruimte: de eigenvectoren vinden natuurlijk: stelsels oplossen door eigenwaarde-definitie uit te schrijven met gekende eigenwaarden, en de herschikken zodat onbekende vector aan 1 kant staat, zie voorbeelden. 

\subsection*{(2) oefening 5.7.8.}
We hebben een lin. trans. $L$. Eigenwaarden gegeven, eigenvectoren niet. Dus: 
\begin{itemize}
    \item $L(v_1)=-1v_1$
    \item $L(v_2)=0$
    \item $L(v_3)=1v_3$
\end{itemize}
Gevraagd: ker($L$) en Im($L$). 
Redenering: 
De vraag die ik me stel: is die lineaire transformatie eigenlijk helemaal gekarakteriseerd door die eigenwaarden? Sowieso geldt: alle scalaire veelvouden van vector $v_2$ beelden af op $0$, en zitten in de kern. En zo ook: alle niet-nul scalaire veelvouden van $v_1$ en $v_3$ beelden zeker niet af op $0$, dus zitten niet in de kern (eigenvectoren zijn altijd niet-nul vectoren). 

Maar wat ik me nu afvraag: zijn eigenvectoren lineair onafhankelijk? Want als dat zo is, dan heb ik 3 LO vectoren uit $\mathbb{R}^3$, en kan ik dat als basis zien. 

Ah: stelling 5.8 gaat daarover: er is een basis die uit eigenvectoren van matrix $A$ bestaat, asa de matrix $A$ diagonaliseerbaar is. (en matrix is een 1-op-1 met lineaire transformaties binnen bepaalde basis). En aha2: stelling 5.18 zegt dat bij eindig-dimensionale vectorruimte de eigenvectoren (uiteraard diegene die bij verschillende eigenwaarden horen) LO zijn. 

Dus: ik heb 3 LO vectoren $\{ v_1, v_2, v_3 \}$ in een 3-dim ruimte $\mathbb{R}^3$, dus ik kan die als basis zien. Of ook: ik kan elke vector schrijven als LC van die drie. 
Bijgevolg kan ik zien wanneer ik als beeld nul heb: nulvector zelf, en alle niet-nul veelvouden van $v_2$. Beeld Im($L$) is dan uiteraard $vct(v_1, v_3)$

\subsection*{(3) opmerking 5.11 p. 211}

In detail alle beweringen van opmerkingen 5.11 bewijzen. 
Hint: toon eerst puntje (2) aan.

Punt (2): zijn $\alpha$ een basis van $V$. De co\"ordinaatvectoren tov $\alpha$ van de eigenvectoren van $L$ bij een eigenwaarde $\lambda$ zijn de eigenvectoren van $L_{\alpha}^{\alpha}$ bij $\lambda$.

We gaan ervan uit dat puntje 2 ook over eindigdimensionale $V$ gaat (bij puntje 1 staat dit expliciet erbij), en $dim V=n$. En we hebben gegeven een lineaire transformatie $L$ met een bepaalde eigenwaarde $\lambda$ en bijhorende eigenvectoren. 
Even ter herinnering: wat is $L_{\alpha}^{\alpha}$ ook al weer? Dit is de \emph{matrix} die $L$ voorstelt bij basis $\alpha$, die met andere woorden de afbeelding kan uitrekenen op basis van co\"ordinaten van een vector. 
Ook even ter herinnering: je hebt eigenvectoren van een matrix (definitie 5.1), maar ook eigenvectoren van een lineaire transformatie (definitie 5.3). Ik heb de indruk dat het zaak is goed onderscheid houden bij het begrijpen van deze opmerking. 

Benoemen we de basisvectoren van basis $\alpha$ van $V$: $\alpha=\{ v_1, v_2, ... v_n  \}$.
En noemen we de $k$ eigenvectoren van $L$ bij $\lambda$: $w_1, w_2, ... w_k $. 
En noemen we de $l$ eigenvectoren van matrix  $L_{\alpha}^{\alpha}$ bij $\lambda$: $x_1, x_2, ... x_l$. 

Om te beginnen: 
Hoe weten we dat $\lambda$ een eigenwaarde is van zowel $L$ als $L_{\alpha}^{\alpha}$? Wel, dat is exact dat gevolg 5.10: de karakteristiek veelterm is onafhankelijk van de gekozen basis. Maar ik snap dat gevolg nog niet helemaal: $L(v)$ schrijven als matrixvermenigvuldiging, hoe ging dat weeral? 
Vorig hoofdstuk nog niet goed verwerkt denk ik. Ah ja, het grote vierkant van de cover, het gaat altijd via de coordinaten tov een basis (want anders kan je niet zomaar veeltermen in een matrixproduct gooien. 

En hoe weten we dat het er even veel zijn, dus dat $l=k$???

Dan geldt voor elke $w_i$: $L(w_i)=\lambda \cdot w_i$. 
Elke $w_i$ kan ook als LC geschreven worden van de basisvectoren: 
$w_i = \sum_{j=1}^{n} \delta_{ij} w_i$. M.a.w.: de co\"ordinatenvector van $w_i$ is: $(\delta_{i1}, \delta_{i2}, ... \delta_{ik})$, anders geschreven: co$_{\alpha}(w_i)=(\delta_{i1}, \delta_{i2}, ... \delta_{ik})$. 
De betekenis van $L_{\alpha}^{\alpha}$ is gewoon: co$L(v) 
 = L_{\alpha}^{\alpha}$co$(v)$. 
Dus ook voor $w_i$:   co$L(w_i)= L_{\alpha}^{\alpha} (\delta_{i1}, \delta_{i2}, ... \delta_{ik})$

... nog niet klaar... conclusie is: ik sta wat achter met vorige hoofdstukken goed begrijpen!! 

\subsection*{(4) als algebra\"ische multipliciteit 1, ook de meetkundige multipliciteit}

Toon aan dat indien de algebra ̈ısche multipliciteit van een eigenwaarde λ  ́e ́en is (m(λ) = 1),
ook de meetkundige mulipliciteit  ́e ́en is (d(λ) = 1).

\subsection*{(5) }
Zij A ∈ R 3×3
een diagonaliseerbare matrix. We onderscheiden de volgende gevallen:
 A heeft precies drie verschillende eigenwaarden,
 A heeft precies twee verschillende eigenwaarden,
 A heeft precies  ́e ́en eigenwaarde.
Bespreek in ieder van deze gevallen hoe de eigenruimten van A eruit zien en hoe deze
ruimten in R3 liggen. 
Beschrijf ook hoe je een basis bestaande uit eigenvectoren van A

kunt kiezen.

\section*{Les 11, 12 dec}

Voorlaatste les. 
Hoeken tussen vectoren, lengtes van vectoren, nog niet aan bod gekomen, terwijl we dat allemaal al gebruikt hebben. Scalair product gebruikt beide. Standaard inproduct op $\mathbb{R}^n$. Notatie (std?) inproduct: $<v,w>$. In dit vak beschouwen we deze informatie als deel van een nieuwe structuur bovenop wat we tot nu in vectorruimten gezien hebben. 
Paar eigenschappen overlopen die je op zicht kan zien. Een ervan: lineariteit, bvb. als je 1 vector vasthoudt en dan afbeelding beschouwt van andere vector op scalair product, dan is dit lineair t.o.v. scalaire vermenigvuldiging van de vectorruimte. 
Alle eigenschappen in definitie: 6.1, voor re\"ele vectorruimten. 

Dan: wat als we over $\mathbb{C}$ gaan? Opgelet, er komt een subtiel verschil: als voor alle $v$ geldt dat $<v,v> \geq 0$. Dan ook voor $<iv, iv> = i<v,iv> = -<v,v>$. Aha, dat is een probleem, want niet meer positief. 
Oplossing: toegevoegde lineariteit in de eerste variabele: de scalairen komen naar buiten met hun complex toegevoegd. En in de tweede variabele: gewone lineariteit. Bijgevolg moet symmetrie ook aangepast worden: volgorde van vectoren veranderen geeft ook toegevoegde van resulterende scalaire product. 
Zie definitie 6.3. 
Opgelet: in andere wiskunde boeken is het lineair in de eerste component. Maar in fysica-wereld is het zoals hier. 
Opgelet2: die $<v,v> \geq 0$: wat betekent dat in $\mathbb{C}$? Het komt erop neer dat het re\"eel moet zijn. 

Latex juiste manier: $\langle v \; , \; w \rangle$

Paar voorbeelden, ik heb niet alles genoteerd hier wat op bord stond: 

Voorbeeld 1: standaard Hermitisch product op $\mathbb{C}^n$. 

Voorbeeld 2: complexe continue functies met domein $[0,1]$. Een mogelijk Hermitisch product is een bepaalde integraal van complex toegevoegde van f maal g (integreren van 0 tot 1). 

Voorbeeld 3: matrices $nxk$: $\langle A \; , \; B \rangle = Tr(A \cdot B^T) $ (spoor). Remember, spoor is product van diagonaalelementen. 

Voorbeeld 4: een ander inproduct met $\alpha_i$ erbij ($\alpha_1$ voor waarde $v_i$, etc.). Blijkt alleen een inprodcut te zijn als die $\alpha_i$ strikt positief zijn. 

Dan: definitie van de norm/lengte. Def. 6.9. En definitie 6.13: afstand tussen twee vectoren (=norm van verschil). 
Fundamentele eigenschap van afstand: driehoeksongelijkheid moet gelden. En ook: ongelijkheid van Cauchy-Schwarz, om daarna de hoek (of toch zijn cosinus) tussen twee vectoren te kunnen defini\"eren. Krachtige eigenschap voor allerhande vectorruimten, ook die met functies. Stelling 6.14. 
Bewijs in de les gezien. Interessant bewijs, via truukje. Kwadratische functie/vergelijking in $\lambda$ erin die positief is. Kan alleen als er 1 of geen nulpunten zijn, discriminant $D \leq 0$. En hieruit volgt het T.B. Deel (2) van de stelling is oefening. 

Gevolg van CS: we kunnen def. 6.16 maken: hoek tussen twee vectoren (tussen $0$ en $\pi$). 

Stelling 6.19 driehoeksongelijkheid volgt hier ook uit. Volgt meteen uit C-S. 

Conclusie: in elke inproductruimte geldt CS en de driehoeksongelijkheid, etc.  

(pauze) 

Standaardbasissen in bvb. $\mathbb{R}^3$: hebben lengte 1 en staan onderling loodrecht. De vraag laat zich dan stellen of dit algemeen zo is: kunnen we altijd zo'n basis vinden? 

Maar eerst: vectoren loodrecht op mekaar zijn per definitie LO. Dit geldt ook algemeen en ook voor meerdere vectoren die twee-aan-twee loodrecht zijn. Stelling 6.23. Kort bewijs op bord. Zoals altijd: neem LC die nulvector oplevert en bewijs dat alle co\"effici\"enten nul zijn. (altijd zo doen, Raf, ook op examen!) 

Opm: nulvector staat per conventie loodrecht op alle vectoren, maar heeft niet veel betekenis. 

Waarom willen we zo'n orthonormale basis (ONB) van inproductruimte $V$ met lengten 1 en allemaal loodrecht? Motivatie van wat we nu gaan doen: stel we vinden zo'n basis, dan: als we kijken naar de inproducten van co\"ordinaten (met standaard inproduct in $\mathbb{R}^n$), dan blijkt dit hetzelfde inproduct te geven. Bewijs in les aan bord gezien: gewoon uitrekenen en nadenken over de sommaties die veel nultermen hebben behalve ene. Heel triviaal bewijs. 

Conclusie: we hadden al eerder gezien dat $\mathbb{R}^n$ een soort prototype voor vectorruimten is, via co\"ordinaatafbeelding. Nu blijkt dat standaard inproduct hier ook een prototype is voor eender wel inproduct. 

Reflectie: ofwel snap ik het niet goed, maar ik vind dit precies mind-blowing: los van hoe inproduct gedefinieerd is (we hebben er al een aantal verschillende gezien op dezelfde vectorruimte), blijkt dat het inproduct bij een orthornormale basis altijd hetzelfde is als het inproduct van de co\"ordinaten. Ik moet dat even laten bezinken. 

Nu nog niet bewezen dat zo'n orthonormale basis altijd bestaat. En dat gaan we nu zien: Gramm-Schmidt algoritme geeft hiervoor een methode. 

Stelling: elke eindig-dim inproductruimte heeft zo een ONB. 
Bewijs is een algoritme. Start met basis ... etc. Proced\'e van Gramm-Schmidt. Proces is iteratief, bewijs dus eigenlijk per inductie, dus eerst $u_1$, dan $u_2$, enz. op zo'n manier dat bij elke stap de voorgebrachte vectorruimte dezelfde is als die voorgebracht door de oorspronkelijke deelruimte van de overeenkomstige reeds verwerkte vectoren van de oorspronkelijke basis. 
Stap 1, $k=1$. Triviaal. Norm klopt, en voortgebrachte ruimte blijft dezelfde. 
Stap 2, $k=2$. Eerst maken we een vector loodrecht op $u_1,v_1$. Via formuleke dat nog uit de lucht valt. Nakijken: loodrecht? ja. Is niet de nulvector (stel wel nul, dan zou er een linaire afhankelijkheid geweest zijn, en dat was niet): Ja. Dan nog even normaliseren. En we hebben hem. Dan nog even kijken dat voortgebrachte ruimte van de $u_i$ nog klopt met de originele $v_i$. 
Stap $k+1$ (gesteld dat tot en met k al orthonormaal): ... 
Niet moeilijk, wel complexe notatie en veel schrijfwerk. Wat er eigenlijk gebeurt: we schrijven een oude vector eerst als loodrechte component plus component in reeds gemaakte deelruimte. Projecties/ontbinden dus. Telkens checken we dat de gemaakte vectoren niet nul zijn, en dan kunnen/mogen we normeren, en telkens checken dat voorgebrachte ruimte niet verandert.  

\section{Les 12, voorbereiding}

\subsection{deelruimte vectoren loodrecht op een rechte}
Rechte $U$ door $(0,0)$ en $(1,2)$ als deelruimte van $\mathbb{R}^2$. Dus $U=\{  (\lambda, 2\lambda) \in \mathbb{R}^2 | \lambda \in \mathbb{R} \}$. 
(a) bereken alle vectoren loodrecht op $U$. $W = \{  (x,y) \in \mathbb{R}^2 | y=\frac{-x}{2} \}$. 
Is $W$  deelruimte? Ja, want $(0,0)$ zit erin, en lineaire combinaties van willekeurige $w \in W$ zitten er ook altijd in (heb ik vlug nagegaan). 
Dimensies van $U$ en $W$ telkens 1. 
Directe som? Ja, als ik het teken in het vlak dan besluit ik van wel. 
Bewijs? 


Beschouw willekeurige $u, w$, dan is willekeurig element uit $U \bigoplus W$ een vector $  (\lambda, 2\lambda)  + (x,y=\frac{-x}{2}) = (\lambda + x , 2\lambda - \frac{x}{2})  $ met $\lambda, x$ vrij te kiezen. 
Nemen we $x=-\lambda$ dan krijgen we $(0, \frac{5}{2}\lambda)$ die erin zit (en dus ook alle veelvouden).  Nemen we $ 2\lambda = \frac{x}{2} $ of $x=4 \lambda$, dan: $( 5\lambda , 0)$, en alle veelvouden. 
TODO: intu\"itief voel ik aan dat directe som de volledige ruimte is, maar dit bewijs hierboven voelt niet juist aan om aan te tonen dat willekeurige $(x,y)$ kan geschreven worden als som van een $u$ en een $w$. Nog wat af te werken. 

\subsection{eigenvectoren matrix: orthogonaal of niet?}

$A = \begin{pmatrix} 6 & -4 \\ -4 & 6 \end{pmatrix}$. $B = \begin{pmatrix} 1 & 1 \\ 0 & 2 \end{pmatrix}$. 

Eigenwaarden van $A$: uitgerekend en gevonden: $\{ 2, 10 \}$. 
Eigenvectoren van $A$ bij $\lambda=2$: alle $(x,x)$. Eigenvectoren van $A$ bij $\lambda=10$: alle $(x,-x)$. 
Zijn bvb. $(1,1)$ en $(1,-1)$ orthogonaal? Ja. Kunnen we ook orthonormale vectoren vinden? Ja, uiteraard, gewoon die beide normeren, zijn nog steeds eigenvectoren. 

Voor $B$: eigenwaarden zijn $\{ 1,2 \}$. Eigenvectoren bij $1$: alles van de vorm $(x,0)$. 
Eigenvectoren bij $2$: alles van de vorm $(x,x)$. Nooit orthogonaal, laat staan orthonormaal. 
Want: $<(x,0), (x,x)> = x^2$ is alleen $0$ als $x=0$ dus voor inproduct van de nulvector met zichzelf. 

Allez, ik verwacht straks in de les dus iets over orthogonaliteit van eigenvectoren... 

\section*{Les 12, 19 dec}

Eindigdimensionale inproductruimte = Euclidische ruimte. 
We kunnen de bekende ruimte als beeld houden van wat we gaan zien, ook al gelden onze intu\"ities niet meer voor andere vectorruimten. Bvb. loodrechte projecties: gebaseerd op kortste afstand. Jpeg-standaard foto's: zeer hoog-dimensionale ruimte (bvb. elke pixel een vector). Algoritme is loodrechte projectie naar lager-dimensionale ruimte. En een snel algoritme. Bij bewegende beelden nog belangrijker om dit snel te doen. 

Definitie 6.30. Loodrecht complement. Bvb.: voor een vlak is het loodrecht complement een loodrechte rechte op dat vlak). 

Hoe vinden we een loodrechte projectie op abstracte manier? Ofwel op zoek naar kortste afstand. Beetje analyse-achtig. Andere methode via meetkundige constructie: langs loodlijn neerlaten = punt schrijven als ontbinding van stuk in vlak en stuk op loodlijn. (dus uit somruimte van oorspronkelijke ruimte en het loodrecht complement). Als dit bovendien op 1 en slechts 1 manier kan, hebben we een loodrechte projectie. 

Stelling 6.31 gaat hierover. 

Opm: loodrecht complement is altijd deelruimte (volgt uit lineariteit van scalaire product). 

Bewijs van stelling 6.31: alleen punt (2) is echt te bewijzen. (1) omdat nulvector de enige is die loodrecht op zichzelf staat. En (3) volgt gewoon uit (2), en (4) uit (3). 
Bewijs: schrijf eender welke vector uit $V$ als som van vector uit $U$ en vector uit $U^{\perp}$. (geg. $U \subset V$). Principe: willekeurige $v$, T.B. $\exists u \in U, w \in U^{\perp}: v=u+w$.
Bewijs: via ONB (ortho-normale basis) voor $U$. Stel $w=v-u$, dan nog gewoon bewijzen dat $w \in U^{\perp}$. Voldoende te bewijzen dat $w$ loodrecht op elke basisvector (want dan loodrecht op elke LC hiervan, dus op volledige deelruimte). 
Gewoon uitrekenen. 

Uitwijding: 
\begin{itemize}
    \item notatie projectie $\text{pr}_U(v)$, loodrechte projectie van vector $v$ op deelruimte  $U$. 
    \item we hebben expliciete formule: $\text{pr}_U(v) = \sum <v, u_i> u_i$. 
    \item het zal blijken dat deze vector het dichtst ligt bij $v$ en toch in $U$. (zie kleinste kwadraten-benadering). 
    \item wavelets: compressie door projectie op laagdimensionale ruimte; wavelets zijn voor spraakcompressie geschikt. (gsm compressie) 
    \item $(U^{\perp})^\perp = U$. Via twee inclusies! Ene is evident: $U \subset  (U^{\perp})^\perp $. Andere inclusie: bewijs via dimensie-verbanden. Deelruimte met dezelfde dimensie. 
    \item uniciteit van loodrecht complement: volgt uit directe som, dus op hoogstens 1 manier te schrijven als som, en we hebben 1 manier gevonden, dus is het uniek. Anders gezegd: (1) uit stelling zegt "hoogstens 1 manier", ... (nog eens over nadenken). 
    \item praktisch: orthonormale basis vinden, en dan projecteren. Dit is ook de beste/snelste manier
\end{itemize}

Dan nog eigenschap: "de loodrechte projectie pr$_U(v)$ is het punt in $U$ zo dicht mogelijk bij $v$. Herinnering afstand $d(u,v) = || u-v || = \sqrt{ <...,...> }$. 
Pythagoras algemeen: als $u \perp v$ dan is $|| u+v ||^2 = ||u||^2+||v||^2$.

Dan bewijs zelf. 
T.B. Stel $u = $pr$_U(v)$. Neem $u' \in U, u' \neq u$, dan is d$(v,u') >$ d$(v,u)$. 

Bewijs: niet zo moeilijk. Definitie op beide afstanden, Pythagoras op de $u'$. Daarin zit $v-u$ en $u-u'$, dat is iets in $U\perp$ en iets in $U$, dus Pythagoras toepassen. Dan zien we extra term en ene die altijd $>0$. Dus grotere afstand. QED. 

Stelling 6.42: spectraalstelling voor symmetrische matrices.

Intro: diagonaliseren van matrices gezien, eigenwaarden/eigenvectoren gezien. Twee obstructies gezien voor diagonaliseerbaarheid: eerste: karakteristiek veelterm niet voldoende re\"ele nulpunten, en tweede: 
voor specifieke eigenwaarden zou meetkundige multipliciteit strikt kleiner kunnen zijn dan de ... 
Het straffe is: voor symmetrische matrices gelden geen van beide obstructies. Altijd diagonaliseerbaar, en altijd orthonormale basis van eigenvectoren te vinden. 

Even herhaling gelijkvormigheid. 
Definitie. $\exists P \text{ inverteerbaar met } B=PAP^{-1}$. 
Denk aan $A$ en $B$ als matrices van zelfde transformatie maar met andere basissen. Dan is $P$ de verandering van basis. 

Even denken: wat maakt symmetrische matrix bijzonder? Kijken we naar standaard inproduct op $\mathbb{R}^n$, en herschrijven we als kolomvectoren en getransponeerden hiervan dus rijvectoren. In $\mathbb{C}^n$ moeten we met complex toegevoegden werken. 
Als $A=A^T$ dan is $<AX,Y> = <X,AY>$ (toverformule) voor alle $X,Y$ in $\mathbb{R}^n$ (en zelfs in $\mathbb{C}^n$).

Neem $X,Y \in \mathbb{C}^n$. $<AX,Y> = (AX)^T$. 
... niet alles opgeschreven TODO
A is reele symmetrische matrix dus complex toegevoelgde doet niks. 
... komt <X,AX> uit. 
(op einde in de les: ook complexe matrices erbij halen). 

Definitie: een complex matrix noemen we Hermitisch als $A=A-toegvoegd-getransponeerd$. Dan blijft bovenstaande formule gelden. Maw: als je transponeert, krijg je complex toegevoegde. 

Die ene toverformule, daar gaan we alles uit afleiden nu. 

(pauze) 

Eigenschap 1: als matrix $A$ (nxn) en $A=A^T$ dan is elke nulpunt van $\phi_A$ re\"eel. 
Bewijs in de les gezien. Via omweg complexe eigenwaarden en complex eigenvectoren, en dan T.B. $\lambda = \overline{\lambda}$.  
Neem $X \in \mathbb{C}^n$, $X \neq 0$ en $AX=\lambda X$. Dan $<AX,X>$ op twee manieren uitrekenen: via $AX=\lambda X$, en dan lineariteit in eerste term voor Hermitisch prodcut;  en anderzijds via toverformule voor symmetrische matrices $<AX,X> = <X,AX>$. Etc. dan verschil nemen, moet nul zijn want gelijk, en $X \neq 0$, dus $||X|| \neq 0$, dus moet ... QED. 

Ons eerste obstakel is weg: stelling van Gauss, karakteristiek veelterm te schrijven als eerstegraadstermen met complexe getallen. Maar hier voor symmetrische matrices zijn dat dus re\"ele getallen. 

Waar we naartoe willen: meetkundige en algbra\"ische multipliciteiten gelijk voor alle eigenwaarden. 

Maar eerst eigenschap 2: 
Als $\lambda \in \mathbb{R}$ eigenwaarde, van $A \in \mathbb{R}^{nxn}, A=A^T$, dan zal voor alle $X \in E_\lambda^{\perp}: AX \in  E_\lambda^{\perp}$. 
Herinner: $E_\lambda = \{ ...  \} $

Bewijs: neem vector $Y \in E_\lambda$. T.B. $AX \perp Y$. 
Toverformule: $<AX,Y> = <X,AY> = <X,\lambda Y> = \lambda <X,Y> = 0$. QED. 


Eigenschap 3: Als $A \in \mathbb{R}^{nxn}$, $A=A^T$, en $\lambda \in \mathbb{R}$ is een eigenwaarde, dan is d($\lambda$) = m($\lambda$. 

Bewijs. Via basis van $E_\lambda$. We kunnen dan "aanvullen" met basis van $E_\lambda^{\perp}$. Omdat $\mathbb{R}^n$ gelijk is aan directe som, is unie van die basissen een basis van $\mathbb{R}^n$. 
Noemen we de transformatie $L_A$, en de matrix ervan $A$: $L_A(X)=AX$. Matrix $L_A$ bouwen via co\"ordinaten van basisvectoren. Via eigenwaarde definitie herschrijven als co\"ordinaten van $\lambda u_1$. Maar dit is gewoon een kolom met $\lambda$ en de rest $0$. Dit doen we voor $k$ kolommen. 
Dan wordt het subtieler. Kolom $k+1$: co$_\beta(L_A(u_{k+1}$. Via vorige eigenschap: na afbeelding zit die vector nog steeds in $E_\lambda^\perp$. Maar wat is de co\"ordinaat hiervan? Sowieso nullen voor de vectoren die loodrecht staan, dus eerste rijen allemaal nul. Rest kennen we niet (lineaire combinatie van basisvectoren van deelruimte loodrecht).  
Bekomen matrix als grote blokmatrix bekijken, met nul-matrices, $\lambda$ keer eenheidsmatrix, en stuk $B$ onbekend. 
$\phi_A(X) = det(X \mathbb{I}_n - \text{onze blokmatrix} $. Dit soort determinant kennen we: product van determinanten van niet-nul blokken. We hebben dan iets staan waarvan $\lambda$ al $k$ keer een nulpunt is. Dus $m(\lambda)=k$. 
Dan is nog voldoende te bewijzen: $\phi_B(\lambda) \neq 0$. 
Uit ongerijmde: stel $\lambda$ wel nulpunt van $\phi_B$. Dan is $\lambda$ een EW van $B$. Dus er is een niet-nul eigenvector $Z$ zodat $BZ=\lambda Z$. Maar hiermee kunnen we een eigenvector maken voor oorspronkelijke matrix $A$: namelijk ..., en ik was den draad kwijt. Komt erop neer: nieuwe eigenvector gevonden niet in $E_\lambda$, maar dat is contradictie, dus hypothese fout. 
Kijken naar die blokmatrix: rechts staat 0 en eronder B. Dan is eigenvector van B ook eigenvector van die volledige matrix. Dit zie ik zelf nog niet goed... TODO dit bewijs beter snappen. 

We weten nu al dat $A$ diagonaliseerbaar is. 
Nog te bewijzen: dat keuze van eigenvectoren ook orthornormaal kan. 
Dit is nog een klein extra bewijs: 
Als $A$ symmetrisch is, dan zijn eigenvectoren bij verschillende eigenwaarden diagonaal. Dit is de vierde eigenschap die we nog gaan bewijzen. 

Eigenschap 4: stel A re\"eel en symmetrisch, dan zijn eigenvectoren uit verschillende eigenwaarden loodrecht. 
Herinner: algemeen zijn ze lineair onafhankelijk. Dit is dus een extraatje voor symmetrische matrices. 
Bewijs makkelijk: $AX=\lambda X$ en $AY=\mu Y$, en $\lambda \neq \mu$. 
Schrijven we $\lambda<X,Y> = <\lambda X, Y> = <AX, Y> = ... = \mu <X,Y>$. Dus is $(\lambda - \mu) . <X,Y> = 0$, dus is ... QED. 
(opnieuw zelfde toverformule gebruikt). 

Nu hebben we spectraalstelling bewezen door  wat we afgelopen weken 
gezien hebben te combineren. 
De $P$ die de gelijkvormigheid "doet" is een transformatie van de ene orthonormale basis naar de andere orthonormale basis. Dit noemen we een orthogonale matrix. 
Stelling 6.46. Interpretatie (2): elementen van product van matrices zien als inproduct van rijen/kolommen. 

Definitie 6.47. Deze $A$ is eigenlijk de matrix $P$ uit de gelijkvormigheids-definitie. Die $P$ is een orthogonale matrix. 

Wat met de complexe getallen: alles blijft hetzelfde, behalve dat het gaat over Hermitische matrices. (getransponeeerde gelijk aan toegevoegde). 


Test latex: $\bar{abc}$. $\overline{abc}$. $\Bar{abc}$. 



\section*{KEEP ME TO JUMP TO END}

\end{document}
