\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}

\title{Nota's colleges lineaire algebra}
\author{Raf Meeusen}
\date{2023-2024}

% mark paragraphs with empty line instead of indented first line
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section{Les 1, 26 sept}

Toepassingen matrices: 
\begin{itemize}
\item google search (eigenwaarden/eigenvectoren worden berekend) 
\item jpeg compressie (projecties uit lin. alg.) 
\end{itemize}


Lineair: zelfs voor niet-lineaire problemen wordt vaak een lineaire benadering (ook: eerste orde benadering) gebruikt voor een oplossing; bvb: slinger waarbij sin theta = theta wordt gesteld

Examen: 
\begin{itemize}
\item duurt 3 uur
\item gesloten boek
\item schriftelijk
\item 2 oefeningen + 1 theorievraag
\item er zijn voorbeeldexamens 
\end{itemize}

Boek: editie 2 ook bruikbaar ; maar er is een editie 3 


Makkelijke verbanden tussen lin. alg. en andere topics: 
\begin{itemize}
    \item fysisch netwerk met knooppunten (elek, stroom auto's, vloeistof): knooppuntvergelijkingen geven stelsel; stelsel = lin. algebra
    \item vlakke meetkunde: 2 vgln. stellen rechte voor; stelsel = alle punten die op beide rechten liggen
    \item Ruimte-meetkunde: vergelijkbaar, vlakken / rechten
\end{itemize}

Vrijheidsgraden als er oneindig veel oplossingen zijn. Oplossingen te schrijven in functie van "vrije parameters". 

"operaties die de oplossingen van een stelsel bewaren" 
(zie ook oefening nabeschouwing met S en S') 
Deze operaties zijn per definitie omkeerbaar.  

Termen: 
\begin{itemize}
\item uitgebreide matrix van een stelsel (met de $b_i$) 
\item achterwaartse subsitutie (bij oplossen stelsel) 
\end{itemize}

Stelsels waarbij oplossing bijna rechtstreeks af te lezen is:  voorbeelden gezien in de les. Allemaal met een matrix die in specifieke vorm is. Dan link gelegd met echelonvorm / trapvorm. 

Termen:
\begin{itemize}
\item echelonvorm
\item trapvorm
\item gebonden variabelen (kolommen met leidende 1) 
\item vrije variabelen (zonder leidende 1); als parameter te kiezen
\item strijdig stelsel
\end{itemize}

Gewoonte: parameter $\lambda$, $\mu$, ...

Elk stelsel kan naar echelon geconverteerd worden. MAAR: een echelonvorm is NIET uniek. 

Methode: algoritme, technieken. 

Dan nog gezien: stelling 1.11. (ivm. 1 oplossing, 0 oplossingen, oneindig veel oplossingen)

Notatie: $\mathbb{R}^{nxk}$ , voor n rijen / k kolommen. 

Voorbeelden bewijzen eigenschappen matrices: onthouden dat een matrix defini{\"e}ren erop neerkomt om elk element te bepalen (notatie Aij = element op rij i en kolom j). 

Definitie: transponeren v/e matrix ($A^T$) . Is spiegelen over diagonaal. 

Termen:
\begin{itemize}
\item vierkante matrix
\item symmetrische matrix ($A = A^T$) ; moet dus vierkant zijn
\item diagonaalmatrix
\end{itemize}

\section{Les 1 nabeschouwing}

Oefening 1, S1 en S2 gemaakt. S3 begonnen, maar niet helemaal afgewerkt.

Oefening 2: gelezen, niet gemaakt. 

Oefening 3: 0, 1 of oneindig. Leg uit adhv prop 1.8 / st.1.11. Even opgezocht: prop 1.8: elke matrix is rij-equivalent met een matrix in echelonvorm, en zelfs met een matrix die rijgereduceerd is. (p. 21). Stelling 1.11: p. 24
stelling zegt dat een stelsel waarvan matrix in echelonvorm, altijd 0, 1 of oneindig oplossingen heeft, en geeft ook de voorwaarden adhv echelonvorm eigenschappen. 

Oefening 4: maak opdracht 1.2 in boek. "toon aan dat  elke vd elementaire rijoperaties ongedaan gemaakt kan worden door opnieuw...". Geef telkens correct notatie voor de inverse elem.rij-op. 

elementaire rij-operaties zijn: 
\begin{itemize}
\item $Ri \rightarrow  lambda * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri + lambda*Rj$
\end{itemize}

inverse: 
\begin{itemize}
\item $Ri \rightarrow  (1/lambda) * Ri $
\item $Ri \rightarrow Rj$
\item $Ri \rightarrow Ri - lambda*Rj$
\end{itemize}

Oefening 5: gemaakt (zonder helemaal uit te schrijven) 
...
Stel daarom eerst dat ... oplossing is van S. Omdat 
$
(ai1 + lambda*aji) * x1 + ... 
= 
[ ai1 * x1 + ... + ain xn  ] + lambda* [ aj1 * x1 + ... ajn * xn] $
en dat we uit S weten dat 
eerste deel gelijk is aan $b_i$ 
en tweede deel gelijk aan $lambda*bj$
is ook aan deze vergelijking (nieuwe $Ri$) voldaan. 

Omgekeerd: stel dat $(x1,...xn)$ oplossing is van S' 
Omdat $lambda*(aj1x1 + ...) = b_j $ (zie j-de rij van S', en we hebben een oplossing),
kunnen we vergelijking van nieuwe Ri herschrijven door links en rechts gelijke delen te schrappen,
en blijft nog gewoon oude Ri uit S over. 

Oefening 6: 
eens gedaan op papier; niet heel moeilijk, maar wel opletten welke notatie juist welke betekenis heeft (volgorde van $_{ij}$ en $T$ en $lamba*$ etc.). 

Oefening 7 (extra): 
Vraag gesteld aan An Speelman: kolom van de constanten telt niet mee! 

kolomoperaties: 
\begin{itemize}
    \item kolommen verwisselen (die NIET de constanten zijn): gewoon variabelen van naam veranderen. 
    \item kolommen maal een lambda: 
kolom van niet constanten: uitkomst van corresponderende var is gedeeld door lambda
\item kolom plus lambda keer andere kolom: ik zie geen makkelijke redenering van wat er met oplossingen gebeurt. 

\end{itemize}



\section{Les 2 voorbereiding}

Gelezen. 
2a) toon aan dat I.A = A = A.I . Voor 2x2. 
\[I = 
1  0
0  1 \]
\[
A = 
a11  a12 \\
a21 a22\]

(onthouden: rij vd linkse matrix x kolom vd rechtse matrix) 

\[(I x A)11 = 1.a11 + 0.a21 =a11\]
\[(I x A)12 = 1.a12 + 0.a22 = a12\]
\[(I x A)21 = 0.a11 + 1.a21  =a21\]
\[(I x A)22 = 0.a12 + 1.a22 =a22\]


idem voor andere. 

2b) 
Zoek voorbeeld van 2x2 matrices waarbij A . B niet gelijk is aan B. 
\[A 
1  0
1  1 
en
0  1
0  0 \]

heb ik eens uitgeteld; verschillend. 


Appendix over sommatieteken. 
TODO: Nieuwe notatie hier: $(ai)_{ni}=1$  
(bij puntje 1. ) 

bewijs de eigenschappen
\begin{itemize}
\item lambda/mu : OK, heb ik eens uitgeschreven
\item dubbele som van ai.bj : OK, heb ik ook eens uitgeschreven. 
\end{itemize}


herschrijf uitdr. met sommatieteken; OK kan ik 
omgekeerd, wat drukt .. uit  ; OK, kan ik 

toon aan dat ... : OK, gedaan; is wat schrijfwerk en lastig, maar geen rocket science



\section{Les 2, 3 okt}


Matrices vermenigvuldigen. Te onthouden: rij links, kolom rechts. Een rij x een kolom = 1 getal. (makkelijk te onthouden, want ik weet ook nog dat nxm matrix maal mxp matrix een nxp matrix geeft, en nxm matrix heeft n rijen met lengte m, en de lengte van de kolommen van de tweede is ook m. Logisch. Aantal rijen = lengte van de kolommen, en vice versa. 

% even vooruitlopen waarom dat matrixproduct zo gedefinieerd is: 
% 
%Functie $L_A(x)$ is een afbeelding van $\mathbf{R}$ $$ R^k naar R^n, met 
%L_A(x) = som (Aij . xj) 
%(elke matrix A definieert een afbeelding La van ...) 
%En dan is La . Lb = L_A.B 
%(en dit maakt de definitie van matrixproduct logisch) 


\begin{itemize}
    \item veel eigenschappen en rekenregels
    \item een nulmatrix $O_{nxk}$
    \item een eenheidsmatrix $I_n$ (vierkante matrix) 
    \item MAAR: vermenigvuldiging niet-commutatief (logisch want niet eens gedefinieerd voor alle nxk en kxl dimensies. 
    \item $AB$ kan nul zijn terwijl $A \neq O$ en $B \neq O$
\end{itemize}


Noot: als we een matrix zien als een "operatie", en een vermenigvuldiging van matrices zien als de opeenvolging van die twee operaties, dan vinden we het ook logisch dat de uitkomst van $AB$ niet zelfde is als $BA$. (schoenen aantrekken, dan kousen, of omgekeerd. 

Matrices waar volgorde van operaties wel omwisselbaar is: noemt men commuterende matrices. 

Vraag: kunnen we matrices delen? Aangezien product van matrices nul kan zijn zonder dat een van de matrices nul is, is het delen niet zo evident als bij re{\"e}le getallen. Bvb. uit $ab=0$ volgt, als $b=0$, dat $a=0$, omdat we mogen delen door $b$. 

Inverteren: een matrix vinden zodanig dat vermenigvuldiging de eenheidsmatrix geeft. Ook hier zijn er verschillende opties: de linkerinvers en de rechterinvers. 

Opmerking: we beperken ons hier tot vierkante matrices wat betreft inverse, hoewel je ook inverse kan berekenen voor niet-vierkante matrices. 

Stelling: voor een vierkante matrix die links inverse B heeft en rechts inverse C, geldt dat $B=C$. (bewijs in les gezien, heel kort). 

Definitie 1.33. We noemen een nxn matrix ofwel inverteerbaar, ofwel niet-inverteerbaar, indien ... Synoniemen: regulier, niet-singulier/singulier. Opgelet: op dit moment hebben we nog niet bewezen dat linker inverse en rechter inverse gelijk zijn! Dat komt later! 

Stelling 1.34: inverse van product (als beide inverteerbaar zijn) =  product van de inversen in omgekeerde volgorde. Het bewijs is gebaseerd op: aantonen dat een gegeven matrix een inverse van een andere, is bewijzen dat zowel links-vermenigvuldigen als rechts-vermenigvuldigen resulteert in de eenheidsmatrix. 

Noot: niet evident hier om de te volgen wat al bewezen is, en wat we al weten, maar nog niet bewezen is (en dus: wat volgt eigenlijk waaruit). Bvb. nog steeds niet bewezen dat linker en rechter inverse gelijk gaan zijn. 

Rap een eigenschap tussendoor: als $AB=0$ en $A$ is inverteerbaar, dan is $B=0$. (bewijs redelijk makkelijk) 

Term/concept: elementaire matrix $E_n$. Komt voort uit oplossen van stelsels met matrix rij-operaties, gecombineerd met matrix-vermenigvuldigen. We kunnen namelijk rij-operaties voorstellen als matrix-vermenigvuldiging met zogenaamde elementaire matrices. 

Interessant: twee rijen van plaats wisselen: ongedaan maken is opnieuw hetzelfde doen! Dus inverse van $E_n$ voor rijen verwisselen, is gewoon dezelfde $E_n$! 

Noteer ook: alle elementaire matrices zijn inverteerbaar. 

ERO: elementaire rij-operaties. Bij elke ERO hoort een EM (elementaire matrix), noemen we $\Sigma$, en die EM bekom je door rij-operaties toepassen op de eenheidsmatrix. ERO toepassen op matrix A komt overeen met matrix-vermenigvuldiging $\Sigma A$. Links-vermenigvuldigen dus. 

Noot: we weten dat twee rijen wisselen, ongedaan gemaakt wordt door opnieuw hetzelfde te doen. Dus: de matrix $\Sigma$ van rij-wissel operatie, moet wel de inverse van zichzelf zijn. 

Grote stelling: 1.39, met 6 equivalente beweringen. De matrix A heeft een links inverse B; het stelsel $AX=0$ heeft enkel de evidente oplossing $X=0$ (dus stelsel met 1 oplossing!); etc. etc. Puntje 4: matrix A is product van elementaire matrices, is een soort sleutel-formulering, en ook een sleutel in het grote bewijs. 

Bewijs: formulering 2 volgt uit 1 door links met B te vermenigvuldigen en uit te werken. Formulering 3 volgt uit 2, doordat stelsels met 1 oplossing in rij-echelon-vorm de eenheidmatrix hebben, en altijd daartoe te herleiden zijn met ERO's. Bewijs van 4 uit 3 (A is product van EM's): op bord geschreven tijdens de les. Ook redelijk makkelijk (moet wel weten dat EM's inverteerbaar zijn, en opnieuw EM geven). 

Tussenstelling: als $C$ inverteerbaar, en $CA=I_n$, dan is A inverteerbaar en $A^{-1}=C$. Het bewijs: als $CA=I_n$, dan is A rechterinvers van C, en dus $A=C^{-1}$

Noot: tijdens de les effe de draad kwijt tussen wat al bewezen was, en wat niet, en de definities inverteerbaar etc. Nog eens nakijken! 

Noot2: belangrijk is: als $AB=I_n$, dan kan je dat op 2 manieren lezen: A is linkerinvers van B, of B is rechterinvers van A. Die twee manieren van lezen heb je nodig om bewijs sluitend te krijgen. 

Bovendien volgt uit deze stelling een algoritme om de inverse te berekenen: doe dezelfde rijoperaties om A te rij-herleiden naar $I_n$, ook gewoon telkens op $I_n$, bijvoorbeeld door de matrix $(A|I_n)$ te gebruiken voor alle rij-operaties. Dan krijgen we rechts $A^{-1}$, want in bewijs hadden we gezien dat $A^{-1} = \Sigma_k \Sigma_{k-1} ... \Sigma_1$. Als bij A een nul-rij ontstaat, is A niet inverteerbaar. 

Dan nog even over LU-decompositie gehad. (lower / upper triangular = benedendriehoeksmatrix en bovendriehoeksmatrix). Benedendriehoeksmatrix heeft niet-nullen op en beneden diagonaal. Het kunnen schrijven van een matrix A als product van een L en een U, heeft interessante toepassingen. Ook interessant: product van twee bovendriehoekmatrices is opnieuw bovendriehoeks. Een diagonaalmatrix is zowel bovendriehoeks als benedendriehoeks. Kijk ook eens naar de EM's en of ze L of U zijn. Echelonvorm is bovendriehoeks. 

Ook "de spil" vernoemd hier ivm rij-herleiden. Dus ook leerstof (spil/gauss-eliminatie...). En volgende redenering: als je A in echelonvorm kan zetten zonder rijen te verwisselen, en we weten dat echelonvorm een U-vorm is. Dus $U= (\Sigma_k ... \Sigma_1) A$, met $\Sigma_k ... \Sigma_1$ inverteerbaar, en inverse hiervan is een L-vorm (indien geen rij-wissels). 

\section{Les 2 nabeschouwing}

1. stelsel herleiden naar trapvorm, dan elementaire matrices hiervoor opschrijven, en dan A herschrijven als product van elementaire (die dan inverse zijn van oorspronkelijke elementaire!) en de trapvorm zelf. 
OK, alleen twijfelde ik wat bij inverteren van de elementaire. Niet slecht om eens voorbeeld van op te schrijven (1/lambda, en +lamba Ri wordt -lambda Ri). 

2. stelling 1.38 bewijzen; had ik al gedaan voor paar getallenvoorbeelden, moet in deze oefening eigenlijk gewoon met variabele lambda en zo, dus heb ik geskipt. Wel goeie oefening om die inversen van elementairen eens uit te schrijven. 

3. Die bewijzen zijn veel schrijfwerk, en met al die verschillende indices en mxn, nxp etc. dimensies, ... Eerst eens proberen definitie 1.22 zelf op te schrijven zonder te spieken. 

4. Doenbaar, wat rekenwerk. Gevolg 1.40 is pagina 40. Maar nog niet gedaan. 

5. Oefening 1.35 uit boek (is op p. 53, idempotente matrix). Nog niet gedaan. 

\section{Les 3 voorbereiding}

A4 lesvoorbereiding, paar stukjes in boek lezen en nadenken. Duurde ongeveer halfuur. 

\section{Les 3, 10 okt}

Determinanten. Vorige les: verband inverteerbaarheid matrix en oplosbaarheid (uniek) van homogeen stelsel. Voor 2x2 matrix kan je matrix beginnen inverteren via rijoperaties, en bekom je dat matrix inverteerbaar is als en slechts als $ad-bc$ niet nul is, en we kunnen dit defini\"eren als de determinant-functie $f(A)$ van een 2x2 matrix. 

Paar eigenschappen van de 2x2 determinant: rijen van plaats wisselen, determinant van eenheidsmatrix is 1, twee matrices met verschillende eerste rij combineren. Deze drie eigenschappen worden als basis genomen van een algemene determinant-afbeelding. Zie boek. 

Eigenschappen van een determinantfunctie: er volgt direct dat die lineariteit ook geldt voor alle andere rijen (want je mag rijen wisselen, alleen maar minteken erbij). Nul-rij geeft nul-determinant. Twee gelijke rijen ook nul (teken verandert maar moet hetzelfde blijven, dus moet 0 zijn). 

Impact van elementaire rij-operaties (ERO) op een determinant-afbeelding: type III in stelling 2.3. Bewijs in de les gezien. Ook voor andere ERO's de impact gezien. 

Stelling 2.4 gezien, vier onderdelen gecombineerd in een stelling: driehoeksmatrices, verband met inverteerbaarheid, determinant van product van matrices, determinant van getransponeerde matrix (dus lineariteit geldt ook voor de kolommen!). In de les het bewijs gezien van deel 2 van de stelling (verband inverteerbaarheid). Ene richting bewijs via ontbinding elementaire matrices. Andere richting van bewijs via contrapositie, en boven- en benedendriehoeksmatrices. 

Gevolg 2.5: o.a. determinant van de inverse, ... 

Nu gaan we bewijzen dat determinant-afbeelding bestaat. Gaat via permutaties, dus eerst heleboel wiskunde rond permutaties, transposities, ... 

Notatie van permutatie $\sigma$ via matrix met bovenste rij (1 2 3 ...), en onderste rij de beelden onder $\sigma$. Demonstratie dat om van een gegeven  permutatie naar de identieke permutatie te gaan via "wissels": altijd oneven aantal wissels nodig. Een wissel noemt men een transpositie. Stelling: elke permutatie kan geschreven worden als een samenstelling van transposities. Het aantal transposities in zo'n gegeven ontbinding is altijd even of altijd oneven. 

Definitie van inversies van een permutatie: koppels van indices $(i,j)$ waarvoor het afgebeelde koppel de orde omkeert. Voorbeeld gezien (alle inversies van een gegeven permutatie).  Definitie van het teken van de permutatie $\sigma$, gedefinieerd aan de hand van aantal inversies. Verband tussen aantal transposities in een ontbonden permutatie en het teken: $sgn(\sigma)=(-1)^m$. 

Afleiding van de/een determintantafbeelding voor 3x3 matrices. Gedaan via lineaire combinaties van eenheidsvectoren $e_1$, $e_2$ en $e_3$. Dan definitie determinant gebruikt, eerst op rij 1, dan rij 2 etc. Je komt dan uiteindelijk op afbeelding op een matrix met alleen $e_i$ als rijen, die die hebben als waarde altijd 1 (eenheidsmatrix), -1 (afgeleid van eenheidsmatrix met oneven aantal rijwissels) of 0 (twee dezelfde rijen). Alleen kijken naar de niet-nul termen in de grote sommatie, dit zijn alleen de waarden $(j,k,l)$ die permutaties zijn van $(1,2,3)$ (dus verschillende j, k en l), dus equivalent te schrijven dat we sommeren over alle permutaties in plaats van de sommeren over alle combinaties van $j,k,l$ waarbij ze van 1 tot 3 gaan. 

De afleiding toont aan dat de determinantafbeelding uniek is. Maar is de formule ook OK voor de andere twee delen van de definitie?  En komt dit overeen met hoe we vroeger determinanten geleerd hebben voor een 3x3? 


\section{Les 3 nabeschouwing}

1. Een determinant op 3 (twee??) manieren berekenen. OK, gedaan via trapvorm (en stelling 2.4 voor driehoeksmatrix), en dan gedaan via formule 2.2 pagina 65 met $\sigma$ en $sgn(\sigma)$ etc. Kwam beide 1 uit. 

2. Bewijs van stelling 2.3 p. 58 verder bewijzen, dus puntje 2 en 3 i.v.m. $f(E)$. Puntje 2 letterlijk uit definitie te halen door D-1 en D-2 samen te nemen. Puntje 3 is gewoon D-3 op eenheidsmatrix, met $\mu=0$, gecombineerd met puntje 1 dat het op elke rij geldt.  

3. gevolg 2.5 punt 2 bewijzen (det van inverse). Niet zo moeilijk: via product van $AA^{-1}$, we weten $f( AA^{-1}) = 1$ en dat determinant van product van matrices gelijk is aan product van determinanten. 

4. redelijk eenvoudige uitbreiding van wat er al staat voor een 2x2, gewoon wat gelezen. 

5. formule 2.2 pagina 65: aantonen dat $f(\mathbb{I})=1$ met deze formule. OK, even op moeten denken. De term onder sommatie $sgn(\sigma)a_{1\sigma(1)}...a_{n\sigma(n)}$ is alleen verschillend van nul, als \emph{alle} $a_{ij}$ tegelijk verschillend zijn van nul, en dat is alleen het geval als bij \emph{alle} $a_{ij}$ geldt dat $i=j$ (eenheidsmatrix, alleen diagonaal heeft 1). Dit krijg je alleen als $\sigma(i)=i$, dus bij de identieke afbeelding. Voor deze afbeelding is $sgn()$ gelijk aan 1. Dus 1 term blijft over van de som, en alle factoren erin zijn 1. 

6. definitie 2.8 pagina 61. Nog eens goed moeten lezen, want er wrong iets. Ik had het wel juist gedaan, maar eigenlijk niet goed begrepen. Via stelling 2.10 p. 62 is het pas helemaal compleet: elke transpositie toepassen op een permutatie, verandert het teken. De identieke permutatie heeft 0 inversies, dus is even. En vanaf dan gaat elke "wissel" het teken veranderen. Maar het aantal inversies is niet gelijk aan het aantal wissels dat je gedaan hebt, je kan namelijk blijven wisselen. En dan snap ik wel beter dat je makkelijk 12 even en 12 oneven permutaties haalt uit $\{1,2,3,4\}$

7. Had ik eerst heel veel moeite mee. Ik dacht dat die permutaties of transposities gingen over het van plaats veranderen van posities van elementen in koppels of vectoren. Maar dat is niet. Het zijn afbeeldingen, dus functies, met zowel domein en bereik dezelfde verzameling $\{1,2,3,...\}$. Het gaat dus niet over het van plaats verwisselen an sich. Dus eventueel tekenen als verzamelingen met pijlen, en zo meerdere keren na mekaar. NIET tekenen als 5-tuples waarbij je telkens elementen van plaats verwisselt! 
7b is dan triviaal, want $(-1)^3=-1$. 7c is dan ook simpel. 


\section{Les 4 voorbereiding} 
Gedaan. 

\section{Les 4, 17 okt}
Niet live, maar video bekeken. Was ziek. 

Eerst nog verder over determinanten. De formule (2.2) p. 65 verder bestudeerd. Deze (2.2) is niet echt een definitie, denk ik, maar resultaat van rekenwerk aan de hand van definitie (2.1) p. 57. Best wel iets speciaal: we defini\"eren een determinant als een afbeelding met 3 eigenschappen, en komen dan tot besluit dat er maar 1 oplossing bestaat hiervoor. 

In de les gedemonstreerd hoe je van (2.2) komt tot rij- of kolomontwikkeling, eigenlijk paragraaf 2.2.4 p.68 uit boek. Introductie $S_n$ als verzameling van alle permutaties van $\{1,2,...,n\}$. Kwam erop neer gewoon eens te proberen alle $a_{1j}$ uit de rest van de formule af te zonderen, en dan te kijken wat er overblijft (voorbeeld voor ontwikkeling eerste rij). Prof introduceerde ook $S_{n,1}, S_{n,2},...$, deelverzamelingen van $S_n$ waarbij 1 afgebeeld wordt op 1, resp. 2 op 2, etc. In boek iets andere notatie gebruikt. Dan interpretatie van de overschot: wat blijft er nog over nadat bvb. $a_1j$ buitengebracht is? Dit wordt $C_{ij}$ genoemd, en door goed te bekijken zien we dat dit ook een determinant is. Voor $C_{11}$ is het vrij makkelijk, maar voor $C_{12}$ is het subtieler. $S_{n,2}$ is eigenlijk verzameling van alle permutaties van $\{1,3,4,...n\}$. En er is nog een belangrijke nuance: er staat daar nog een kolom 2 tussen die geschrapt wordt, waardoor het teken verandert. Het volledige bewijs van hoe dat juist werkt met die tekens, is iets complexer, en moeten we niet kennen. Maar je kan het "onthouden" door in te zien dat je eerst nog kolommen moet omwisselen om de geschrapte kolom uit beeld te krijgen, en we weten dat elke kolom-omwisseling de determinant van teken doet veranderen. Bvb. kolom 4 naar links schuiven, is eerst een wissel kolom 4 met kolom 3, dan kolom 3 met 2, dan 2 met 1; dan pas staat geschrapte kolom uit beeld en is de overblijvende matrix een "geheel". 
Besluit is dan: $C_{1i} = (-1)^{i-1} det (M_{1i})$, en dat $det A=  \sum_{i=1}^{n} a_{1i}C_{1i}$. 

Definitie (2.19) besproken (cofactor en minor), en verband (cofactor is determinant minor, op teken na). Paar toepassingen gezien, redelijk triviaal. Dan terug naar hoe we aan determinanten waren gekomen: inverteren, inverteerbaarheid, en concept adjunt matrix gezien. Definitie 2.22, eigenschap van stelling 2.23, en daaruit volgt dus dat $A^{-1}= \frac{1}{det(A)} \cdot adj(A)$. Opmerking: niet effici\"ent om inverse te berekenen! Ook bewijs gezien van stelling 2.23 voor \'e\'en deel, in twee stappen: diagonaal elementen gelijk aan $det(A)$, en dan niet-diagonaal elementen gelijk aan nul. Truuk in het bewijs: matrix met 2 gelijke rijen introduceren, waarvan we weten dat determinant 0 is. Als je die ontwikkelt naar de j-de rij (met j-de rij gelijk aan i-de rij), dat krijg je een ontwikkeling met resultaat nul, maar die ontwikkeling komt ook overeen met een stuk van de formule in het bewijs ergens. 

(pauze) 

Vectorruimten: iets nieuws. Een niveau van abstractie. Begonnen met voorbeelden: oplossingen-verzameling van homogene stelsels, matrices met optelling en scalaire vermenigvuldiging, de veeltermen met re\"ele co\"effici\"enten. Sprake van zogenaamde "evidente rekenregels" voor optelling en scalaire vermenigvuldiging, en ook distributiviteit van ene op andere. 

Nog een voorbeeld: continue functies. Kan je ook optellen en scalaire vermenigvuldigen. 

Dus: twee bewerkingen die we telkens hebben, optelling en scalaire vermenigvuldiging. 
Eerst even focus op die optelling: verzameling met 1 binaire bewerking die commutatief is: commutatieve groep (def. 3.2). Noot: er zijn ook niet-commutatieve groepen, bvb. samenstellen van permutaties, de vermenigvuldiging bij de inverteerbare matrices (jaja, die ene bewerking moet geen klassieke optelling zijn!). Anderzijds is er natuurlijk de gewone optelling in $\mathbb{Z}$ die wel een commutatieve groep is. 
Niet-commutatieve groepen hebben vaak een $\times$ of $\cdot$ symbool voor hun operatie, en het neutraal element wordt dan vaak als $1$ geschreven (en niet als $0$ zoals bij commutatieve groep). Tegengesteld element is nodig bij commutatieve groep, daarom is ($\mathbb{Z}, \cdot$) geen commutatieve groep: wel associatief, wel een neutraal element ($1$), en ook commutatief, maar geen tegengesteld element (inverse). Rationale getallen met vermenigvuldiging zijn wel commutatieve groep, maar je moet 0 weglaten (want je kan met $0 \cdot x$ nooit neutraal element $1$ uitkomen!). 

Even uitwijding naar aanleiding van een vraag: verschil axioma's en definities. Iemand vond het geen definities die werden gegeven, maar axioma's. Prof gaf hem ongelijk, zei dat axioma's vooral uit de verzamelingenleer komen. 

Principe vectorruimten: paar definities, voor de rest een wit blad. En dan gaan we stellingen bewijzen. Beginnen met paar eenvoudige stellingen: lemma 3.7 en lemma 3.8 in de les even bewezen. 
Opgelet: symbool $0$ is soms een re\"eel getal, en soms de nulvector. Zou uit context moeten blijken, maar toch belangrijk om bij stil te staan bij die bewijzen. Ook symbool $-$ heeft die dubbele betekenis: voor een getal is het het negatieve getal (bvb. $-1$), maar voor een vector bvb. $-v$ betekent het notatie van tegengestelde. Gelukkig is $(-1)v = -v$, zie lemma 3.8. 

Concept deel-vectorruimte: komt vaak voor! Stelling 3.11 i.v.m. deelruimtecriterium. Voorbeeld: $\mathbb{R}^n$ is een vectorruimte, en de oplossingen van een stelsel $AX=0$ is een deelvectorruimte van $\mathbb{R}^n$. 

\section{Les 4, nabeschouwing}

\subsection {Kolom/rij-ontwikkeling}

Oefening rap gemaakt, lukte wel. Opgelet: tekens! denk aan $(-1)^{i+j}$

Oefening 2 extra:  Leg zorgvuldig uit waarom in 2.2.6 in het bewijs van de formule van Cramer geldt
dat ... . Boek pagina 75. 
\[ 
det \begin{pmatrix}
a_{11} & ... & b_1 & ... & a_{1n} \\ 
a_{21} & ... & b_2 & ... & a_{2n} \\
...    & ... & ... & ... & ... \\ 
a_{n1} & ... & b_n & ... & a_{nn} 
\end{pmatrix} 
= b_1 C_{1i} + b_2 C_{2i} + ... 
\] 
Antwoord: gewoon kolomontwikkeling naar kolom $i$ (waar de $b_i$ staan). De kolom zelf waarnaar ontwikkeld wordt, komt niet voor in de minoren/cofactoren, dus zijn het de gewone $C_{ij}$ van de oorspronkelijke matrix, want de rest van de matrix is ongewijzigd. 


\subsection {Vectorruimten}

3. Bekijk Voorbeeld 3.6(3), pagina 93. 
(a) Toon voor de vectorruimte ($\mathbb{R}, \mathbb{R}[X], +$) aan dat de eigenschappen van neutraal element en tegengesteld element uit Definitie 3.2 gelden en toon 1 eigenschap naar keuze aan uit Definitie 3.3.

NE: er bestaat element 0 zodat $v+0=0=0+v$; voor ($\mathbb{R}, \mathbb{R}[X], +$) is dit de veelterm met alle  $a_i=0$. (noot: veelterm van graad 0). Triviaal dat uit definitie optelling (i.e. co\"effici\"enten van gelijke graad bij elkaar optellen), dat $v+0 = 0 = 0+v$. 

TE: bestaat ook, namelijk de veelterm $v'$ met co\"effici\"enten $a'_i = -a_i$. Ook hier is het triviaal uit definitie optelling, dat $v + v'$ een veelterm geeft met alle co\"effici\"enten gelijk aan $0$. 

Eigenschap naar keuze uit definitie 3.3 (p. 91): eigenschap 4 i.v.m. scalar 1: $1 . v=v$. Vermenigvuldiging met scalar voor veeltermen doe je door al zijn co\"effici\"enten met die scalar te vermenigvuldigen. Evident dat je dan opnieuw $v$ krijgt voor scalar 1. 

(b) Waarom vormen de monische veeltermen geen vectorruimte?
Monische veeltermen: hoogstegraadsco\"effici\"ent is 1. Bvb. $X^3 + 3X^2 -7X + 31$. 
Heel eenvoudig: tel voorbeeld op bij zichzelf. En hij is niet monisch meer, dus som zit niet meer in de deelverzameling $U$ (zoals in stelling 3.11 voor deelruimtecriterium), dus is het geen deelruimte, dus geen vectorruimte (wel een deelverzameling van alle veeltermen).  



4. Werk de opmerking op het einde van Voorbeeld 3.6(4) uit. (p. 93). De opmerking is: veeltermen van graad precies $n$ vormen geen vectorruimte. Ga na hoe dit komt. Opnieuw kijken naar criteria deelvectorruimten: is die optelling nog inwendig? Ook hier niet: neem hoogste co\"effici\"ent, zet er een $-$ voor, en tel op bij oorspronkelijke veelterm: de graad wordt eentje lager, niet meer precies $n$ dus, maar $n-1$. 

5. Bewijs Lemma 3.7. En bewijs het derde puntje van Lemma 3.8. 
Lemma 3.7: tiens, was ook oefening. Zie oefeningen. 
Lemma 3.8, punt 3: $(-\lambda)v = -(\lambda v) = \lambda(-v)$. Bvb. gebruikmakend van 
\[ (-\lambda)v = [(-1)\lambda]v = [\lambda (-1)]v = \lambda(-1)v = \lambda(-v) \]
(achtereenvolgens: gemengde associativiteit, commutativiteit ($\mathbb{R},\cdot$), Lemma 3.8 puntje 2), hebben we al de gelijkheid van de linkse en de rechtse uitdrukking aangetoond. 
Voor de middelste uitdrukking: 
\[
-(\lambda v) = (-1)(\lambda v) = (-1 \lambda) v = (- \lambda)v
\]
(achtereenvolgens puntje 2 Lemma 3.8, gemengde associativiteit, en \emph{gewone} tekenverandering in $\mathbb{R}$). 

6. Toon aan dat er geen re\"ele vectorruimten bestaan met eindig veel vectoren.
Raar. Volgens boek bestaat dat wel, namelijk de triviale vectorruimte $V=\{0\}$. 
Stel dat bedoeling van vraag is: vectorruimten met meer dan 1 element. Dan begon ik te lezen in definitie, en zit met een vraag: waar uit definitie leid je af dat elke mogelijke $\lambda$ maal elke mogelijke $v$ opnieuw een vector is in $V$?? 


7. Bekijk Voorbeeld 3.6(6), p.94. Hier staan voorbeelden van complexe vectorruimten. 
Bekijk dan Voorbeeld 3.6(2) op p. 93 en leg uit waarom ($\mathbb{C}, \mathbb{R}, +$) geen (complexe) vectorruimte is.
Ook dit lijkt gebaseerd op de aanname dat elke mogelijke $\lambda$ maal elke mogelijke $v$ steeds een vector is in $V$. (aanname wordt precies vaak gemaakt, bvb. bij deelruimten, maar ik snap niet goed waar ze vandaan komt). 


\section{Les 5, voorbereiding}
OK, gelezen en over nagedacht. (ivm de voortgebrachte ruimte van een deelverzameling $D$ van de commutatieve groep van een vectorruimte). 

\section{Les 5, 24 okt}

Voorbeelden deelruimten. 

Voorbeeld 1: veeltermen $P$ met  graad $\leq n$, is een deelruimte van $\mathbb{R}[X]$. 

Voorbeeld 2: veeltermen met graad exact $n$ zijn geen deelruimte. (zie oefening ergens; van zichzelf aftrekken = graad minder). Overigens zit nul-veelterm hier ook niet in. 

Voorbeeld 3: wat zijn alle deelruimten van het vlak? Alle rechten door de oorsprong (nul zit erin, en veelvouden blijven op rechte liggen). En dan nog de twee triviale: {0}, de ganse ruimte. 

Voorbeeld 4: deelruimten van $\mathbb{R}^3$? Vlakken door de oorsprong, ook alle rechten door de oorsprong, en de twee triviale deelruimten. We voelen hier al aan: de dimensie is verschillend voor al deze deelruimten. 

Nieuw concept dat vaak terugkomt: lineaire combinaties. (\emph{de} prototypische operatie bij vectorruimten). De vector $\alpha_1 v1 + \alpha_2 v2 + ... \alpha_n v_n$ noemen we een lineaire combinatie van $v_1, ..., v_n$. Andere notatie: $\sum_{i-1}^n \alpha_i v_i $. 
We voelen hier al aan voor $\mathbb{R}^3$: alle lineaire combinaties van twee vectoren (die niet op 1 rechte liggen), die vormen een vlak. Dit kan veralgemeend worden. 

Definitie 3.15. Deelruimte voorgebracht door $D$. Noteer: dit is de kleinst mogelijke deelruimte waarin alle vectoren van $D$ in zitten. In woorden uitgelegd in de les waarom dit een deelruimte is. Notatie $vct(D)$, en andere notaties zie boek. 

Bekijken we deelruimten van de ruimte, "een rechte" en "een vlak". Ze gaan door de oorsprong (dus niet evenwijdig tenzij rechte in vlak ligt). Bekijken we twee rechten (die niet samenvallen), de voortgebrachte ruimte is het vlak door die twee rechten. Wat valt er op: de veelvouden van de vectoren op de rechten zitten allemaal in de deelruimten, voor de "nieuwe" vectoren van de voortgebrachte ruimte moeten we alleen kijken naar de sommen van vectoren. Weer een interessant concept: 

Definitie somruimte: Als $V_1, ... V_n \subset V$ allemaal deelruimten, dan is $V_1 + V_2 + ... V_n = \{ v_1 + ... + v_n | v_i \in V_i \}$  terug een deelruimte. Notatie $V_1 + V_2 + ... V_n = vct(V_1 \cup V_2 ... \cup V_n)$ . Nagaan: waarom is dat een deelruimte? (via eigenschappen voor deelruimte). 

Definitie "directe som" (eerst voor twee deelruimten gezien, verderop voor meer dan twee): is ook een somruimte, maar hierbij kunnen we elke vector van de somruimte op \emph{unieke} wijze kunnen schrijven als som van iets in $U$ en iets in $V$. Voorbeeld: somruimte van vlak en rechte, vs. somruimte van twee rechten. Beide geven volledige 3D ruimte, maar alleen de somruimte van twee rechten is een directe som. 

Propositie: $U+W$ is een directe som als en slechts als $U \cap W = \{ 0 \} $. Bewijs van een "asa", dus twee implicaties. Stap 1 de pijl terug: via veronderstelling dat $v=u_1 + w_1$ maar ook $v=u_2 + w_2$, en dan bewijzen dat $u_1=u_2$ en $w_1=w_2$. (met gegeven dat doorsnede $V$ en $W$ alleen de nulvector is). We komen tot een gelijkheid van twee vectoren waarvan we weten dat ene in $V$ zit en andere in $W$, dus ze zitten alle twee in de doorsnede, dus ze moeten beide $0$ zijn, en dus ... 
Stap 2 de pijl naar rechts. Gegeven direct som, bewijzen dat doorsnede alleen $0$ bevat. Of via contrapositie: als doorsnede niet gelijk is aan $\{ 0 \} $, bewijzen dat er een vector is in $U+W$ die twee schrijfwijzen heeft. In les helemaal uitgeschreven. 

Opnieuw even bekijken in termen van dimensies. We verwachten dat voor direct som, de som van de dimensies gelijk is aan de dimensie van de somruimte. Voor niet-directe som: we gaan te veel hebben, hoeveel te veel? Ah, de dimensie van de doorsnede moeten we er misschien van aftrekken? Zie volgende les. 

Definitie 3.20: directe som voor meer dan 2 deelruimten. Uitbreiding propositie in verband met doorsnede: kijken naar doorsnede van 1 van de deelruimten met de somruimte van al de rest! En vooral: moet gelden voor elke van de deelruimten die je apart neemt! 
Niet zo evident, maar denk aan drie rechten door 0 in hetzelfde vlak. Somruimte van twee rechten is al het vlak, en doorsnede van dat vlak met derde rechte is opnieuw rechte. Bewijs is quasi zelfde als voor som van twee deelruimten. 

(pauze; voorstelling POC, permanente onderwijscommissie, studentenvertegenwoordiging hierin) 

Volgende (fundamenteel) concept: lineaire afhankelijkheid, onafhankelijkheid. Hangt ook samen met begrip dimensie. Zolang je van een groep vectoren een vector kan schrijven als lineaire combinatie van de andere vectoren, dan is hij eigenlijk overtollig wat betreft de somruimte. Hij is dan ook niet lineair onafhankelijk. Als we zo'n groep vectoren in een deelverzameling $D$ steken, dan spreken we van de lineaire afhankelijkheid of onafhankelijkheid van die deelverzamling $D$. Definitie 3.26. Ook "vrij" als concept! (lineair onafhankelijk). 

Als $D$ maar 1 vector bevat: uiteraard lineair onafhankelijk. Tweede vector erbij: lineair onafhankelijk als hij geen veelvoud is van de eerste. Derde erbij: als hij niet in het vlak ligt (als we over ruimte spreken). Vraag: kunnen we een vierde vinden in $\mathbb{R}^3$? We voelen aan dat dat niet kan. 

Met voorbeelden gezien: hoe nagaan of vectoren lineair onafhankelijk zijn. Met veeltermen gezien. We voelen al dat het moeilijk kan zijn om heel zeker te zijn van ons stuk bij bvb. $\{ 1, 1+X, 1+X+X^2 \}$. En zeker bij  $\{ X,X^2-X^3,  1+X+X^3, 1-X+X^2 \}$

Propositie 3.27: voldoende om te bewijzen dat enige lineaire combinatie die nulvector oplevert, diegene is wanneer de co\"effici\"enten nul zijn. Opgelet: er staat iets voor eindige en niet per se eindige $D$ in de propositie, dat had ik niet helemaal door. 

Dan eerst de propositie toegepast op paar voorbeelden. Stellen dat een lineaire combinatie 0 is, en dan bewijzen dat hieruit volgt dat alle co\"effici\"enten nul zijn, is tegelijk een bewijs dat de vectoren lineair onafhankelijk zijn. 
En dat is vrij gemakkelijk bij veeltermen. 

Bewijs in de les gedaan. Maar alleen voor eindig aantal vectoren in $D$. Opnieuw twee richtingen van de "asa" apart te bewijzen. Pijl rechts: te bewijzen dat alle $\lambda_i=0$. Uit ongerijmde: stel een $\lambda_j \neq 0$. We brengen in dat bewijs die $\lambda_j v_j$ apart uit de vergelijking. Dan alles delen door $\lambda_j$, en dan kunnen we die $v_j$ schrijven als lineaire combinatie van de andere $v_i$. Maar dat kon niet, was de aanname, ... tegenspraak. 
Dan pijl terug. Stel ze zijn lineair afhankelijk. T.B. we kunnen $\lambda_i$ vinden niet alle 0 en toch lineaire combinatie gelijk aan 0. Bewijs van dit deel ook in les uitgeschreven gezien. Vrij eenvoudig om dan een lin. combinatie te schrijven met minstens een niet-0 co\"effici\"ent. 

Zeer belangrijke propositie, gaan we vaak gebruiken als criterium voor lineaire onafhankelijkheid van gegeven vectoren. 

Volgende concept: wanneer hebben we genoeg vectoren om de hele ruimte op te spannen. Concept heet "voortbrengend". Definitie 3.31. Voortbrengend gaat over "genoeg". Concept "vrij" gaat over "niet te veel". Combinatie vrij en voortbrengend: 

Concept "basis". Definitie 3.33: een basis is vrij en voortbrengend. 

Voorbeeld basis voor veeltermen met graad $\leq n$. Dat het voortbrengend is, is triviaal, want volgt uit definitie van de verzameling van die veeltermen (is al geschreven als lineaire combinatie van die basis). Dat die basis vrij is, is ook vrij logisch (hoewel ik bewijs in de les niet heel formeel vond). 
Ander voorbeeld, van een basis voor de veeltermen met graad hoogstens 2. Je voelt wel aan dat het kan. 

We zien dus al onmiddellijk met die voorbeelden: een basis is niet per se uniek. Wel is het aantal elementen in een basis precies altijd hetzelfde. Dat gaan we later de dimensie noemen. 




\section{Les 5, nabeschouwing}

\subsection{Deelruimten} 
NB 5.1: Maak oefening 3.6.2 (p. 128-129) voor W2, W5, W6 en W8: zie oefenzittingen. 

NB 5.2:  Teken Venn-diagram met $V, D, vct(D)$. OK, getekend. Als $D$ een deelruimte is, dan is $vct(D)=D$. 

NB 5.3a:  Neem propositie 3.14 p.98. Toon aan voor oneindig veel deelruimten. 
Propositie: algemene doorsnede van deelruimten $U_i$ is opnieuw een deelruimte (ook voor oneindig veel deelruimten). 
Bewijs: de nulvector zit in alle deelruimten, dus ook in de algemene doorsnede,  dus aan dit criterium voor deelruimten is voldaan. Volgend deel criterium: zit een willekeurige $rx +sy$ met $x,y \in \bigcap$ en $r,s \in \mathbb{R}$ ook in $\bigcap$? Antwoord is ja, want voor elke $U_i$ geldt dat $x,y \in U_i$, en elke $U_i$ is een vectorruimte, dus elke $rx +sy$ zit in $U_i$, en dat geldt voor alle $i$. Dus zit $rx +sy$ ook in $\bigcap$. QED. 

NB 5.3b:  opdracht 3.18, p.100. Toon aan dat $vct(D)$ de doorsnede is van alle deelruimten die $D$ bevatten. 
Definitie $vct(D)$: de vectorruimte voortgebracht door $D$, of ook: de verzameling van alle lineaire combinaties van $x_i \in D$. Wat over nagedacht, maar uiteindelijk geskipt, ik vond het een lastige. Er stond ook nog een hint (je mag gebruiken dat vct(D) de kleinste deelruimte is van $V$ die $D$ omvat). 
Opnieuw bekeken: de hint wil eigenlijk zeggen: noem $\{ U_i | i \in I, D \subset  U_i \subset V \}$  de verzameling van alle deelruimten van $V$ die $D$ omvatten. Dan geldt: $vct(D) \subset U_i$ voor alle $i \in I$, want $vct(D)$ is een deelruimte, en is de kleinste van alle. ?? is dit zo? wat is dat juist, "de kleinste". opnieuw in boek lezen... 


\subsection{Som/direct som} 
NB 5.4:  Venn-diagram getekend. Met in achterhoofd: $V=\mathbb{R}^3$, $U$ is x-as, $W$ is y-as, dan is $U+W$ het vlak door x-as en y-as. En dan in Venn-diagram paar punten gezet $(0,0,0), (x_1,0,0), (0,y_1,0), ... (x_1,y_2,0)$. $U+W$ omspant duidelijk $U \cup W$. 

NB 5.5:  oefening 3.6.28 p. 132. Zie oefeningen. 

NB 5.6:  
Propositie 3.23, p. 102. Puntje 2. Voorbeeld: neem $V=\mathbb{R}^2$, $U_1$ de x-as, $U_2$ de y-as, en $U_3$ de diagonaal tussen x- en y-as. Dan gelden dat alle paarsgewijze doorsneden gelijk zijn aan $\{0\}$. En er geldt ook $U_1+U_2+U_3=W$. Toch is het geen directe som, want je kan elke vector op meerdere manieren schrijven als $u_1+u_2+u_3$. 
Kijken we naar de echte voorwaarde van propositie 3.23: $U_1 \cap ( U_2 + U_3 ) = U_1 \neq \{0\}$. Verschil tussen de beweringen: in propositie wordt uit de som 1 deelruimte uitgelicht, en dan gezien dat de overblijvende som niks gemeenschappelijk heeft met de uitgelichte deelruimte (behalve nulvector). Dat is iets helemaal anders dan twee aan twee doorsnedes bekijken. 


\subsection{Lin. (on)afh.}
NB 5.7, opdracht 3.30 p. 106, puntje 1: 
Uitgerekend: $\lambda_1 v_1 + \lambda_2 v_2 + \lambda_3 v_3$ gelijkgesteld aan nulmatrix, stelsel van gemaakt, en opgelost naar $\lambda_i$. Drie $\lambda_i$ zijn effectief 0. Dus lineair onafhankelijk. 


\section*{Les 6, voorbereiding}

\subsection*{1}
Geef een basis voor vectorruimte $\mathbb{R}^{n \times m}$ , dus alle $(n \times m)$-matrices.

Definitie basis terug opgezocht: een basis is vrij (lin. onafh.) en voortbrengend $vct(basis) = V$.
Mij lijkt dat je $m\timesn$ matrices moet hebben van de vorm \begin{pmatrix}
    0 & ... & 0 \\
      & ... & \\ 
    0 &  a_{ij} =1 & 0 \\
      & ... & \\
    0 & ... & 0 
\end{pmatrix} met $i = \{1,2,...m\} $ en $j=\{ 1,2,...n\}$.

Dus voor $2 \times 2$ matrices: $B = \left\{  \begin{pmatrix} 1 & 0\\ 0&0 \end{pmatrix}, \begin{pmatrix} 0 & 1\\ 0&0 \end{pmatrix}, \begin{pmatrix} 0 & 0\\ 1&0 \end{pmatrix}, \begin{pmatrix} 0 & 0\\ 0&1 \end{pmatrix}    \right\} $ is een basis. 

\subsection*{2}
De vectorruimte voortgebracht door $(1, 2, 3)$ en $(4, 5, 6)$. 
Aan welke voorwaarden moeten $a, b, c \in \mathbb{R} $ voldoen opdat de vector $(a, b, c)$ in die voorgebrachte ruimte zit? 

Voortgebrachte ruimte: alle lineaire combinaties $(x+4y, 2x+5y, 3x+6y)$, of $a=x+4y$, $b=2x+5y$ en $c=3x+6y$. 
Nu willen ze dit graag geschreven zien als vergelijking van het vlak door die twee punten en door $(0,0,0)$. 
Eigenlijk staat dit er toch al: parameters $x$ en $y$ te kiezen, en er komen 3 co\"ordinaten uit. 
Algemeen vlak: $lx + my + nz = 0$. Dus stelsel is: 
$\left\{ \begin{aligned}
l1 + m2 + n3 &= 0 \\
l4 + m5 + n6 &= 0 
\end{aligned} \right.$ 
Of 
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
4l + 5m + 6n &= 0 
\end{aligned} \right.$ 
Als en slechts als
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
   - 3m - 6n &= 0 
\end{aligned} \right.$ 
of
$\left\{ \begin{aligned}
1l + 2m + 3n &= 0 \\
    m + 2n &= 0 
\end{aligned} \right.$ 

Noem $n=\lambda$, dan: 
$\left\{ \begin{aligned}
l  &=  \lambda \\
m &= -2 \lambda
\end{aligned} \right.$ 
Dus het vlak: $x -2  y + z = 0$

Djuu, ik maak hier een rekenfout, of erger: redeneerfout, en ik vind ze niet. Later opnieuw bekijken! 

\section{Les 6}


\section{KEEP ME TO JUMP TO END}

\end{document}
